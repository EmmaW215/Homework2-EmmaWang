{
  "metadata": {
    "total_papers": 200,
    "successful": 200,
    "failed": 0,
    "timestamp": "2025-11-02T22:14:47.621936",
    "category": "cs.CL"
  },
  "papers": [
    {
      "url": "https://arxiv.org/abs/2510.26802v1",
      "title": "Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark",
      "abstract": "Recent video generation models can produce high-fidelity, temporally coherent\nvideos, indicating that they may encode substantial world knowledge. Beyond\nrealistic synthesis, they also exhibit emerging behaviors indicative of visual\nperception, modeling, and manipulation. Yet, an important question still\nremains: Are video models ready to serve as zero-shot reasoners in challenging\nvisual reasoning scenarios? In this work, we conduct an empirical study to\ncomprehensively investigate this question, focusing on the leading and popular\nVeo-3. We evaluate its reasoning behavior across 12 dimensions, including\nspatial, geometric, physical, temporal, and embodied logic, systematically\ncharacterizing both its strengths and failure modes. To standardize this study,\nwe curate the evaluation data into MME-CoF, a compact benchmark that enables\nin-depth and thorough assessment of Chain-of-Frame (CoF) reasoning. Our\nfindings reveal that while current video models demonstrate promising reasoning\npatterns on short-horizon spatial coherence, fine-grained grounding, and\nlocally consistent dynamics, they remain limited in long-horizon causal\nreasoning, strict geometric constraints, and abstract logic. Overall, they are\nnot yet reliable as standalone zero-shot reasoners, but exhibit encouraging\nsigns as complementary visual engines alongside dedicated reasoning models.\nProject page: https://video-cof.github.io",
      "authors": [
        "Ziyu Guo",
        "Xinyan Chen",
        "Renrui Zhang",
        "Ruichuan An",
        "Yu Qi",
        "Dongzhi Jiang",
        "Xiangtai Li",
        "Manyuan Zhang",
        "Hongsheng Li",
        "Pheng-Ann Heng"
      ],
      "date": "2025-10-30",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26802v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26802v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.26790v1",
      "title": "Gistify! Codebase-Level Understanding via Runtime Execution",
      "abstract": "As coding agents are increasingly deployed in large codebases, the need to\nautomatically design challenging, codebase-level evaluation is central. We\npropose Gistify, a task where a coding LLM must create a single, minimal,\nself-contained file that can reproduce a specific functionality of a codebase.\nThe coding LLM is given full access to a codebase along with a specific\nentrypoint (e.g., a python command), and the generated file must replicate the\noutput of the same command ran under the full codebase, while containing only\nthe essential components necessary to execute the provided command. Success on\nGistify requires both structural understanding of the codebase, accurate\nmodeling of its execution flow as well as the ability to produce potentially\nlarge code patches. Our findings show that current state-of-the-art models\nstruggle to reliably solve Gistify tasks, especially ones with long executions\ntraces.",
      "authors": [
        "Hyunji Lee",
        "Minseon Kim",
        "Chinmay Singh",
        "Matheus Pereira",
        "Atharv Sonwane",
        "Isadora White",
        "Elias Stengel-Eskin",
        "Mohit Bansal",
        "Zhengyan Shi",
        "Alessandro Sordoni",
        "Marc-Alexandre Côté",
        "Xingdi Yuan",
        "Lucas Caccia"
      ],
      "date": "2025-10-30",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26790v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26790v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.26788v1",
      "title": "Defeating the Training-Inference Mismatch via FP16",
      "abstract": "Reinforcement learning (RL) fine-tuning of large language models (LLMs) often\nsuffers from instability due to the numerical mismatch between the training and\ninference policies. While prior work has attempted to mitigate this issue\nthrough algorithmic corrections or engineering alignments, we show that its\nroot cause lies in the floating point precision itself. The widely adopted\nBF16, despite its large dynamic range, introduces large rounding errors that\nbreaks the consistency between training and inference. In this work, we\ndemonstrate that simply reverting to \\textbf{FP16} effectively eliminates this\nmismatch. The change is simple, fully supported by modern frameworks with only\na few lines of code change, and requires no modification to the model\narchitecture or learning algorithm. Our results suggest that using FP16\nuniformly yields more stable optimization, faster convergence, and stronger\nperformance across diverse tasks, algorithms and frameworks. We hope these\nfindings motivate a broader reconsideration of precision trade-offs in RL\nfine-tuning.",
      "authors": [
        "Penghui Qi",
        "Zichen Liu",
        "Xiangxin Zhou",
        "Tianyu Pang",
        "Chao Du",
        "Wee Sun Lee",
        "Min Lin"
      ],
      "date": "2025-10-30",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26788v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26788v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.26787v1",
      "title": "Remote Labor Index: Measuring AI Automation of Remote Work",
      "abstract": "AIs have made rapid progress on research-oriented benchmarks of knowledge and\nreasoning, but it remains unclear how these gains translate into economic value\nand automation. To measure this, we introduce the Remote Labor Index (RLI), a\nbroadly multi-sector benchmark comprising real-world, economically valuable\nprojects designed to evaluate end-to-end agent performance in practical\nsettings. AI agents perform near the floor on RLI, with the highest-performing\nagent achieving an automation rate of 2.5%. These results help ground\ndiscussions of AI automation in empirical evidence, setting a common basis for\ntracking AI impacts and enabling stakeholders to proactively navigate AI-driven\nlabor automation.",
      "authors": [
        "Mantas Mazeika",
        "Alice Gatti",
        "Cristina Menghini",
        "Udari Madhushani Sehwag",
        "Shivam Singhal",
        "Yury Orlovskiy",
        "Steven Basart",
        "Manasi Sharma",
        "Denis Peskoff",
        "Elaine Lau",
        "Jaehyuk Lim",
        "Lachlan Carroll",
        "Alice Blair",
        "Vinaya Sivakumar",
        "Sumana Basu",
        "Brad Kenstler",
        "Yuntao Ma",
        "Julian Michael",
        "Xiaoke Li",
        "Oliver Ingebretsen",
        "Aditya Mehta",
        "Jean Mottola",
        "John Teichmann",
        "Kevin Yu",
        "Zaina Shaik",
        "Adam Khoja",
        "Richard Ren",
        "Jason Hausenloy",
        "Long Phan",
        "Ye Htet",
        "Ankit Aich",
        "Tahseen Rabbani",
        "Vivswan Shah",
        "Andriy Novykov",
        "Felix Binder",
        "Kirill Chugunov",
        "Luis Ramirez",
        "Matias Geralnik",
        "Hernán Mesura",
        "Dean Lee",
        "Ed-Yeremai Hernandez Cardona",
        "Annette Diamond",
        "Summer Yue",
        "Alexandr Wang",
        "Bing Liu",
        "Ernesto Hernandez",
        "Dan Hendrycks"
      ],
      "date": "2025-10-30",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26787v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26787v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.26768v1",
      "title": "AMO-Bench: Large Language Models Still Struggle in High School Math Competitions",
      "abstract": "We present AMO-Bench, an Advanced Mathematical reasoning benchmark with\nOlympiad level or even higher difficulty, comprising 50 human-crafted problems.\nExisting benchmarks have widely leveraged high school math competitions for\nevaluating mathematical reasoning capabilities of large language models (LLMs).\nHowever, many existing math competitions are becoming less effective for\nassessing top-tier LLMs due to performance saturation (e.g., AIME24/25). To\naddress this, AMO-Bench introduces more rigorous challenges by ensuring all 50\nproblems are (1) cross-validated by experts to meet at least the International\nMathematical Olympiad (IMO) difficulty standards, and (2) entirely original\nproblems to prevent potential performance leakages from data memorization.\nMoreover, each problem in AMO-Bench requires only a final answer rather than a\nproof, enabling automatic and robust grading for evaluation. Experimental\nresults across 26 LLMs on AMO-Bench show that even the best-performing model\nachieves only 52.4% accuracy on AMO-Bench, with most LLMs scoring below 40%.\nBeyond these poor performances, our further analysis reveals a promising\nscaling trend with increasing test-time compute on AMO-Bench. These results\nhighlight the significant room for improving the mathematical reasoning in\ncurrent LLMs. We release AMO-Bench to facilitate further research into\nadvancing the reasoning abilities of language models.\nhttps://amo-bench.github.io/",
      "authors": [
        "Shengnan An",
        "Xunliang Cai",
        "Xuezhi Cao",
        "Xiaoyu Li",
        "Yehao Lin",
        "Junlin Liu",
        "Xinxuan Lv",
        "Dan Ma",
        "Xuanlin Wang",
        "Ziwen Wang",
        "Shuang Zhou"
      ],
      "date": "2025-10-30",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26768v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26768v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.26745v1",
      "title": "Deep sequence models tend to memorize geometrically; it is unclear why",
      "abstract": "In sequence modeling, the parametric memory of atomic facts has been\npredominantly abstracted as a brute-force lookup of co-occurrences between\nentities. We contrast this associative view against a geometric view of how\nmemory is stored. We begin by isolating a clean and analyzable instance of\nTransformer reasoning that is incompatible with memory as strictly a storage of\nthe local co-occurrences specified during training. Instead, the model must\nhave somehow synthesized its own geometry of atomic facts, encoding global\nrelationships between all entities, including non-co-occurring ones. This in\nturn has simplified a hard reasoning task involving an $\\ell$-fold composition\ninto an easy-to-learn 1-step geometric task.\n  From this phenomenon, we extract fundamental aspects of neural embedding\ngeometries that are hard to explain. We argue that the rise of such a geometry,\ndespite optimizing over mere local associations, cannot be straightforwardly\nattributed to typical architectural or optimizational pressures.\nCounterintuitively, an elegant geometry is learned even when it is not more\nsuccinct than a brute-force lookup of associations.\n  Then, by analyzing a connection to Node2Vec, we demonstrate how the geometry\nstems from a spectral bias that -- in contrast to prevailing theories -- indeed\narises naturally despite the lack of various pressures. This analysis also\npoints to practitioners a visible headroom to make Transformer memory more\nstrongly geometric. We hope the geometric view of parametric memory encourages\nrevisiting the default intuitions that guide researchers in areas like\nknowledge acquisition, capacity, discovery and unlearning.",
      "authors": [
        "Shahriar Noroozizadeh",
        "Vaishnavh Nagarajan",
        "Elan Rosenfeld",
        "Sanjiv Kumar"
      ],
      "date": "2025-10-30",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26745v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "stat.ML"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26745v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.26732v1",
      "title": "Cross-Platform Evaluation of Reasoning Capabilities in Foundation Models",
      "abstract": "This paper presents a comprehensive cross-platform evaluation of reasoning\ncapabilities in contemporary foundation models, establishing an\ninfrastructure-agnostic benchmark across three computational paradigms: HPC\nsupercomputing (MareNostrum 5), cloud platforms (Nebius AI Studio), and\nuniversity clusters (a node with eight H200 GPUs).\n  We evaluate 15 foundation models across 79 problems spanning eight academic\ndomains (Physics, Mathematics, Chemistry, Economics, Biology, Statistics,\nCalculus, and Optimization) through three experimental phases: (1) Baseline\nestablishment: Six models (Mixtral-8x7B, Phi-3, LLaMA 3.1-8B, Gemma-2-9b,\nMistral-7B, OLMo-7B) evaluated on 19 problems using MareNostrum 5, establishing\nmethodology and reference performance; (2) Infrastructure validation: The\n19-problem benchmark repeated on university cluster (seven models including\nFalcon-Mamba state-space architecture) and Nebius AI Studio (nine\nstate-of-the-art models: Hermes-4 70B/405B, LLaMA 3.1-405B/3.3-70B, Qwen3\n30B/235B, DeepSeek-R1, GPT-OSS 20B/120B) to confirm infrastructure-agnostic\nreproducibility; (3) Extended evaluation: Full 79-problem assessment on both\nuniversity cluster and Nebius platforms, probing generalization at scale across\narchitectural diversity.\n  The findings challenge conventional scaling assumptions, establish training\ndata quality as more critical than model size, and provide actionable\nguidelines for model selection across educational, production, and research\ncontexts. The tri-infrastructure methodology and 79-problem benchmark enable\nlongitudinal tracking of reasoning capabilities as foundation models evolve.",
      "authors": [
        "J. de Curtò",
        "I. de Zarzà",
        "Pablo García",
        "Jordi Cabot"
      ],
      "date": "2025-10-30",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26732v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26732v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.26707v1",
      "title": "Value Drifts: Tracing Value Alignment During LLM Post-Training",
      "abstract": "As LLMs occupy an increasingly important role in society, they are more and\nmore confronted with questions that require them not only to draw on their\ngeneral knowledge but also to align with certain human value systems.\nTherefore, studying the alignment of LLMs with human values has become a\ncrucial field of inquiry. Prior work, however, mostly focuses on evaluating the\nalignment of fully trained models, overlooking the training dynamics by which\nmodels learn to express human values. In this work, we investigate how and at\nwhich stage value alignment arises during the course of a model's\npost-training. Our analysis disentangles the effects of post-training\nalgorithms and datasets, measuring both the magnitude and time of value drifts\nduring training. Experimenting with Llama-3 and Qwen-3 models of different\nsizes and popular supervised fine-tuning (SFT) and preference optimization\ndatasets and algorithms, we find that the SFT phase generally establishes a\nmodel's values, and subsequent preference optimization rarely re-aligns these\nvalues. Furthermore, using a synthetic preference dataset that enables\ncontrolled manipulation of values, we find that different preference\noptimization algorithms lead to different value alignment outcomes, even when\npreference data is held constant. Our findings provide actionable insights into\nhow values are learned during post-training and help to inform data curation,\nas well as the selection of models and algorithms for preference optimization\nto improve model alignment to human values.",
      "authors": [
        "Mehar Bhatia",
        "Shravan Nayak",
        "Gaurav Kamath",
        "Marius Mosbach",
        "Karolina Stańczak",
        "Vered Shwartz",
        "Siva Reddy"
      ],
      "date": "2025-10-30",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26707v1",
      "categories": [
        "cs.CL",
        "cs.CY",
        "cs.LG"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26707v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.26697v2",
      "title": "The End of Manual Decoding: Towards Truly End-to-End Language Models",
      "abstract": "The \"end-to-end\" label for LLMs is a misnomer. In practice, they depend on a\nnon-differentiable decoding process that requires laborious, hand-tuning of\nhyperparameters like temperature and top-p. This paper introduces AutoDeco, a\nnovel architecture that enables truly \"end-to-end\" generation by learning to\ncontrol its own decoding strategy. We augment the standard transformer with\nlightweight heads that, at each step, dynamically predict context-specific\ntemperature and top-p values alongside the next-token logits. This approach\ntransforms decoding into a parametric, token-level process, allowing the model\nto self-regulate its sampling strategy within a single forward pass.\n  Through extensive experiments on eight benchmarks, we demonstrate that\nAutoDeco not only significantly outperforms default decoding strategies but\nalso achieves performance comparable to an oracle-tuned baseline derived from\n\"hacking the test set\"-a practical upper bound for any static method.\nCrucially, we uncover an emergent capability for instruction-based decoding\ncontrol: the model learns to interpret natural language commands (e.g.,\n\"generate with low randomness\") and adjusts its predicted temperature and top-p\non a token-by-token basis, opening a new paradigm for steerable and interactive\nLLM decoding.",
      "authors": [
        "Zhichao Wang",
        "Dongyang Ma",
        "Xinting Huang",
        "Deng Cai",
        "Tian Lan",
        "Jiahao Xu",
        "Haitao Mi",
        "Xiaoying Tang",
        "Yan Wang"
      ],
      "date": "2025-10-30",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26697v2",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26697v2"
    },
    {
      "url": "https://arxiv.org/abs/2510.26692v1",
      "title": "Kimi Linear: An Expressive, Efficient Attention Architecture",
      "abstract": "We introduce Kimi Linear, a hybrid linear attention architecture that, for\nthe first time, outperforms full attention under fair comparisons across\nvarious scenarios -- including short-context, long-context, and reinforcement\nlearning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an\nexpressive linear attention module that extends Gated DeltaNet with a\nfiner-grained gating mechanism, enabling more effective use of limited\nfinite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware\nefficiency through a specialized variant of the Diagonal-Plus-Low-Rank (DPLR)\ntransition matrices, which substantially reduces computation compared to the\ngeneral DPLR formulation while remaining more consistent with the classical\ndelta rule.\n  We pretrain a Kimi Linear model with 3B activated parameters and 48B total\nparameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention\n(MLA). Our experiments show that with an identical training recipe, Kimi Linear\noutperforms full MLA with a sizeable margin across all evaluated tasks, while\nreducing KV cache usage by up to 75% and achieving up to 6 times decoding\nthroughput for a 1M context. These results demonstrate that Kimi Linear can be\na drop-in replacement for full attention architectures with superior\nperformance and efficiency, including tasks with longer input and output\nlengths.\n  To support further research, we open-source the KDA kernel and vLLM\nimplementations, and release the pre-trained and instruction-tuned model\ncheckpoints.",
      "authors": [
        "Kimi Team",
        "Yu Zhang",
        "Zongyu Lin",
        "Xingcheng Yao",
        "Jiaxi Hu",
        "Fanqing Meng",
        "Chengyin Liu",
        "Xin Men",
        "Songlin Yang",
        "Zhiyuan Li",
        "Wentao Li",
        "Enzhe Lu",
        "Weizhou Liu",
        "Yanru Chen",
        "Weixin Xu",
        "Longhui Yu",
        "Yejie Wang",
        "Yu Fan",
        "Longguang Zhong",
        "Enming Yuan",
        "Dehao Zhang",
        "Yizhi Zhang",
        "T. Y. Liu",
        "Haiming Wang",
        "Shengjun Fang",
        "Weiran He",
        "Shaowei Liu",
        "Yiwei Li",
        "Jianlin Su",
        "Jiezhong Qiu",
        "Bo Pang",
        "Junjie Yan",
        "Zhejun Jiang",
        "Weixiao Huang",
        "Bohong Yin",
        "Jiacheng You",
        "Chu Wei",
        "Zhengtao Wang",
        "Chao Hong",
        "Yutian Chen",
        "Guanduo Chen",
        "Yucheng Wang",
        "Huabin Zheng",
        "Feng Wang",
        "Yibo Liu",
        "Mengnan Dong",
        "Zheng Zhang",
        "Siyuan Pan",
        "Wenhao Wu",
        "Yuhao Wu",
        "Longyu Guan",
        "Jiawen Tao",
        "Guohong Fu",
        "Xinran Xu",
        "Yuzhi Wang",
        "Guokun Lai",
        "Yuxin Wu",
        "Xinyu Zhou",
        "Zhilin Yang",
        "Yulun Du"
      ],
      "date": "2025-10-30",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26692v1",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26692v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.26683v1",
      "title": "Evontree: Ontology Rule-Guided Self-Evolution of Large Language Models",
      "abstract": "Large language models (LLMs) have demonstrated exceptional capabilities\nacross multiple domains by leveraging massive pre-training and curated\nfine-tuning data. However, in data-sensitive fields such as healthcare, the\nlack of high-quality, domain-specific training corpus hinders LLMs' adaptation\nfor specialized applications. Meanwhile, domain experts have distilled domain\nwisdom into ontology rules, which formalize relationships among concepts and\nensure the integrity of knowledge management repositories. Viewing LLMs as\nimplicit repositories of human knowledge, we propose Evontree, a novel\nframework that leverages a small set of high-quality ontology rules to\nsystematically extract, validate, and enhance domain knowledge within LLMs,\nwithout requiring extensive external datasets. Specifically, Evontree extracts\ndomain ontology from raw models, detects inconsistencies using two core\nontology rules, and reinforces the refined knowledge via self-distilled\nfine-tuning. Extensive experiments on medical QA benchmarks with\nLlama3-8B-Instruct and Med42-v2 demonstrate consistent outperformance over both\nunmodified models and leading supervised baselines, achieving up to a 3.7%\nimprovement in accuracy. These results confirm the effectiveness, efficiency,\nand robustness of our approach for low-resource domain adaptation of LLMs.",
      "authors": [
        "Mingchen Tu",
        "Zhiqiang Liu",
        "Juan Li",
        "Liangyurui Liu",
        "Junjie Wang",
        "Lei Liang",
        "Wen Zhang"
      ],
      "date": "2025-10-30",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26683v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26683v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.26658v1",
      "title": "The Era of Agentic Organization: Learning to Organize with Language Models",
      "abstract": "We envision a new era of AI, termed agentic organization, where agents solve\ncomplex problems by working collaboratively and concurrently, enabling outcomes\nbeyond individual intelligence. To realize this vision, we introduce\nasynchronous thinking (AsyncThink) as a new paradigm of reasoning with large\nlanguage models, which organizes the internal thinking process into\nconcurrently executable structures. Specifically, we propose a thinking\nprotocol where an organizer dynamically assigns sub-queries to workers, merges\nintermediate knowledge, and produces coherent solutions. More importantly, the\nthinking structure in this protocol can be further optimized through\nreinforcement learning. Experiments demonstrate that AsyncThink achieves 28%\nlower inference latency compared to parallel thinking while improving accuracy\non mathematical reasoning. Moreover, AsyncThink generalizes its learned\nasynchronous thinking capabilities, effectively tackling unseen tasks without\nadditional training.",
      "authors": [
        "Zewen Chi",
        "Li Dong",
        "Qingxiu Dong",
        "Yaru Hao",
        "Xun Wu",
        "Shaohan Huang",
        "Furu Wei"
      ],
      "date": "2025-10-30",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26658v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26658v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.26622v1",
      "title": "Encoder-Decoder or Decoder-Only? Revisiting Encoder-Decoder Large Language Model",
      "abstract": "Recent large language model (LLM) research has undergone an architectural\nshift from encoder-decoder modeling to nowadays the dominant decoder-only\nmodeling. This rapid transition, however, comes without a rigorous comparative\nanalysis especially \\textit{from the scaling perspective}, raising concerns\nthat the potential of encoder-decoder models may have been overlooked. To fill\nthis gap, we revisit encoder-decoder LLM (RedLLM), enhancing it with recent\nrecipes from decoder-only LLM (DecLLM). We conduct a comprehensive comparison\nbetween RedLLM, pretrained with prefix language modeling (LM), and DecLLM,\npretrained with causal LM, at different model scales, ranging from $\\sim$150M\nto $\\sim$8B. Using RedPajama V1 (1.6T tokens) for pretraining and FLAN for\ninstruction tuning, our experiments show that RedLLM produces compelling\nscaling properties and surprisingly strong performance. While DecLLM is overall\nmore compute-optimal during pretraining, RedLLM demonstrates comparable scaling\nand context length extrapolation capabilities. After instruction tuning, RedLLM\nachieves comparable and even better results on various downstream tasks while\nenjoying substantially better inference efficiency. We hope our findings could\ninspire more efforts on re-examining RedLLM, unlocking its potential for\ndeveloping powerful and efficient LLMs.",
      "authors": [
        "Biao Zhang",
        "Yong Cheng",
        "Siamak Shakeri",
        "Xinyi Wang",
        "Min Ma",
        "Orhan Firat"
      ],
      "date": "2025-10-30",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26622v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26622v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.26615v1",
      "title": "SlideAgent: Hierarchical Agentic Framework for Multi-Page Visual Document Understanding",
      "abstract": "Multi-page visual documents such as manuals, brochures, presentations, and\nposters convey key information through layout, colors, icons, and cross-slide\nreferences. While large language models (LLMs) offer opportunities in document\nunderstanding, current systems struggle with complex, multi-page visual\ndocuments, particularly in fine-grained reasoning over elements and pages. We\nintroduce SlideAgent, a versatile agentic framework for understanding\nmulti-modal, multi-page, and multi-layout documents, especially slide decks.\nSlideAgent employs specialized agents and decomposes reasoning into three\nspecialized levels-global, page, and element-to construct a structured,\nquery-agnostic representation that captures both overarching themes and\ndetailed visual or textual cues. During inference, SlideAgent selectively\nactivates specialized agents for multi-level reasoning and integrates their\noutputs into coherent, context-aware answers. Extensive experiments show that\nSlideAgent achieves significant improvement over both proprietary (+7.9\noverall) and open-source models (+9.8 overall).",
      "authors": [
        "Yiqiao Jin",
        "Rachneet Kaur",
        "Zhen Zeng",
        "Sumitra Ganesh",
        "Srijan Kumar"
      ],
      "date": "2025-10-30",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26615v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26615v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.26606v2",
      "title": "Normative Reasoning in Large Language Models: A Comparative Benchmark from Logical and Modal Perspectives",
      "abstract": "Normative reasoning is a type of reasoning that involves normative or deontic\nmodality, such as obligation and permission. While large language models (LLMs)\nhave demonstrated remarkable performance across various reasoning tasks, their\nability to handle normative reasoning remains underexplored. In this paper, we\nsystematically evaluate LLMs' reasoning capabilities in the normative domain\nfrom both logical and modal perspectives. Specifically, to assess how well LLMs\nreason with normative modals, we make a comparison between their reasoning with\nnormative modals and their reasoning with epistemic modals, which share a\ncommon formal structure. To this end, we introduce a new dataset covering a\nwide range of formal patterns of reasoning in both normative and epistemic\ndomains, while also incorporating non-formal cognitive factors that influence\nhuman reasoning. Our results indicate that, although LLMs generally adhere to\nvalid reasoning patterns, they exhibit notable inconsistencies in specific\ntypes of normative reasoning and display cognitive biases similar to those\nobserved in psychological studies of human reasoning. These findings highlight\nchallenges in achieving logical consistency in LLMs' normative reasoning and\nprovide insights for enhancing their reliability. All data and code are\nreleased publicly at https://github.com/kmineshima/NeuBAROCO.",
      "authors": [
        "Kentaro Ozeki",
        "Risako Ando",
        "Takanobu Morishita",
        "Hirohiko Abe",
        "Koji Mineshima",
        "Mitsuhiro Okada"
      ],
      "date": "2025-10-30",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26606v2",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26606v2"
    },
    {
      "url": "https://arxiv.org/abs/2510.26577v1",
      "title": "Inference-Cost-Aware Dynamic Tree Construction for Efficient Inference in Large Language Models",
      "abstract": "Large Language Models (LLMs) face significant inference latency challenges\nstemming from their autoregressive design and large size. To address this,\nspeculative decoding emerges as a solution, enabling the simultaneous\ngeneration and validation of multiple tokens. While recent approaches like\nEAGLE-2 and EAGLE-3 improve speculative decoding using dynamic tree structures,\nthey often neglect the impact of crucial system variables such as GPU devices\nand batch sizes.\n  Therefore, we introduce a new dynamic tree decoding approach called CAST that\ntakes into account inference costs, including factors such as GPU\nconfigurations and batch sizes, to dynamically refine the tree structure.\nThrough comprehensive experimentation across six diverse tasks and utilizing\nsix distinct LLMs, our methodology demonstrates remarkable results, achieving\nspeeds up to 5.2 times faster than conventional decoding methods. Moreover, it\ngenerally outperforms existing state-of-the-art techniques from 5% to 20%.",
      "authors": [
        "Yinrong Hong",
        "Zhiquan Tan",
        "Kai Hu"
      ],
      "date": "2025-10-30",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26577v1",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26577v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.26575v1",
      "title": "InfoFlow: Reinforcing Search Agent Via Reward Density Optimization",
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) is a promising approach\nfor enhancing agentic deep search. However, its application is often hindered\nby low \\textbf{Reward Density} in deep search scenarios, where agents expend\nsignificant exploratory costs for infrequent and often null final rewards. In\nthis paper, we formalize this challenge as the \\textbf{Reward Density\nOptimization} problem, which aims to improve the reward obtained per unit of\nexploration cost. This paper introduce \\textbf{InfoFlow}, a systematic\nframework that tackles this problem from three aspects. 1) \\textbf{Subproblem\ndecomposition}: breaking down long-range tasks to assign process rewards,\nthereby providing denser learning signals. 2) \\textbf{Failure-guided hints}:\ninjecting corrective guidance into stalled trajectories to increase the\nprobability of successful outcomes. 3) \\textbf{Dual-agent refinement}:\nemploying a dual-agent architecture to offload the cognitive burden of deep\nexploration. A refiner agent synthesizes the search history, which effectively\ncompresses the researcher's perceived trajectory, thereby reducing exploration\ncost and increasing the overall reward density. We evaluate InfoFlow on\nmultiple agentic search benchmarks, where it significantly outperforms strong\nbaselines, enabling lightweight LLMs to achieve performance comparable to\nadvanced proprietary LLMs.",
      "authors": [
        "Kun Luo",
        "Hongjin Qian",
        "Zheng Liu",
        "Ziyi Xia",
        "Shitao Xiao",
        "Siqi Bao",
        "Jun Zhao",
        "Kang Liu"
      ],
      "date": "2025-10-30",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26575v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26575v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.26543v1",
      "title": "The Structure of Relation Decoding Linear Operators in Large Language Models",
      "abstract": "This paper investigates the structure of linear operators introduced in\nHernandez et al. [2023] that decode specific relational facts in transformer\nlanguage models. We extend their single-relation findings to a collection of\nrelations and systematically chart their organization. We show that such\ncollections of relation decoders can be highly compressed by simple order-3\ntensor networks without significant loss in decoding accuracy. To explain this\nsurprising redundancy, we develop a cross-evaluation protocol, in which we\napply each linear decoder operator to the subjects of every other relation. Our\nresults reveal that these linear maps do not encode distinct relations, but\nextract recurring, coarse-grained semantic properties (e.g., country of capital\ncity and country of food are both in the country-of-X property). This\nproperty-centric structure clarifies both the operators' compressibility and\nhighlights why they generalize only to new relations that are semantically\nclose. Our findings thus interpret linear relational decoding in transformer\nlanguage models as primarily property-based, rather than relation-specific.",
      "authors": [
        "Miranda Anna Christ",
        "Adrián Csiszárik",
        "Gergely Becsó",
        "Dániel Varga"
      ],
      "date": "2025-10-30",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26543v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26543v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.26521v1",
      "title": "Hebrew Diacritics Restoration using Visual Representation",
      "abstract": "Diacritics restoration in Hebrew is a fundamental task for ensuring accurate\nword pronunciation and disambiguating textual meaning. Despite the language's\nhigh degree of ambiguity when unvocalized, recent machine learning approaches\nhave significantly advanced performance on this task.\n  In this work, we present DIVRIT, a novel system for Hebrew diacritization\nthat frames the task as a zero-shot classification problem. Our approach\noperates at the word level, selecting the most appropriate diacritization\npattern for each undiacritized word from a dynamically generated candidate set,\nconditioned on the surrounding textual context. A key innovation of DIVRIT is\nits use of a Hebrew Visual Language Model, which processes undiacritized text\nas an image, allowing diacritic information to be embedded directly within the\ninput's vector representation.\n  Through a comprehensive evaluation across various configurations, we\ndemonstrate that the system effectively performs diacritization without relying\non complex, explicit linguistic analysis. Notably, in an ``oracle'' setting\nwhere the correct diacritized form is guaranteed to be among the provided\ncandidates, DIVRIT achieves a high level of accuracy. Furthermore, strategic\narchitectural enhancements and optimized training methodologies yield\nsignificant improvements in the system's overall generalization capabilities.\nThese findings highlight the promising potential of visual representations for\naccurate and automated Hebrew diacritization.",
      "authors": [
        "Yair Elboher",
        "Yuval Pinter"
      ],
      "date": "2025-10-30",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26521v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26521v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.26512v1",
      "title": "Inside CORE-KG: Evaluating Structured Prompting and Coreference Resolution for Knowledge Graphs",
      "abstract": "Human smuggling networks are increasingly adaptive and difficult to analyze.\nLegal case documents offer critical insights but are often unstructured,\nlexically dense, and filled with ambiguous or shifting references, which pose\nsignificant challenges for automated knowledge graph (KG) construction. While\nrecent LLM-based approaches improve over static templates, they still generate\nnoisy, fragmented graphs with duplicate nodes due to the absence of guided\nextraction and coreference resolution. The recently proposed CORE-KG framework\naddresses these limitations by integrating a type-aware coreference module and\ndomain-guided structured prompts, significantly reducing node duplication and\nlegal noise. In this work, we present a systematic ablation study of CORE-KG to\nquantify the individual contributions of its two key components. Our results\nshow that removing coreference resolution results in a 28.32% increase in node\nduplication and a 4.32% increase in noisy nodes, while removing structured\nprompts leads to a 4.34% increase in node duplication and a 73.33% increase in\nnoisy nodes. These findings offer empirical insights for designing robust\nLLM-based pipelines for extracting structured representations from complex\nlegal texts.",
      "authors": [
        "Dipak Meher",
        "Carlotta Domeniconi"
      ],
      "date": "2025-10-30",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26512v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26512v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.26498v1",
      "title": "A Multi-agent Large Language Model Framework to Automatically Assess Performance of a Clinical AI Triage Tool",
      "abstract": "Purpose: The purpose of this study was to determine if an ensemble of\nmultiple LLM agents could be used collectively to provide a more reliable\nassessment of a pixel-based AI triage tool than a single LLM.\n  Methods: 29,766 non-contrast CT head exams from fourteen hospitals were\nprocessed by a commercial intracranial hemorrhage (ICH) AI detection tool.\nRadiology reports were analyzed by an ensemble of eight open-source LLM models\nand a HIPAA compliant internal version of GPT-4o using a single multi-shot\nprompt that assessed for presence of ICH. 1,726 examples were manually\nreviewed. Performance characteristics of the eight open-source models and\nconsensus were compared to GPT-4o. Three ideal consensus LLM ensembles were\ntested for rating the performance of the triage tool.\n  Results: The cohort consisted of 29,766 head CTs exam-report pairs. The\nhighest AUC performance was achieved with llama3.3:70b and GPT-4o (AUC= 0.78).\nThe average precision was highest for Llama3.3:70b and GPT-4o (AP=0.75 & 0.76).\nLlama3.3:70b had the highest F1 score (0.81) and recall (0.85), greater\nprecision (0.78), specificity (0.72), and MCC (0.57). Using MCC (95% CI) the\nideal combination of LLMs were: Full-9 Ensemble 0.571 (0.552-0.591), Top-3\nEnsemble 0.558 (0.537-0.579), Consensus 0.556 (0.539-0.574), and GPT4o 0.522\n(0.500-0.543). No statistically significant differences were observed between\nTop-3, Full-9, and Consensus (p > 0.05).\n  Conclusion: An ensemble of medium to large sized open-source LLMs provides a\nmore consistent and reliable method to derive a ground truth retrospective\nevaluation of a clinical AI triage tool over a single LLM alone.",
      "authors": [
        "Adam E. Flanders",
        "Yifan Peng",
        "Luciano Prevedello",
        "Robyn Ball",
        "Errol Colak",
        "Prahlad Menon",
        "George Shih",
        "Hui-Ming Lin",
        "Paras Lakhani"
      ],
      "date": "2025-10-30",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26498v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26498v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.26495v1",
      "title": "Rethinking Text-to-SQL: Dynamic Multi-turn SQL Interaction for Real-world Database Exploration",
      "abstract": "Recent advances in Text-to-SQL have achieved strong results in static,\nsingle-turn tasks, where models generate SQL queries from natural language\nquestions. However, these systems fall short in real-world interactive\nscenarios, where user intents evolve and queries must be refined over multiple\nturns. In applications such as finance and business analytics, users\niteratively adjust query constraints or dimensions based on intermediate\nresults. To evaluate such dynamic capabilities, we introduce DySQL-Bench, a\nbenchmark assessing model performance under evolving user interactions. Unlike\nprevious manually curated datasets, DySQL-Bench is built through an automated\ntwo-stage pipeline of task synthesis and verification. Structured tree\nrepresentations derived from raw database tables guide LLM-based task\ngeneration, followed by interaction-oriented filtering and expert validation.\nHuman evaluation confirms 100% correctness of the synthesized data. We further\npropose a multi-turn evaluation framework simulating realistic interactions\namong an LLM-simulated user, the model under test, and an executable database.\nThe model must adapt its reasoning and SQL generation as user intents change.\nDySQL-Bench covers 13 domains across BIRD and Spider 2 databases, totaling\n1,072 tasks. Even GPT-4o attains only 58.34% overall accuracy and 23.81% on the\nPass@5 metric, underscoring the benchmark's difficulty. All code and data are\nreleased at https://github.com/Aurora-slz/Real-World-SQL-Bench .",
      "authors": [
        "Linzhuang Sun",
        "Tianyu Guo",
        "Hao Liang",
        "Yuying Li",
        "Qifeng Cai",
        "Jingxuan Wei",
        "Bihui Yu",
        "Wentao Zhang",
        "Bin Cui"
      ],
      "date": "2025-10-30",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26495v1",
      "categories": [
        "cs.DB",
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26495v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.26493v1",
      "title": "Context Engineering 2.0: The Context of Context Engineering",
      "abstract": "Karl Marx once wrote that ``the human essence is the ensemble of social\nrelations'', suggesting that individuals are not isolated entities but are\nfundamentally shaped by their interactions with other entities, within which\ncontexts play a constitutive and essential role. With the advent of computers\nand artificial intelligence, these contexts are no longer limited to purely\nhuman--human interactions: human--machine interactions are included as well.\nThen a central question emerges: How can machines better understand our\nsituations and purposes? To address this challenge, researchers have recently\nintroduced the concept of context engineering. Although it is often regarded as\na recent innovation of the agent era, we argue that related practices can be\ntraced back more than twenty years. Since the early 1990s, the field has\nevolved through distinct historical phases, each shaped by the intelligence\nlevel of machines: from early human--computer interaction frameworks built\naround primitive computers, to today's human--agent interaction paradigms\ndriven by intelligent agents, and potentially to human--level or superhuman\nintelligence in the future. In this paper, we situate context engineering,\nprovide a systematic definition, outline its historical and conceptual\nlandscape, and examine key design considerations for practice. By addressing\nthese questions, we aim to offer a conceptual foundation for context\nengineering and sketch its promising future. This paper is a stepping stone for\na broader community effort toward systematic context engineering in AI systems.",
      "authors": [
        "Qishuo Hua",
        "Lyumanshan Ye",
        "Dayuan Fu",
        "Yang Xiao",
        "Xiaojie Cai",
        "Yunze Wu",
        "Jifan Lin",
        "Junfei Wang",
        "Pengfei Liu"
      ],
      "date": "2025-10-30",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26493v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26493v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.26484v1",
      "title": "Bayesian Network Fusion of Large Language Models for Sentiment Analysis",
      "abstract": "Large language models (LLMs) continue to advance, with an increasing number\nof domain-specific variants tailored for specialised tasks. However, these\nmodels often lack transparency and explainability, can be costly to fine-tune,\nrequire substantial prompt engineering, yield inconsistent results across\ndomains, and impose significant adverse environmental impact due to their high\ncomputational demands. To address these challenges, we propose the Bayesian\nnetwork LLM fusion (BNLF) framework, which integrates predictions from three\nLLMs, including FinBERT, RoBERTa, and BERTweet, through a probabilistic\nmechanism for sentiment analysis. BNLF performs late fusion by modelling the\nsentiment predictions from multiple LLMs as probabilistic nodes within a\nBayesian network. Evaluated across three human-annotated financial corpora with\ndistinct linguistic and contextual characteristics, BNLF demonstrates\nconsistent gains of about six percent in accuracy over the baseline LLMs,\nunderscoring its robustness to dataset variability and the effectiveness of\nprobabilistic fusion for interpretable sentiment classification.",
      "authors": [
        "Rasoul Amirzadeh",
        "Dhananjay Thiruvady",
        "Fatemeh Shiri"
      ],
      "date": "2025-10-30",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26484v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26484v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.26474v1",
      "title": "Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing",
      "abstract": "Self-improvement has emerged as a mainstream paradigm for advancing the\nreasoning capabilities of large vision-language models (LVLMs), where models\nexplore and learn from successful trajectories iteratively. However, we\nidentify a critical issue during this process: the model excels at generating\nhigh-quality trajectories for simple queries (i.e., head data) but struggles\nwith more complex ones (i.e., tail data). This leads to an imbalanced\noptimization that drives the model to prioritize simple reasoning skills, while\nhindering its ability to tackle more complex reasoning tasks. Over iterations,\nthis imbalance becomes increasingly pronounced--a dynamic we term the \"Matthew\neffect\"--which ultimately hinders further model improvement and leads to\nperformance bottlenecks. To counteract this challenge, we introduce four\nefficient strategies from two perspectives: distribution-reshaping and\ntrajectory-resampling, to achieve head-tail re-balancing during the\nexploration-and-learning self-improvement process. Extensive experiments on\nQwen2-VL-7B-Instruct and InternVL2.5-4B models across visual reasoning tasks\ndemonstrate that our methods consistently improve visual reasoning\ncapabilities, outperforming vanilla self-improvement by 3.86 points on average.",
      "authors": [
        "Xin Guo",
        "Zhiheng Xi",
        "Yiwen Ding",
        "Yitao Zhai",
        "Xiaowei Shi",
        "Xunliang Cai",
        "Tao Gui",
        "Qi Zhang",
        "Xuanjing Huang"
      ],
      "date": "2025-10-30",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26474v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26474v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.26457v1",
      "title": "SecureReviewer: Enhancing Large Language Models for Secure Code Review through Secure-aware Fine-tuning",
      "abstract": "Identifying and addressing security issues during the early phase of the\ndevelopment lifecycle is critical for mitigating the long-term negative impacts\non software systems. Code review serves as an effective practice that enables\ndevelopers to check their teammates' code before integration into the codebase.\nTo streamline the generation of review comments, various automated code review\napproaches have been proposed, where LLM-based methods have significantly\nadvanced the capabilities of automated review generation. However, existing\nmodels primarily focus on general-purpose code review, their effectiveness in\nidentifying and addressing security-related issues remains underexplored.\nMoreover, adapting existing code review approaches to target security issues\nfaces substantial challenges, including data scarcity and inadequate evaluation\nmetrics. To address these limitations, we propose SecureReviewer, a new\napproach designed for enhancing LLMs' ability to identify and resolve\nsecurity-related issues during code review. Specifically, we first construct a\ndataset tailored for training and evaluating secure code review capabilities.\nLeveraging this dataset, we fine-tune LLMs to generate code review comments\nthat can effectively identify security issues and provide fix suggestions with\nour proposed secure-aware fine-tuning strategy. To mitigate hallucination in\nLLMs and enhance the reliability of their outputs, we integrate the RAG\ntechnique, which grounds the generated comments in domain-specific security\nknowledge. Additionally, we introduce SecureBLEU, a new evaluation metric\ndesigned to assess the effectiveness of review comments in addressing security\nissues. Experimental results demonstrate that SecureReviewer outperforms\nstate-of-the-art baselines in both security issue detection accuracy and the\noverall quality and practical utility of generated review comments.",
      "authors": [
        "Fang Liu",
        "Simiao Liu",
        "Yinghao Zhu",
        "Xiaoli Lian",
        "Li Zhang"
      ],
      "date": "2025-10-30",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26457v1",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26457v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.26446v1",
      "title": "1+1>2: A Synergistic Sparse and Low-Rank Compression Method for Large Language Models",
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable proficiency in\nlanguage comprehension and generation; however, their widespread adoption is\nconstrained by substantial bandwidth and computational demands. While pruning\nand low-rank approximation have each demonstrated promising performance\nindividually, their synergy for LLMs remains underexplored. We introduce\n\\underline{S}ynergistic \\underline{S}parse and \\underline{L}ow-Rank\n\\underline{C}ompression (SSLC) methods for LLMs, which leverages the strengths\nof both techniques: low-rank approximation compresses the model by retaining\nits essential structure with minimal information loss, whereas sparse\noptimization eliminates non-essential weights, preserving those crucial for\ngeneralization. Based on theoretical analysis, we first formulate the low-rank\napproximation and sparse optimization as a unified problem and solve it by\niterative optimization algorithm. Experiments on LLaMA and Qwen2.5 models\n(7B-70B) show that SSLC, without any additional training steps, consistently\nsurpasses standalone methods, achieving state-of-the-arts results. Notably,\nSSLC compresses Qwen2.5 by 50\\% with no performance drop and achieves at least\n1.63$\\times$ speedup, offering a practical solution for efficient LLM\ndeployment.",
      "authors": [
        "Zeliang Zong",
        "Kai Zhang",
        "Zheyang Li",
        "Wenming Tan",
        "Ye Ren",
        "Yiyan Zhai",
        "Jilin Hu"
      ],
      "date": "2025-10-30",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26446v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26446v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.26423v1",
      "title": "Nexus: Execution-Grounded Multi-Agent Test Oracle Synthesis",
      "abstract": "Test oracle generation in non-regression testing is a longstanding challenge\nin software engineering, where the goal is to produce oracles that can\naccurately determine whether a function under test (FUT) behaves as intended\nfor a given input. In this paper, we introduce Nexus, a novel multi-agent\nframework to address this challenge. Nexus generates test oracles by leveraging\na diverse set of specialized agents that synthesize test oracles through a\nstructured process of deliberation, validation, and iterative self-refinement.\nDuring the deliberation phase, a panel of four specialist agents, each\nembodying a distinct testing philosophy, collaboratively critiques and refines\nan initial set of test oracles. Then, in the validation phase, Nexus generates\na plausible candidate implementation of the FUT and executes the proposed\noracles against it in a secure sandbox. For any oracle that fails this\nexecution-based check, Nexus activates an automated selfrefinement loop, using\nthe specific runtime error to debug and correct the oracle before\nre-validation. Our extensive evaluation on seven diverse benchmarks\ndemonstrates that Nexus consistently and substantially outperforms\nstate-of-theart baselines. For instance, Nexus improves the test-level oracle\naccuracy on the LiveCodeBench from 46.30% to 57.73% for GPT-4.1-Mini. The\nimproved accuracy also significantly enhances downstream tasks: the bug\ndetection rate of GPT4.1-Mini generated test oracles on HumanEval increases\nfrom 90.91% to 95.45% for Nexus compared to baselines, and the success rate of\nautomated program repair improves from 35.23% to 69.32%.",
      "authors": [
        "Dong Huang",
        "Mingzhe Du",
        "Jie M. Zhang",
        "Zheng Lin",
        "Meng Luo",
        "Qianru Zhang",
        "See-Kiong Ng"
      ],
      "date": "2025-10-30",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26423v1",
      "categories": [
        "cs.SE",
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26423v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.26422v1",
      "title": "OmniEduBench: A Comprehensive Chinese Benchmark for Evaluating Large Language Models in Education",
      "abstract": "With the rapid development of large language models (LLMs), various LLM-based\nworks have been widely applied in educational fields. However, most existing\nLLMs and their benchmarks focus primarily on the knowledge dimension, largely\nneglecting the evaluation of cultivation capabilities that are essential for\nreal-world educational scenarios. Additionally, current benchmarks are often\nlimited to a single subject or question type, lacking sufficient diversity.\nThis issue is particularly prominent within the Chinese context. To address\nthis gap, we introduce OmniEduBench, a comprehensive Chinese educational\nbenchmark. OmniEduBench consists of 24.602K high-quality question-answer pairs.\nThe data is meticulously divided into two core dimensions: the knowledge\ndimension and the cultivation dimension, which contain 18.121K and 6.481K\nentries, respectively. Each dimension is further subdivided into 6 fine-grained\ncategories, covering a total of 61 different subjects (41 in the knowledge and\n20 in the cultivation). Furthermore, the dataset features a rich variety of\nquestion formats, including 11 common exam question types, providing a solid\nfoundation for comprehensively evaluating LLMs' capabilities in education.\nExtensive experiments on 11 mainstream open-source and closed-source LLMs\nreveal a clear performance gap. In the knowledge dimension, only Gemini-2.5 Pro\nsurpassed 60\\% accuracy, while in the cultivation dimension, the\nbest-performing model, QWQ, still trailed human intelligence by nearly 30\\%.\nThese results highlight the substantial room for improvement and underscore the\nchallenges of applying LLMs in education.",
      "authors": [
        "Min Zhang",
        "Hao Chen",
        "Hao Chen",
        "Wenqi Zhang",
        "Didi Zhu",
        "Xin Lin",
        "Bo Jiang",
        "Aimin Zhou",
        "Fei Wu",
        "Kun Kuang"
      ],
      "date": "2025-10-30",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26422v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26422v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.26354v1",
      "title": "On the Role of Context for Discourse Relation Classification in Scientific Writing",
      "abstract": "With the increasing use of generative Artificial Intelligence (AI) methods to\nsupport science workflows, we are interested in the use of discourse-level\ninformation to find supporting evidence for AI generated scientific claims. A\nfirst step towards this objective is to examine the task of inferring discourse\nstructure in scientific writing.\n  In this work, we present a preliminary investigation of pretrained language\nmodel (PLM) and Large Language Model (LLM) approaches for Discourse Relation\nClassification (DRC), focusing on scientific publications, an under-studied\ngenre for this task. We examine how context can help with the DRC task, with\nour experiments showing that context, as defined by discourse structure, is\ngenerally helpful. We also present an analysis of which scientific discourse\nrelation types might benefit most from context.",
      "authors": [
        "Stephen Wan",
        "Wei Liu",
        "Michael Strube"
      ],
      "date": "2025-10-30",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26354v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26354v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.26352v1",
      "title": "The Geometry of Dialogue: Graphing Language Models to Reveal Synergistic Teams for Multi-Agent Collaboration",
      "abstract": "While a multi-agent approach based on large language models (LLMs) represents\na promising strategy to surpass the capabilities of single models, its success\nis critically dependent on synergistic team composition. However, forming\noptimal teams is a significant challenge, as the inherent opacity of most\nmodels obscures the internal characteristics necessary for effective\ncollaboration. In this paper, we propose an interaction-centric framework for\nautomatic team composition that does not require any prior knowledge including\ntheir internal architectures, training data, or task performances. Our method\nconstructs a \"language model graph\" that maps relationships between models from\nthe semantic coherence of pairwise conversations, and then applies community\ndetection to identify synergistic model clusters. Our experiments with diverse\nLLMs demonstrate that the proposed method discovers functionally coherent\ngroups that reflect their latent specializations. Priming conversations with\nspecific topics identified synergistic teams which outperform random baselines\non downstream benchmarks and achieve comparable accuracy to that of\nmanually-curated teams based on known model specializations. Our findings\nprovide a new basis for the automated design of collaborative multi-agent LLM\nteams.",
      "authors": [
        "Kotaro Furuya",
        "Yuichi Kitagawa"
      ],
      "date": "2025-10-30",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26352v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.MA"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26352v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.26345v1",
      "title": "MisSynth: Improving MISSCI Logical Fallacies Classification with Synthetic Data",
      "abstract": "Health-related misinformation is very prevalent and potentially harmful. It\nis difficult to identify, especially when claims distort or misinterpret\nscientific findings. We investigate the impact of synthetic data generation and\nlightweight fine-tuning techniques on the ability of large language models\n(LLMs) to recognize fallacious arguments using the MISSCI dataset and\nframework. In this work, we propose MisSynth, a pipeline that applies\nretrieval-augmented generation (RAG) to produce synthetic fallacy samples,\nwhich are then used to fine-tune an LLM model. Our results show substantial\naccuracy gains with fine-tuned models compared to vanilla baselines. For\ninstance, the LLaMA 3.1 8B fine-tuned model achieved an over 35% F1-score\nabsolute improvement on the MISSCI test split over its vanilla baseline. We\ndemonstrate that introducing synthetic fallacy data to augment limited\nannotated resources can significantly enhance zero-shot LLM classification\nperformance on real-world scientific misinformation tasks, even with limited\ncomputational resources. The code and synthetic dataset are available on\nhttps://github.com/mxpoliakov/MisSynth.",
      "authors": [
        "Mykhailo Poliakov",
        "Nadiya Shvai"
      ],
      "date": "2025-10-30",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26345v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26345v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.26336v1",
      "title": "From Amateur to Master: Infusing Knowledge into LLMs via Automated Curriculum Learning",
      "abstract": "Large Language Models (LLMs) excel at general tasks but underperform in\nspecialized domains like economics and psychology, which require deep,\nprincipled understanding. To address this, we introduce ACER (Automated\nCurriculum-Enhanced Regimen) that transforms generalist models into domain\nexperts without sacrificing their broad capabilities. ACER first synthesizes a\ncomprehensive, textbook-style curriculum by generating a table of contents for\na subject and then creating question-answer (QA) pairs guided by Bloom's\ntaxonomy. This ensures systematic topic coverage and progressively increasing\ndifficulty. The resulting synthetic corpus is used for continual pretraining\nwith an interleaved curriculum schedule, aligning learning across both content\nand cognitive dimensions.\n  Experiments with Llama 3.2 (1B and 3B) show significant gains in specialized\nMMLU subsets. In challenging domains like microeconomics, where baselines\nstruggle, ACER boosts accuracy by 5 percentage points. Across all target\ndomains, we observe a consistent macro-average improvement of 3 percentage\npoints. Notably, ACER not only prevents catastrophic forgetting but also\nfacilitates positive cross-domain knowledge transfer, improving performance on\nnon-target domains by 0.7 points. Beyond MMLU, ACER enhances performance on\nknowledge-intensive benchmarks like ARC and GPQA by over 2 absolute points,\nwhile maintaining stable performance on general reasoning tasks. Our results\ndemonstrate that ACER offers a scalable and effective recipe for closing\ncritical domain gaps in LLMs.",
      "authors": [
        "Nishit Neema",
        "Srinjoy Mukherjee",
        "Sapan Shah",
        "Gokul Ramakrishnan",
        "Ganesh Venkatesh"
      ],
      "date": "2025-10-30",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26336v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26336v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.26322v1",
      "title": "SCRIBE: Structured Chain Reasoning for Interactive Behaviour Explanations using Tool Calling",
      "abstract": "Language models can be used to provide interactive, personalized student\nfeedback in educational settings. However, real-world deployment faces three\nkey challenges: privacy concerns, limited computational resources, and the need\nfor pedagogically valid responses. These constraints require small, open-source\nmodels that can run locally and reliably ground their outputs in correct\ninformation. We introduce SCRIBE, a framework for multi-hop, tool-augmented\nreasoning designed to generate valid responses to student questions about\nfeedback reports. SCRIBE combines domain-specific tools with a self-reflective\ninference pipeline that supports iterative reasoning, tool use, and error\nrecovery. We distil these capabilities into 3B and 8B models via two-stage LoRA\nfine-tuning on synthetic GPT-4o-generated data. Evaluation with a human-aligned\nGPT-Judge and a user study with 108 students shows that 8B-SCRIBE models\nachieve comparable or superior quality to much larger models in key dimensions\nsuch as relevance and actionability, while being perceived on par with GPT-4o\nand Llama-3.3 70B by students. These findings demonstrate the viability of\nSCRIBE for low-resource, privacy-sensitive educational applications.",
      "authors": [
        "Fares Fawzi",
        "Vinitra Swamy",
        "Dominik Glandorf",
        "Tanya Nazaretsky",
        "Tanja Käser"
      ],
      "date": "2025-10-30",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26322v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26322v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.26298v1",
      "title": "Can Agent Conquer Web? Exploring the Frontiers of ChatGPT Atlas Agent in Web Games",
      "abstract": "OpenAI's ChatGPT Atlas introduces new capabilities for web interaction,\nenabling the model to analyze webpages, process user intents, and execute\ncursor and keyboard inputs directly within the browser. While its capacity for\ninformation retrieval tasks has been demonstrated, its performance in dynamic,\ninteractive environments remains less explored. In this study, we conduct an\nearly evaluation of Atlas's web interaction capabilities using browser-based\ngames as test scenarios, including Google's T-Rex Runner, Sudoku, Flappy Bird,\nand Stein.world. We employ in-game performance scores as quantitative metrics\nto assess performance across different task types. Our results show that Atlas\nperforms strongly in logical reasoning tasks like Sudoku, completing puzzles\nsignificantly faster than human baselines, but struggles substantially in\nreal-time games requiring precise timing and motor control, often failing to\nprogress beyond initial obstacles. These findings suggest that while Atlas\ndemonstrates capable analytical processing, there remain notable limitations in\ndynamic web environments requiring real-time interaction. The website of our\nproject can be found at https://atlas-game-eval.github.io.",
      "authors": [
        "Jingran Zhang",
        "Ning Li",
        "Justin Cui"
      ],
      "date": "2025-10-30",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26298v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26298v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.26285v1",
      "title": "Unravelling the Mechanisms of Manipulating Numbers in Language Models",
      "abstract": "Recent work has shown that different large language models (LLMs) converge to\nsimilar and accurate input embedding representations for numbers. These\nfindings conflict with the documented propensity of LLMs to produce erroneous\noutputs when dealing with numeric information. In this work, we aim to explain\nthis conflict by exploring how language models manipulate numbers and quantify\nthe lower bounds of accuracy of these mechanisms. We find that despite\nsurfacing errors, different language models learn interchangeable\nrepresentations of numbers that are systematic, highly accurate and universal\nacross their hidden states and the types of input contexts. This allows us to\ncreate universal probes for each LLM and to trace information -- including the\ncauses of output errors -- to specific layers. Our results lay a fundamental\nunderstanding of how pre-trained LLMs manipulate numbers and outline the\npotential of more accurate probing techniques in addressed refinements of LLMs'\narchitectures.",
      "authors": [
        "Michal Štefánik",
        "Timothee Mickus",
        "Marek Kadlčík",
        "Bertram Højer",
        "Michal Spiegel",
        "Raúl Vázquez",
        "Aman Sinha",
        "Josef Kuchař",
        "Philipp Mondorf"
      ],
      "date": "2025-10-30",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26285v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.NE"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26285v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.26277v1",
      "title": "Do LLMs Signal When They're Right? Evidence from Neuron Agreement",
      "abstract": "Large language models (LLMs) commonly boost reasoning via\nsample-evaluate-ensemble decoders, achieving label free gains without ground\ntruth. However, prevailing strategies score candidates using only external\noutputs such as token probabilities, entropies, or self evaluations, and these\nsignals can be poorly calibrated after post training. We instead analyze\ninternal behavior based on neuron activations and uncover three findings: (1)\nexternal signals are low dimensional projections of richer internal dynamics;\n(2) correct responses activate substantially fewer unique neurons than\nincorrect ones throughout generation; and (3) activations from correct\nresponses exhibit stronger cross sample agreement, whereas incorrect ones\ndiverge. Motivated by these observations, we propose Neuron Agreement Decoding\n(NAD), an unsupervised best-of-N method that selects candidates using\nactivation sparsity and cross sample neuron agreement, operating solely on\ninternal signals and without requiring comparable textual outputs. NAD enables\nearly correctness prediction within the first 32 generated tokens and supports\naggressive early stopping. Across math and science benchmarks with verifiable\nanswers, NAD matches majority voting; on open ended coding benchmarks where\nmajority voting is inapplicable, NAD consistently outperforms Avg@64. By\npruning unpromising trajectories early, NAD reduces token usage by 99% with\nminimal loss in generation quality, showing that internal signals provide\nreliable, scalable, and efficient guidance for label free ensemble decoding.",
      "authors": [
        "Kang Chen",
        "Yaoning Wang",
        "Kai Xiong",
        "Zhuoka Feng",
        "Wenhe Sun",
        "Haotian Chen",
        "Yixin Cao"
      ],
      "date": "2025-10-30",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26277v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26277v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.26274v1",
      "title": "PVMark: Enabling Public Verifiability for LLM Watermarking Schemes",
      "abstract": "Watermarking schemes for large language models (LLMs) have been proposed to\nidentify the source of the generated text, mitigating the potential threats\nemerged from model theft. However, current watermarking solutions hardly\nresolve the trust issue: the non-public watermark detection cannot prove itself\nfaithfully conducting the detection. We observe that it is attributed to the\nsecret key mostly used in the watermark detection -- it cannot be public, or\nthe adversary may launch removal attacks provided the key; nor can it be\nprivate, or the watermarking detection is opaque to the public. To resolve the\ndilemma, we propose PVMark, a plugin based on zero-knowledge proof (ZKP),\nenabling the watermark detection process to be publicly verifiable by third\nparties without disclosing any secret key. PVMark hinges upon the proof of\n`correct execution' of watermark detection on which a set of ZKP constraints\nare built, including mapping, random number generation, comparison, and\nsummation. We implement multiple variants of PVMark in Python, Rust and Circom,\ncovering combinations of three watermarking schemes, three hash functions, and\nfour ZKP protocols, to show our approach effectively works under a variety of\ncircumstances. By experimental results, PVMark efficiently enables public\nverifiability on the state-of-the-art LLM watermarking schemes yet without\ncompromising the watermarking performance, promising to be deployed in\npractice.",
      "authors": [
        "Haohua Duan",
        "Liyao Xiang",
        "Xin Zhang"
      ],
      "date": "2025-10-30",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26274v1",
      "categories": [
        "cs.CR",
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26274v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.26271v1",
      "title": "Distilling Multilingual Vision-Language Models: When Smaller Models Stay Multilingual",
      "abstract": "Vision-language models (VLMs) exhibit uneven performance across languages, a\nproblem that is often exacerbated when the model size is reduced. While\nKnowledge distillation (KD) demonstrates promising results in transferring\nknowledge from larger to smaller VLMs, applying KD in multilingualism is an\nunderexplored area. This paper presents a controlled empirical study of KD\nbehavior across five distillation approaches, isolating their effects on\ncross-lingual representation consistency and downstream performance stability\nunder model compression. We study five distillation formulations across CLIP\nand SigLIP2, and evaluate them on in-domain retrieval and out-of-domain visual\nQA. We find that some configurations preserve or even improve multilingual\nretrieval robustness despite halving model size, but others fail to maintain\ncross-task stability, exposing design-sensitive trade-offs that aggregate\naccuracy alone does not reveal.",
      "authors": [
        "Sukrit Sriratanawilai",
        "Jhayahgrit Thongwat",
        "Romrawin Chumpu",
        "Patomporn Payoungkhamdee",
        "Sarana Nutanong",
        "Peerat Limkonchotiwat"
      ],
      "date": "2025-10-30",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26271v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26271v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.26254v1",
      "title": "Language Models Are Borrowing-Blind: A Multilingual Evaluation of Loanword Identification across 10 Languages",
      "abstract": "Throughout language history, words are borrowed from one language to another\nand gradually become integrated into the recipient's lexicon. Speakers can\noften differentiate these loanwords from native vocabulary, particularly in\nbilingual communities where a dominant language continuously imposes lexical\nitems on a minority language. This paper investigates whether pretrained\nlanguage models, including large language models, possess similar capabilities\nfor loanword identification. We evaluate multiple models across 10 languages.\nDespite explicit instructions and contextual information, our results show that\nmodels perform poorly in distinguishing loanwords from native ones. These\nfindings corroborate previous evidence that modern NLP systems exhibit a bias\ntoward loanwords rather than native equivalents. Our work has implications for\ndeveloping NLP tools for minority languages and supporting language\npreservation in communities under lexical pressure from dominant languages.",
      "authors": [
        "Mérilin Sousa Silva",
        "Sina Ahmadi"
      ],
      "date": "2025-10-30",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26254v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26254v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.26253v1",
      "title": "Pragmatic Theories Enhance Understanding of Implied Meanings in LLMs",
      "abstract": "The ability to accurately interpret implied meanings plays a crucial role in\nhuman communication and language use, and language models are also expected to\npossess this capability. This study demonstrates that providing language models\nwith pragmatic theories as prompts is an effective in-context learning approach\nfor tasks to understand implied meanings. Specifically, we propose an approach\nin which an overview of pragmatic theories, such as Gricean pragmatics and\nRelevance Theory, is presented as a prompt to the language model, guiding it\nthrough a step-by-step reasoning process to derive a final interpretation.\nExperimental results showed that, compared to the baseline, which prompts\nintermediate reasoning without presenting pragmatic theories (0-shot\nChain-of-Thought), our methods enabled language models to achieve up to 9.6\\%\nhigher scores on pragmatic reasoning tasks. Furthermore, we show that even\nwithout explaining the details of pragmatic theories, merely mentioning their\nnames in the prompt leads to a certain performance improvement (around 1-3%) in\nlarger models compared to the baseline.",
      "authors": [
        "Takuma Sato",
        "Seiya Kawano",
        "Koichiro Yoshino"
      ],
      "date": "2025-10-30",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26253v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26253v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.26241v1",
      "title": "Which Way Does Time Flow? A Psychophysics-Grounded Evaluation for Vision-Language Models",
      "abstract": "Modern vision-language models (VLMs) excel at many multimodal tasks, yet\ntheir grasp of temporal information in video remains weak and, crucially,\nunder-evaluated. We probe this gap with a deceptively simple but revealing\nchallenge: judging the arrow of time (AoT)-whether a short clip is played\nforward or backward. We introduce AoT-PsyPhyBENCH, a psychophysically validated\nbenchmark that tests whether VLMs can infer temporal direction in natural\nvideos using the same stimuli and behavioral baselines established for humans.\nOur comprehensive evaluation of open-weight and proprietary, reasoning and\nnon-reasoning VLMs reveals that most models perform near chance, and even the\nbest lag far behind human accuracy on physically irreversible processes (e.g.,\nfree fall, diffusion/explosion) and causal manual actions (division/addition)\nthat humans recognize almost instantly. These results highlight a fundamental\ngap in current multimodal systems: while they capture rich visual-semantic\ncorrelations, they lack the inductive biases required for temporal continuity\nand causal understanding. We release the code and data for AoT-PsyPhyBENCH to\nencourage further progress in the physical and temporal reasoning capabilities\nof VLMs.",
      "authors": [
        "Shiho Matta",
        "Lis Kanashiro Pereira",
        "Peitao Han",
        "Fei Cheng",
        "Shigeru Kitazawa"
      ],
      "date": "2025-10-30",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26241v1",
      "categories": [
        "cs.CV",
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26241v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.26205v1",
      "title": "Towards Global Retrieval Augmented Generation: A Benchmark for Corpus-Level Reasoning",
      "abstract": "Retrieval-augmented generation (RAG) has emerged as a leading approach to\nreducing hallucinations in large language models (LLMs). Current RAG evaluation\nbenchmarks primarily focus on what we call local RAG: retrieving relevant\nchunks from a small subset of documents to answer queries that require only\nlocalized understanding within specific text chunks. However, many real-world\napplications require a fundamentally different capability -- global RAG --\nwhich involves aggregating and analyzing information across entire document\ncollections to derive corpus-level insights (for example, \"What are the top 10\nmost cited papers in 2023?\"). In this paper, we introduce GlobalQA -- the first\nbenchmark specifically designed to evaluate global RAG capabilities, covering\nfour core task types: counting, extremum queries, sorting, and top-k\nextraction. Through systematic evaluation across different models and\nbaselines, we find that existing RAG methods perform poorly on global tasks,\nwith the strongest baseline achieving only 1.51 F1 score. To address these\nchallenges, we propose GlobalRAG, a multi-tool collaborative framework that\npreserves structural coherence through chunk-level retrieval, incorporates\nLLM-driven intelligent filters to eliminate noisy documents, and integrates\naggregation modules for precise symbolic computation. On the Qwen2.5-14B model,\nGlobalRAG achieves 6.63 F1 compared to the strongest baseline's 1.51 F1,\nvalidating the effectiveness of our method.",
      "authors": [
        "Qi Luo",
        "Xiaonan Li",
        "Tingshuo Fan",
        "Xinchi Chen",
        "Xipeng Qiu"
      ],
      "date": "2025-10-30",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26205v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26205v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.26202v1",
      "title": "What's In My Human Feedback? Learning Interpretable Descriptions of Preference Data",
      "abstract": "Human feedback can alter language models in unpredictable and undesirable\nways, as practitioners lack a clear understanding of what feedback data\nencodes. While prior work studies preferences over certain attributes (e.g.,\nlength or sycophancy), automatically extracting relevant features without\npre-specifying hypotheses remains challenging. We introduce What's In My Human\nFeedback? (WIMHF), a method to explain feedback data using sparse autoencoders.\nWIMHF characterizes both (1) the preferences a dataset is capable of measuring\nand (2) the preferences that the annotators actually express. Across 7\ndatasets, WIMHF identifies a small number of human-interpretable features that\naccount for the majority of the preference prediction signal achieved by\nblack-box models. These features reveal a wide diversity in what humans prefer,\nand the role of dataset-level context: for example, users on Reddit prefer\ninformality and jokes, while annotators in HH-RLHF and PRISM disprefer them.\nWIMHF also surfaces potentially unsafe preferences, such as that LMArena users\ntend to vote against refusals, often in favor of toxic content. The learned\nfeatures enable effective data curation: re-labeling the harmful examples in\nArena yields large safety gains (+37%) with no cost to general performance.\nThey also allow fine-grained personalization: on the Community Alignment\ndataset, we learn annotator-specific weights over subjective features that\nimprove preference prediction. WIMHF provides a human-centered analysis method\nfor practitioners to better understand and use preference data.",
      "authors": [
        "Rajiv Movva",
        "Smitha Milli",
        "Sewon Min",
        "Emma Pierson"
      ],
      "date": "2025-10-30",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26202v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26202v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.26200v1",
      "title": "Don't Let It Fade: Preserving Edits in Diffusion Language Models via Token Timestep Allocation",
      "abstract": "While diffusion language models (DLMs) enable fine-grained refinement, their\npractical controllability remains fragile. We identify and formally\ncharacterize a central failure mode called update forgetting, in which uniform\nand context agnostic updates induce token level fluctuations across timesteps,\nerasing earlier semantic edits and disrupting the cumulative refinement\nprocess, thereby degrading fluency and coherence. As this failure originates in\nuniform and context agnostic updates, effective control demands explicit token\nordering. We propose Token Timestep Allocation (TTA), which realizes soft and\nsemantic token ordering via per token timestep schedules: critical tokens are\nfrozen early, while uncertain tokens receive continued refinement. This\ntimestep based ordering can be instantiated as either a fixed policy or an\nadaptive policy driven by task signals, thereby supporting a broad spectrum of\nrefinement strategies. Because it operates purely at inference time, it applies\nuniformly across various DLMs and naturally extends to diverse supervision\nsources. Empirically, TTA improves controllability and fluency: on sentiment\ncontrol, it yields more than 20 percent higher accuracy and nearly halves\nperplexity using less than one fifth the steps; in detoxification, it lowers\nmaximum toxicity (12.2 versus 14.5) and perplexity (26.0 versus 32.0).\nTogether, these results demonstrate that softened ordering via timestep\nallocation is the critical lever for mitigating update forgetting and achieving\nstable and controllable diffusion text generation.",
      "authors": [
        "Woojin Kim",
        "Jaeyoung Do"
      ],
      "date": "2025-10-30",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26200v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26200v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.26193v1",
      "title": "RCScore: Quantifying Response Consistency in Large Language Models",
      "abstract": "Current LLM evaluations often rely on a single instruction template,\noverlooking models' sensitivity to instruction style-a critical aspect for\nreal-world deployments. We present RCScore, a multi-dimensional framework\nquantifying how instruction formulation affects model responses. By\nsystematically transforming benchmark problems into multiple instruction\nstyles, RCScore reveals performance variations undetected by conventional\nmetrics. Our experiments across ten LLMs on four reasoning benchmarks\ndemonstrate that instruction style can shift accuracy by up to 16.7% points. We\nintroduce Cross-Response Similarity (CRS), a method applying RCScore metrics to\nmeasure stylistic self-consistency, and establish its strong correlation with\ntask accuracy, suggesting consistency as a valuable proxy for model\nreliability. Additional findings show that deterministic decoding produces more\nstylistically stable outputs, and model scale correlates positively with\ncross-style consistency. RCScore offers a principled approach to assess\ninstruction robustness.",
      "authors": [
        "Dongjun Jang",
        "Youngchae Ahn",
        "Hyopil Shin"
      ],
      "date": "2025-10-30",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26193v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26193v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.26190v1",
      "title": "SP-MCQA: Evaluating Intelligibility of TTS Beyond the Word Level",
      "abstract": "The evaluation of intelligibility for TTS has reached a bottleneck, as\nexisting assessments heavily rely on word-by-word accuracy metrics such as WER,\nwhich fail to capture the complexity of real-world speech or reflect human\ncomprehension needs. To address this, we propose Spoken-Passage Multiple-Choice\nQuestion Answering, a novel subjective approach evaluating the accuracy of key\ninformation in synthesized speech, and release SP-MCQA-Eval, an 8.76-hour\nnews-style benchmark dataset for SP-MCQA evaluation. Our experiments reveal\nthat low WER does not necessarily guarantee high key-information accuracy,\nexposing a gap between traditional metrics and practical intelligibility.\nSP-MCQA shows that even state-of-the-art (SOTA) models still lack robust text\nnormalization and phonetic accuracy. This work underscores the urgent need for\nhigh-level, more life-like evaluation criteria now that many systems already\nexcel at WER yet may fall short on real-world intelligibility.",
      "authors": [
        "Hitomi Jin Ling Tee",
        "Chaoren Wang",
        "Zijie Zhang",
        "Zhizheng Wu"
      ],
      "date": "2025-10-30",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26190v1",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26190v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.26183v1",
      "title": "Similarity-Distance-Magnitude Language Models",
      "abstract": "We introduce Similarity-Distance-Magnitude (SDM) language models (LMs), which\nare sequence prediction models fine-tuned to maximize the proportion of\ngenerations in the well-calibrated, high-probability region partitioned by a\nfinal-layer SDM activation layer used for binary classification of\ninstruction-following. We demonstrate that existing pre-trained decoder-only\nTransformer LMs can be readily converted into SDM LMs via supervised\nfine-tuning, using the final-layer SDM activation layer during training to\nestimate a change-of-base for a supervised next-token loss over a contrastive\ninput encoding scheme, with additional hard negative examples generated online\nduring training. This results in reduced abstentions (i.e., improved\nstatistical efficiency) compared to strong supervised baselines.",
      "authors": [
        "Allen Schmaltz"
      ],
      "date": "2025-10-30",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26183v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26183v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.26182v1",
      "title": "MossNet: Mixture of State-Space Experts is a Multi-Head Attention",
      "abstract": "Large language models (LLMs) have significantly advanced generative\napplications in natural language processing (NLP). Recent trends in model\narchitectures revolve around efficient variants of transformers or\nstate-space/gated-recurrent models (SSMs, GRMs). However, prevailing\nSSM/GRM-based methods often emulate only a single attention head, potentially\nlimiting their expressiveness. In this work, we propose MossNet, a novel\nmixture-of-state-space-experts architecture that emulates a linear multi-head\nattention (MHA). MossNet leverages a mixture-of-experts (MoE) implementation\nnot only in channel-mixing multi-layered perceptron (MLP) blocks but also in\nthe time-mixing SSM kernels to realize multiple \"attention heads.\" Extensive\nexperiments on language modeling and downstream evaluations show that MossNet\noutperforms both transformer- and SSM-based architectures of similar model size\nand data budgets. Larger variants of MossNet, trained on trillions of tokens,\nfurther confirm its scalability and superior performance. In addition,\nreal-device profiling on a Samsung Galaxy S24 Ultra and an Nvidia A100 GPU\ndemonstrate favorable runtime speed and resource usage compared to similarly\nsized baselines. Our results suggest that MossNet is a compelling new direction\nfor efficient, high-performing recurrent LLM architectures.",
      "authors": [
        "Shikhar Tuli",
        "James Seale Smith",
        "Haris Jeelani",
        "Chi-Heng Lin",
        "Abhishek Patel",
        "Vasili Ramanishka",
        "Yen-Chang Hsu",
        "Hongxia Jin"
      ],
      "date": "2025-10-30",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26182v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26182v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.26167v1",
      "title": "One Model to Critique Them All: Rewarding Agentic Tool-Use via Efficient Reasoning",
      "abstract": "Reward models (RMs) play a critical role in aligning large language models\n(LLMs) with human preferences. Yet in the domain of tool learning, the lack of\nRMs specifically designed for function-calling tasks has limited progress\ntoward more capable agentic AI. We introduce ToolRM, a family of lightweight\ngenerative RMs tailored for general tool-use scenarios. To build these models,\nwe propose a novel pipeline that constructs pairwise preference data using\nrule-based scoring and multidimensional sampling. This yields\nToolPref-Pairwise-30K, a diverse, balanced, and challenging dataset of critique\ntasks that supports reinforcement learning with verifiable feedback. To\nevaluate tool-use RMs, we also introduce TRBench$_{BFCL}$, a benchmark built on\nthe agentic evaluation suite BFCL. Trained on our constructed data, models from\nthe Qwen3-4B/8B series achieve up to 14.28% higher accuracy, substantially\noutperforming frontier models such as Claude 4 and OpenAI o3 in pairwise reward\njudgments. Beyond training objectives, ToolRM generalizes to broader critique\ntasks, including Best-of-N sampling and self-correction. Experiments on\nACEBench highlight its effectiveness and efficiency, enabling inference-time\nscaling and reducing output token usage by over 66%. We release data and model\ncheckpoints to facilitate future research.",
      "authors": [
        "Renhao Li",
        "Jianhong Tu",
        "Yang Su",
        "Hamid Alinejad-Rokny",
        "Derek F. Wong",
        "Junyang Lin",
        "Min Yang"
      ],
      "date": "2025-10-30",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26167v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26167v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.26143v1",
      "title": "Reasoning Curriculum: Bootstrapping Broad LLM Reasoning from Math",
      "abstract": "Reinforcement learning (RL) can elicit strong reasoning in large language\nmodels (LLMs), yet most open efforts focus on math and code. We propose\nReasoning Curriculum, a simple two-stage curriculum that first elicits\nreasoning skills in pretraining-aligned domains such as math, then adapts and\nrefines these skills across other domains via joint RL. Stage 1 performs a\nbrief cold start and then math-only RL with verifiable rewards to develop\nreasoning skills. Stage 2 runs joint RL on mixed-domain data to transfer and\nconsolidate these skills. The curriculum is minimal and backbone-agnostic,\nrequiring no specialized reward models beyond standard verifiability checks.\nEvaluated on Qwen3-4B and Llama-3.1-8B over a multi-domain suite, reasoning\ncurriculum yields consistent gains. Ablations and a cognitive-skill analysis\nindicate that both stages are necessary and that math-first elicitation\nincreases cognitive behaviors important for solving complex problems. Reasoning\nCurriculum provides a compact, easy-to-adopt recipe for general reasoning.",
      "authors": [
        "Bo Pang",
        "Deqian Kong",
        "Silvio Savarese",
        "Caiming Xiong",
        "Yingbo Zhou"
      ],
      "date": "2025-10-30",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26143v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26143v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.26124v1",
      "title": "On the Influence of Discourse Relations in Persuasive Texts",
      "abstract": "This paper investigates the relationship between Persuasion Techniques (PTs)\nand Discourse Relations (DRs) by leveraging Large Language Models (LLMs) and\nprompt engineering. Since no dataset annotated with both PTs and DRs exists, we\ntook the SemEval 2023 Task 3 dataset labelled with 19 PTs as a starting point\nand developed LLM-based classifiers to label each instance of the dataset with\none of the 22 PDTB 3.0 level-2 DRs. In total, four LLMs were evaluated using 10\ndifferent prompts, resulting in 40 unique DR classifiers. Ensemble models using\ndifferent majority-pooling strategies were used to create 5 silver datasets of\ninstances labelled with both persuasion techniques and level-2 PDTB senses. The\nsilver dataset sizes vary from 1,281 instances to 204 instances, depending on\nthe majority pooling technique used. Statistical analysis of these silver\ndatasets shows that six discourse relations (namely Cause, Purpose, Contrast,\nCause+Belief, Concession, and Condition) play a crucial role in persuasive\ntexts, especially in the use of Loaded Language, Exaggeration/Minimisation,\nRepetition and to cast Doubt. This insight can contribute to detecting online\npropaganda and misinformation, as well as to our general understanding of\neffective communication.",
      "authors": [
        "Nawar Turk",
        "Sevag Kaspar",
        "Leila Kosseim"
      ],
      "date": "2025-10-30",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26124v1",
      "categories": [
        "cs.CL",
        "I.2.7; I.2.6"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26124v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.26122v1",
      "title": "Reasoning Path Divergence: A New Metric and Curation Strategy to Unlock LLM Diverse Thinking",
      "abstract": "While Test-Time Scaling (TTS) has proven effective in improving the reasoning\nability of large language models (LLMs), low diversity in model outputs often\nbecomes a bottleneck; this is partly caused by the common \"one problem, one\nsolution\" (1P1S) training practice, which provides a single canonical answer\nand can push models toward a narrow set of reasoning paths. To address this, we\npropose a \"one problem, multiple solutions\" (1PNS) training paradigm that\nexposes the model to a variety of valid reasoning trajectories and thus\nincreases inference diversity. A core challenge for 1PNS is reliably measuring\nsemantic differences between multi-step chains of thought, so we introduce\nReasoning Path Divergence (RPD), a step-level metric that aligns and scores\nLong Chain-of-Thought solutions to capture differences in intermediate\nreasoning. Using RPD, we curate maximally diverse solution sets per problem and\nfine-tune Qwen3-4B-Base. Experiments show that RPD-selected training yields\nmore varied outputs and higher pass@k, with an average +2.80% gain in pass@16\nover a strong 1P1S baseline and a +4.99% gain on AIME24, demonstrating that\n1PNS further amplifies the effectiveness of TTS. Our code is available at\nhttps://github.com/fengjujf/Reasoning-Path-Divergence .",
      "authors": [
        "Feng Ju",
        "Zeyu Qin",
        "Rui Min",
        "Zhitao He",
        "Lingpeng Kong",
        "Yi R. Fung"
      ],
      "date": "2025-10-30",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26122v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26122v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.26101v1",
      "title": "QCoder Benchmark: Bridging Language Generation and Quantum Hardware through Simulator-Based Feedback",
      "abstract": "Large language models (LLMs) have increasingly been applied to automatic\nprogramming code generation. This task can be viewed as a language generation\ntask that bridges natural language, human knowledge, and programming logic.\nHowever, it remains underexplored in domains that require interaction with\nhardware devices, such as quantum programming, where human coders write Python\ncode that is executed on a quantum computer. To address this gap, we introduce\nQCoder Benchmark, an evaluation framework that assesses LLMs on quantum\nprogramming with feedback from simulated hardware devices. Our benchmark offers\ntwo key features. First, it supports evaluation using a quantum simulator\nenvironment beyond conventional Python execution, allowing feedback of\ndomain-specific metrics such as circuit depth, execution time, and error\nclassification, which can be used to guide better generation. Second, it\nincorporates human-written code submissions collected from real programming\ncontests, enabling both quantitative comparisons and qualitative analyses of\nLLM outputs against human-written codes. Our experiments reveal that even\nadvanced models like GPT-4o achieve only around 18.97% accuracy, highlighting\nthe difficulty of the benchmark. In contrast, reasoning-based models such as o3\nreach up to 78% accuracy, outperforming averaged success rates of human-written\ncodes (39.98%). We release the QCoder Benchmark dataset and public evaluation\nAPI to support further research.",
      "authors": [
        "Taku Mikuriya",
        "Tatsuya Ishigaki",
        "Masayuki Kawarada",
        "Shunya Minami",
        "Tadashi Kadowaki",
        "Yohichi Suzuki",
        "Soshun Naito",
        "Shunya Takata",
        "Takumi Kato",
        "Tamotsu Basseda",
        "Reo Yamada",
        "Hiroya Takamura"
      ],
      "date": "2025-10-30",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26101v1",
      "categories": [
        "cs.CL",
        "cs.PL",
        "quant-ph"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26101v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.26095v1",
      "title": "ORBIT -- Open Recommendation Benchmark for Reproducible Research with Hidden Tests",
      "abstract": "Recommender systems are among the most impactful AI applications, interacting\nwith billions of users every day, guiding them to relevant products, services,\nor information tailored to their preferences. However, the research and\ndevelopment of recommender systems are hindered by existing datasets that fail\nto capture realistic user behaviors and inconsistent evaluation settings that\nlead to ambiguous conclusions. This paper introduces the Open Recommendation\nBenchmark for Reproducible Research with HIdden Tests (ORBIT), a unified\nbenchmark for consistent and realistic evaluation of recommendation models.\nORBIT offers a standardized evaluation framework of public datasets with\nreproducible splits and transparent settings for its public leaderboard.\nAdditionally, ORBIT introduces a new webpage recommendation task, ClueWeb-Reco,\nfeaturing web browsing sequences from 87 million public, high-quality webpages.\nClueWeb-Reco is a synthetic dataset derived from real, user-consented, and\nprivacy-guaranteed browsing data. It aligns with modern recommendation\nscenarios and is reserved as the hidden test part of our leaderboard to\nchallenge recommendation models' generalization ability. ORBIT measures 12\nrepresentative recommendation models on its public benchmark and introduces a\nprompted LLM baseline on the ClueWeb-Reco hidden test. Our benchmark results\nreflect general improvements of recommender systems on the public datasets,\nwith variable individual performances. The results on the hidden test reveal\nthe limitations of existing approaches in large-scale webpage recommendation\nand highlight the potential for improvements with LLM integrations. ORBIT\nbenchmark, leaderboard, and codebase are available at\nhttps://www.open-reco-bench.ai.",
      "authors": [
        "Jingyuan He",
        "Jiongnan Liu",
        "Vishan Vishesh Oberoi",
        "Bolin Wu",
        "Mahima Jagadeesh Patel",
        "Kangrui Mao",
        "Chuning Shi",
        "I-Ta Lee",
        "Arnold Overwijk",
        "Chenyan Xiong"
      ],
      "date": "2025-10-30",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26095v1",
      "categories": [
        "cs.IR",
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26095v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.26038v1",
      "title": "Do Students Debias Like Teachers? On the Distillability of Bias Mitigation Methods",
      "abstract": "Knowledge distillation (KD) is an effective method for model compression and\ntransferring knowledge between models. However, its effect on model's\nrobustness against spurious correlations that degrade performance on\nout-of-distribution data remains underexplored. This study investigates the\neffect of knowledge distillation on the transferability of ``debiasing''\ncapabilities from teacher models to student models on natural language\ninference (NLI) and image classification tasks. Through extensive experiments,\nwe illustrate several key findings: (i) overall the debiasing capability of a\nmodel is undermined post-KD; (ii) training a debiased model does not benefit\nfrom injecting teacher knowledge; (iii) although the overall robustness of a\nmodel may remain stable post-distillation, significant variations can occur\nacross different types of biases; and (iv) we pin-point the internal attention\npattern and circuit that causes the distinct behavior post-KD. Given the above\nfindings, we propose three effective solutions to improve the distillability of\ndebiasing methods: developing high quality data for augmentation, implementing\niterative knowledge distillation, and initializing student models with weights\nobtained from teacher models. To the best of our knowledge, this is the first\nstudy on the effect of KD on debiasing and its interenal mechanism at scale.\nOur findings provide understandings on how KD works and how to design better\ndebiasing methods.",
      "authors": [
        "Jiali Cheng",
        "Chirag Agarwal",
        "Hadi Amiri"
      ],
      "date": "2025-10-30",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26038v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26038v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.26037v1",
      "title": "SIRAJ: Diverse and Efficient Red-Teaming for LLM Agents via Distilled Structured Reasoning",
      "abstract": "The ability of LLM agents to plan and invoke tools exposes them to new safety\nrisks, making a comprehensive red-teaming system crucial for discovering\nvulnerabilities and ensuring their safe deployment. We present SIRAJ: a generic\nred-teaming framework for arbitrary black-box LLM agents. We employ a dynamic\ntwo-step process that starts with an agent definition and generates diverse\nseed test cases that cover various risk outcomes, tool-use trajectories, and\nrisk sources. Then, it iteratively constructs and refines model-based\nadversarial attacks based on the execution trajectories of former attempts. To\noptimize the red-teaming cost, we present a model distillation approach that\nleverages structured forms of a teacher model's reasoning to train smaller\nmodels that are equally effective. Across diverse evaluation agent settings,\nour seed test case generation approach yields 2 -- 2.5x boost to the coverage\nof risk outcomes and tool-calling trajectories. Our distilled 8B red-teamer\nmodel improves attack success rate by 100%, surpassing the 671B Deepseek-R1\nmodel. Our ablations and analyses validate the effectiveness of the iterative\nframework, structured reasoning, and the generalization of our red-teamer\nmodels.",
      "authors": [
        "Kaiwen Zhou",
        "Ahmed Elgohary",
        "A S M Iftekhar",
        "Amin Saied"
      ],
      "date": "2025-10-30",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26037v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26037v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.26032v1",
      "title": "Artificial Intelligence-Enabled Analysis of Radiology Reports: Epidemiology and Consequences of Incidental Thyroid Findings",
      "abstract": "Importance Incidental thyroid findings (ITFs) are increasingly detected on\nimaging performed for non-thyroid indications. Their prevalence, features, and\nclinical consequences remain undefined. Objective To develop, validate, and\ndeploy a natural language processing (NLP) pipeline to identify ITFs in\nradiology reports and assess their prevalence, features, and clinical outcomes.\nDesign, Setting, and Participants Retrospective cohort of adults without prior\nthyroid disease undergoing thyroid-capturing imaging at Mayo Clinic sites from\nJuly 1, 2017, to September 30, 2023. A transformer-based NLP pipeline\nidentified ITFs and extracted nodule characteristics from image reports from\nmultiple modalities and body regions. Main Outcomes and Measures Prevalence of\nITFs, downstream thyroid ultrasound, biopsy, thyroidectomy, and thyroid cancer\ndiagnosis. Logistic regression identified demographic and imaging-related\nfactors. Results Among 115,683 patients (mean age, 56.8 [SD 17.2] years; 52.9%\nwomen), 9,077 (7.8%) had an ITF, of which 92.9% were nodules. ITFs were more\nlikely in women, older adults, those with higher BMI, and when imaging was\nordered by oncology or internal medicine. Compared with chest CT, ITFs were\nmore likely via neck CT, PET, and nuclear medicine scans. Nodule\ncharacteristics were poorly documented, with size reported in 44% and other\nfeatures in fewer than 15% (e.g. calcifications). Compared with patients\nwithout ITFs, those with ITFs had higher odds of thyroid nodule diagnosis,\nbiopsy, thyroidectomy and thyroid cancer diagnosis. Most cancers were\npapillary, and larger when detected after ITFs vs no ITF. Conclusions ITFs were\ncommon and strongly associated with cascades leading to the detection of small,\nlow-risk cancers. These findings underscore the role of ITFs in thyroid cancer\noverdiagnosis and the need for standardized reporting and more selective\nfollow-up.",
      "authors": [
        "Felipe Larios",
        "Mariana Borras-Osorio",
        "Yuqi Wu",
        "Ana Gabriela Claros",
        "David Toro-Tobon",
        "Esteban Cabezas",
        "Ricardo Loor-Torres",
        "Maria Mateo Chavez",
        "Kerly Guevara Maldonado",
        "Luis Vilatuna Andrango",
        "Maria Lizarazo Jimenez",
        "Ivan Mateo Alzamora",
        "Misk Al Zahidy",
        "Marcelo Montero",
        "Ana Cristina Proano",
        "Cristian Soto Jacome",
        "Jungwei W. Fan",
        "Oscar J. Ponce-Ponte",
        "Megan E. Branda",
        "Naykky Singh Ospina",
        "Juan P. Brito"
      ],
      "date": "2025-10-30",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26032v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26032v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.26024v1",
      "title": "Rethinking Cross-lingual Alignment: Balancing Transfer and Cultural Erasure in Multilingual LLMs",
      "abstract": "Cross-lingual alignment (CLA) aims to align multilingual representations,\nenabling Large Language Models (LLMs) to seamlessly transfer knowledge across\nlanguages. While intuitive, we hypothesize, this pursuit of representational\nconvergence can inadvertently cause \"cultural erasure\", the functional loss of\nproviding culturally-situated responses that should diverge based on the query\nlanguage. In this work, we systematically analyze this trade-off by introducing\na holistic evaluation framework, the transfer-localization plane, which\nquantifies both desirable knowledge transfer and undesirable cultural erasure.\nUsing this framework, we re-evaluate recent CLA approaches and find that they\nconsistently improve factual transfer at the direct cost of cultural\nlocalization across all six languages studied. Our investigation into the\ninternal representations of these models reveals a key insight: universal\nfactual transfer and culturally-specific knowledge are optimally steerable at\ndifferent model layers. Based on this finding, we propose Surgical Steering, a\nnovel inference-time method that disentangles these two objectives. By applying\ntargeted activation steering to distinct layers, our approach achieves a better\nbalance between the two competing dimensions, effectively overcoming the\nlimitations of current alignment techniques.",
      "authors": [
        "HyoJung Han",
        "Sweta Agrawal",
        "Eleftheria Briakou"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26024v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26024v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.26020v1",
      "title": "PORTool: Tool-Use LLM Training with Rewarded Tree",
      "abstract": "Current tool-use large language models (LLMs) are trained on static datasets,\nenabling them to interact with external tools and perform multi-step,\ntool-integrated reasoning, which produces tool-call trajectories. However,\nthese models imitate how a query is resolved in a generic tool-call routine,\nthereby failing to explore possible solutions and demonstrating limited\nperformance in an evolved, dynamic tool-call environment. In this work, we\npropose PORTool, a reinforcement learning (RL) method that encourages a\ntool-use LLM to explore various trajectories yielding the correct answer.\nSpecifically, this method starts with generating multiple rollouts for a given\nquery, and some of them share the first few tool-call steps, thereby forming a\ntree-like structure. Next, we assign rewards to each step, based on its ability\nto produce a correct answer and make successful tool calls. A shared step\nacross different trajectories receives the same reward, while different steps\nunder the same fork receive different rewards. Finally, these step-wise rewards\nare used to calculate fork-relative advantages, blended with\ntrajectory-relative advantages, to train the LLM for tool use. The experiments\nutilize 17 tools to address user queries, covering both time-sensitive and\ntime-invariant topics. We conduct ablation studies to systematically justify\nthe necessity and the design robustness of step-wise rewards. Furthermore, we\ncompare the proposed PORTool with other training approaches and demonstrate\nsignificant improvements in final accuracy and the number of tool-call steps.",
      "authors": [
        "Feijie Wu",
        "Weiwu Zhu",
        "Yuxiang Zhang",
        "Soumya Chatterjee",
        "Jiarong Zhu",
        "Fan Mo",
        "Rodin Luo",
        "Jing Gao"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26020v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26020v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.26006v1",
      "title": "CAVE: Detecting and Explaining Commonsense Anomalies in Visual Environments",
      "abstract": "Humans can naturally identify, reason about, and explain anomalies in their\nenvironment. In computer vision, this long-standing challenge remains limited\nto industrial defects or unrealistic, synthetically generated anomalies,\nfailing to capture the richness and unpredictability of real-world anomalies.\nIn this work, we introduce CAVE, the first benchmark of real-world visual\nanomalies. CAVE supports three open-ended tasks: anomaly description,\nexplanation, and justification; with fine-grained annotations for visual\ngrounding and categorizing anomalies based on their visual manifestations,\ntheir complexity, severity, and commonness. These annotations draw inspiration\nfrom cognitive science research on how humans identify and resolve anomalies,\nproviding a comprehensive framework for evaluating Vision-Language Models\n(VLMs) in detecting and understanding anomalies. We show that state-of-the-art\nVLMs struggle with visual anomaly perception and commonsense reasoning, even\nwith advanced prompting strategies. By offering a realistic and cognitively\ngrounded benchmark, CAVE serves as a valuable resource for advancing research\nin anomaly detection and commonsense reasoning in VLMs.",
      "authors": [
        "Rishika Bhagwatkar",
        "Syrielle Montariol",
        "Angelika Romanou",
        "Beatriz Borges",
        "Irina Rish",
        "Antoine Bosselut"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.26006v1",
      "categories": [
        "cs.CV",
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.26006v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25992v1",
      "title": "Supervised Reinforcement Learning: From Expert Trajectories to Step-wise Reasoning",
      "abstract": "Large Language Models (LLMs) often struggle with problems that require\nmulti-step reasoning. For small-scale open-source models, Reinforcement\nLearning with Verifiable Rewards (RLVR) fails when correct solutions are rarely\nsampled even after many attempts, while Supervised Fine-Tuning (SFT) tends to\noverfit long demonstrations through rigid token-by-token imitation. To address\nthis gap, we propose Supervised Reinforcement Learning (SRL), a framework that\nreformulates problem solving as generating a sequence of logical \"actions\". SRL\ntrains the model to generate an internal reasoning monologue before committing\nto each action. It provides smoother rewards based on the similarity between\nthe model's actions and expert actions extracted from the SFT dataset in a\nstep-wise manner. This supervision offers richer learning signals even when all\nrollouts are incorrect, while encouraging flexible reasoning guided by expert\ndemonstrations. As a result, SRL enables small models to learn challenging\nproblems previously unlearnable by SFT or RLVR. Moreover, initializing training\nwith SRL before refining with RLVR yields the strongest overall performance.\nBeyond reasoning benchmarks, SRL generalizes effectively to agentic software\nengineering tasks, establishing it as a robust and versatile training framework\nfor reasoning-oriented LLMs.",
      "authors": [
        "Yihe Deng",
        "I-Hung Hsu",
        "Jun Yan",
        "Zifeng Wang",
        "Rujun Han",
        "Gufeng Zhang",
        "Yanfei Chen",
        "Wei Wang",
        "Tomas Pfister",
        "Chen-Yu Lee"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25992v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25992v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25979v1",
      "title": "AttnCache: Accelerating Self-Attention Inference for LLM Prefill via Attention Cache",
      "abstract": "Large Language Models (LLMs) are widely used in generative applications such\nas chatting, code generation, and reasoning. However, many realworld workloads\nsuch as classification, question answering, recommendation, and text embedding\nrely solely on the prefill stage of inference, where the model encodes input\nsequences without performing autoregressive decoding. In these prefill only\nscenarios, the self-attention computation becomes the primary performance\nbottleneck due to its quadratic complexity with respect to sequence length. In\nthis paper, we observe that semantically different sentences often produce\nsimilar attention maps across layers and heads. Building on this insight, we\npropose AttnCache, a framework that accelerates the prefill stage of LLM\ninference by retrieving and reusing similar attention maps. Based on an\nattention map memorization database, AttnCache employs efficient caching and\nsimilarity search techniques to identify and reuse pre-cached attention maps\nduring inference, thereby reducing the computational overhead of\nself-attention. Experimental results show that AttnCache achieves an average of\n1.2x end-to-end and 2x attention speedup on CPU, and 1.6x end-to-end and 3x\nattention speedup on GPU, with negligible accuracy degradation.",
      "authors": [
        "Dinghong Song",
        "Yuan Feng",
        "Yiwei Wang",
        "Shangye Chen",
        "Cyril Guyot",
        "Filip Blagojevic",
        "Hyeran Jeon",
        "Pengfei Su",
        "Dong Li"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25979v1",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25979v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25977v2",
      "title": "NeuronMM: High-Performance Matrix Multiplication for LLM Inference on AWS Trainium",
      "abstract": "AI accelerators, customized to AI workloads, provide cost-effective and\nhigh-performance solutions for training and inference. Trainium, an AI\naccelerator recently developed by Amazon Web Services (AWS), provides an\nattractive option for LLM training and inference through its heterogeneous\narchitecture. However, leveraging Trainium architecture for high performance\ncan be challenging because of its systolic array architecture and special\nrequirement on data layout. In this paper, we design high-performance matrix\nmultiplication (matmul), a critical compute kernel, for LLM inference on\nTrainium. We introduce a series of techniques customized to Trainium based on\nkernel fusion and novel caching strategies to reduce data movement across the\nsoftware-managed memory hierarchy, maximize SRAM bandwidth, and avoid expensive\nmatrix transpose. Evaluating with nine datasets and four recent LLMs, we show\nthat our system largely outperforms the state-of-the-art matmul implemented by\nAWS on Trainium: at the level of matmul kernel, it achieves an average 1.35x\nspeedup (up to 2.22x), which translates to an average 1.66x speedup (up to\n2.49x) for end-to-end LLM inference.",
      "authors": [
        "Dinghong Song",
        "Jierui Xu",
        "Weichu Yang",
        "Pengfei Su",
        "Dong Li"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25977v2",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25977v2"
    },
    {
      "url": "https://arxiv.org/abs/2510.25975v1",
      "title": "SymCode: A Neurosymbolic Approach to Mathematical Reasoning via Verifiable Code Generation",
      "abstract": "Large Language Models (LLMs) often struggle with complex mathematical\nreasoning, where prose-based generation leads to unverified and arithmetically\nunsound solutions. Current prompting strategies like Chain of Thought still\noperate within this unreliable medium, lacking a mechanism for deterministic\nverification. To address these limitations, we introduce SymCode, a\nneurosymbolic framework that reframes mathematical problem-solving as a task of\nverifiable code generation using the SymPy library. We evaluate SymCode on\nchallenging benchmarks, including MATH-500 and OlympiadBench, demonstrating\nsignificant accuracy improvements of up to 13.6 percentage points over\nbaselines. Our analysis shows that SymCode is not only more token-efficient but\nalso fundamentally shifts model failures from opaque logical fallacies towards\ntransparent, programmatic errors. By grounding LLM reasoning in a deterministic\nsymbolic engine, SymCode represents a key step towards more accurate and\ntrustworthy AI in formal domains.",
      "authors": [
        "Sina Bagheri Nezhad",
        "Yao Li",
        "Ameeta Agrawal"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25975v1",
      "categories": [
        "cs.CL",
        "cs.PL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25975v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25967v1",
      "title": "Semantic Label Drift in Cross-Cultural Translation",
      "abstract": "Machine Translation (MT) is widely employed to address resource scarcity in\nlow-resource languages by generating synthetic data from high-resource\ncounterparts. While sentiment preservation in translation has long been\nstudied, a critical but underexplored factor is the role of cultural alignment\nbetween source and target languages. In this paper, we hypothesize that\nsemantic labels are drifted or altered during MT due to cultural divergence.\nThrough a series of experiments across culturally sensitive and neutral\ndomains, we establish three key findings: (1) MT systems, including modern\nLarge Language Models (LLMs), induce label drift during translation,\nparticularly in culturally sensitive domains; (2) unlike earlier statistical MT\ntools, LLMs encode cultural knowledge, and leveraging this knowledge can\namplify label drift; and (3) cultural similarity or dissimilarity between\nsource and target languages is a crucial determinant of label preservation. Our\nfindings highlight that neglecting cultural factors in MT not only undermines\nlabel fidelity but also risks misinterpretation and cultural conflict in\ndownstream applications.",
      "authors": [
        "Mohsinul Kabir",
        "Tasnim Ahmed",
        "Md Mezbaur Rahman",
        "Polydoros Giannouris",
        "Sophia Ananiadou"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25967v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25967v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25947v1",
      "title": "Revisiting Multilingual Data Mixtures in Language Model Pretraining",
      "abstract": "The impact of different multilingual data mixtures in pretraining large\nlanguage models (LLMs) has been a topic of ongoing debate, often raising\nconcerns about potential trade-offs between language coverage and model\nperformance (i.e., the curse of multilinguality). In this work, we investigate\nthese assumptions by training 1.1B and 3B parameter LLMs on diverse\nmultilingual corpora, varying the number of languages from 25 to 400. Our study\nchallenges common beliefs surrounding multilingual training. First, we find\nthat combining English and multilingual data does not necessarily degrade the\nin-language performance of either group, provided that languages have a\nsufficient number of tokens included in the pretraining corpus. Second, we\nobserve that using English as a pivot language (i.e., a high-resource language\nthat serves as a catalyst for multilingual generalization) yields benefits\nacross language families, and contrary to expectations, selecting a pivot\nlanguage from within a specific family does not consistently improve\nperformance for languages within that family. Lastly, we do not observe a\nsignificant \"curse of multilinguality\" as the number of training languages\nincreases in models at this scale. Our findings suggest that multilingual data,\nwhen balanced appropriately, can enhance language model capabilities without\ncompromising performance, even in low-resource settings",
      "authors": [
        "Negar Foroutan",
        "Paul Teiletche",
        "Ayush Kumar Tarun",
        "Antoine Bosselut"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25947v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25947v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25941v1",
      "title": "RECAP: Reproducing Copyrighted Data from LLMs Training with an Agentic Pipeline",
      "abstract": "If we cannot inspect the training data of a large language model (LLM), how\ncan we ever know what it has seen? We believe the most compelling evidence\narises when the model itself freely reproduces the target content. As such, we\npropose RECAP, an agentic pipeline designed to elicit and verify memorized\ntraining data from LLM outputs. At the heart of RECAP is a feedback-driven\nloop, where an initial extraction attempt is evaluated by a secondary language\nmodel, which compares the output against a reference passage and identifies\ndiscrepancies. These are then translated into minimal correction hints, which\nare fed back into the target model to guide subsequent generations. In\naddition, to address alignment-induced refusals, RECAP includes a jailbreaking\nmodule that detects and overcomes such barriers. We evaluate RECAP on\nEchoTrace, a new benchmark spanning over 30 full books, and the results show\nthat RECAP leads to substantial gains over single-iteration approaches. For\ninstance, with GPT-4.1, the average ROUGE-L score for the copyrighted text\nextraction improved from 0.38 to 0.47 - a nearly 24% increase.",
      "authors": [
        "André V. Duarte",
        "Xuying li",
        "Bin Zeng",
        "Arlindo L. Oliveira",
        "Lei Li",
        "Zhuo Li"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25941v1",
      "categories": [
        "cs.CL",
        "I.2"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25941v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25932v1",
      "title": "FakeZero: Real-Time, Privacy-Preserving Misinformation Detection for Facebook and X",
      "abstract": "Social platforms distribute information at unprecedented speed, which in turn\naccelerates the spread of misinformation and threatens public discourse. We\npresent FakeZero, a fully client-side, cross-platform browser extension that\nflags unreliable posts on Facebook and X (formerly Twitter) while the user\nscrolls. All computation, DOM scraping, tokenisation, Transformer inference,\nand UI rendering run locally through the Chromium messaging API, so no personal\ndata leaves the device.FakeZero employs a three-stage training curriculum:\nbaseline fine-tuning and domain-adaptive training enhanced with focal loss,\nadversarial augmentation, and post-training quantisation. Evaluated on a\ndataset of 239,000 posts, the DistilBERT-Quant model (67.6 MB) reaches 97.1%\nmacro-F1, 97.4% accuracy, and an AUROC of 0.996, with a median latency of\napproximately 103 ms on a commodity laptop. A memory-efficient TinyBERT-Quant\nvariant retains 95.7% macro-F1 and 96.1% accuracy while shrinking the model to\n14.7 MB and lowering latency to approximately 40 ms, showing that high-quality\nfake-news detection is feasible under tight resource budgets with only modest\nperformance loss.By providing inline credibility cues, the extension can serve\nas a valuable tool for policymakers seeking to curb the spread of\nmisinformation across social networks. With user consent, FakeZero also opens\nthe door for researchers to collect large-scale datasets of fake news in the\nwild, enabling deeper analysis and the development of more robust detection\ntechniques.",
      "authors": [
        "Soufiane Essahli",
        "Oussama Sarsar",
        "Imane Fouad",
        "Anas Motii",
        "Ahmed Bentajer"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25932v1",
      "categories": [
        "cs.CR",
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25932v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25904v1",
      "title": "Evaluating the Impact of LLM-Assisted Annotation in a Perspectivized Setting: the Case of FrameNet Annotation",
      "abstract": "The use of LLM-based applications as a means to accelerate and/or substitute\nhuman labor in the creation of language resources and dataset is a reality.\nNonetheless, despite the potential of such tools for linguistic research,\ncomprehensive evaluation of their performance and impact on the creation of\nannotated datasets, especially under a perspectivized approach to NLP, is still\nmissing. This paper contributes to reduction of this gap by reporting on an\nextensive evaluation of the (semi-)automatization of FrameNet-like semantic\nannotation by the use of an LLM-based semantic role labeler. The methodology\nemployed compares annotation time, coverage and diversity in three experimental\nsettings: manual, automatic and semi-automatic annotation. Results show that\nthe hybrid, semi-automatic annotation setting leads to increased frame\ndiversity and similar annotation coverage, when compared to the human-only\nsetting, while the automatic setting performs considerably worse in all\nmetrics, except for annotation time.",
      "authors": [
        "Frederico Belcavello",
        "Ely Matos",
        "Arthur Lorenzi",
        "Lisandra Bonoto",
        "Lívia Ruiz",
        "Luiz Fernando Pereira",
        "Victor Herbst",
        "Yulla Navarro",
        "Helen de Andrade Abreu",
        "Lívia Dutra",
        "Tiago Timponi Torrent"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25904v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25904v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25884v1",
      "title": "Approximating Human Preferences Using a Multi-Judge Learned System",
      "abstract": "Aligning LLM-based judges with human preferences is a significant challenge,\nas they are difficult to calibrate and often suffer from rubric sensitivity,\nbias, and instability. Overcoming this challenge advances key applications,\nsuch as creating reliable reward models for Reinforcement Learning from Human\nFeedback (RLHF) and building effective routing systems that select the\nbest-suited model for a given user query. In this work, we propose a framework\nfor modeling diverse, persona-based preferences by learning to aggregate\noutputs from multiple rubric-conditioned judges. We investigate the performance\nof this approach against naive baselines and assess its robustness through case\nstudies on both human and LLM-judges biases. Our primary contributions include\na persona-based method for synthesizing preference labels at scale and two\ndistinct implementations of our aggregator: Generalized Additive Model (GAM)\nand a Multi-Layer Perceptron (MLP).",
      "authors": [
        "Eitán Sprejer",
        "Fernando Avalos",
        "Augusto Bernardi",
        "Jose Pedro Brito de Azevedo Faustino",
        "Jacob Haimes",
        "Narmeen Fatimah Oozeer"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25884v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25884v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25860v1",
      "title": "Through the Judge's Eyes: Inferred Thinking Traces Improve Reliability of LLM Raters",
      "abstract": "Large language models (LLMs) are increasingly used as raters for evaluation\ntasks. However, their reliability is often limited for subjective tasks, when\nhuman judgments involve subtle reasoning beyond annotation labels. Thinking\ntraces, the reasoning behind a judgment, are highly informative but challenging\nto collect and curate. We present a human-LLM collaborative framework to infer\nthinking traces from label-only annotations. The proposed framework uses a\nsimple and effective rejection sampling method to reconstruct these traces at\nscale. These inferred thinking traces are applied to two complementary tasks:\n(1) fine-tuning open LLM raters; and (2) synthesizing clearer annotation\nguidelines for proprietary LLM raters. Across multiple datasets, our methods\nlead to significantly improved LLM-human agreement. Additionally, the refined\nannotation guidelines increase agreement among different LLM models. These\nresults suggest that LLMs can serve as practical proxies for otherwise\nunrevealed human thinking traces, enabling label-only corpora to be extended\ninto thinking-trace-augmented resources that enhance the reliability of LLM\nraters.",
      "authors": [
        "Xingjian Zhang",
        "Tianhong Gao",
        "Suliang Jin",
        "Tianhao Wang",
        "Teng Ye",
        "Eytan Adar",
        "Qiaozhu Mei"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25860v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.HC"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25860v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25771v1",
      "title": "Gaperon: A Peppered English-French Generative Language Model Suite",
      "abstract": "We release Gaperon, a fully open suite of French-English-coding language\nmodels designed to advance transparency and reproducibility in large-scale\nmodel training. The Gaperon family includes 1.5B, 8B, and 24B parameter models\ntrained on 2-4 trillion tokens, released with all elements of the training\npipeline: French and English datasets filtered with a neural quality\nclassifier, an efficient data curation and training framework, and hundreds of\nintermediate checkpoints. Through this work, we study how data filtering and\ncontamination interact to shape both benchmark and generative performance. We\nfind that filtering for linguistic quality enhances text fluency and coherence\nbut yields subpar benchmark results, and that late deliberate contamination --\ncontinuing training on data mixes that include test sets -- recovers\ncompetitive scores while only reasonably harming generation quality. We discuss\nhow usual neural filtering can unintentionally amplify benchmark leakage. To\nsupport further research, we also introduce harmless data poisoning during\npretraining, providing a realistic testbed for safety studies. By openly\nreleasing all models, datasets, code, and checkpoints, Gaperon establishes a\nreproducible foundation for exploring the trade-offs between data curation,\nevaluation, safety, and openness in multilingual language model development.",
      "authors": [
        "Nathan Godey",
        "Wissam Antoun",
        "Rian Touchent",
        "Rachel Bawden",
        "Éric de la Clergerie",
        "Benoît Sagot",
        "Djamé Seddah"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25771v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25771v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25766v1",
      "title": "Decomposition-Enhanced Training for Post-Hoc Attributions In Language Models",
      "abstract": "Large language models (LLMs) are increasingly used for long-document question\nanswering, where reliable attribution to sources is critical for trust.\nExisting post-hoc attribution methods work well for extractive QA but struggle\nin multi-hop, abstractive, and semi-extractive settings, where answers\nsynthesize information across passages. To address these challenges, we argue\nthat post-hoc attribution can be reframed as a reasoning problem, where answers\nare decomposed into constituent units, each tied to specific context. We first\nshow that prompting models to generate such decompositions alongside\nattributions improves performance. Building on this, we introduce DecompTune, a\npost-training method that teaches models to produce answer decompositions as\nintermediate reasoning steps. We curate a diverse dataset of complex QA tasks,\nannotated with decompositions by a strong LLM, and post-train Qwen-2.5 (7B and\n14B) using a two-stage SFT + GRPO pipeline with task-specific curated rewards.\nAcross extensive experiments and ablations, DecompTune substantially improves\nattribution quality, outperforming prior methods and matching or exceeding\nstate-of-the-art frontier models.",
      "authors": [
        "Sriram Balasubramaniam",
        "Samyadeep Basu",
        "Koustava Goswami",
        "Ryan Rossi",
        "Varun Manjunatha",
        "Roshan Santhosh",
        "Ruiyi Zhang",
        "Soheil Feizi",
        "Nedim Lipka"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25766v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25766v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25761v2",
      "title": "DiagramEval: Evaluating LLM-Generated Diagrams via Graphs",
      "abstract": "Diagrams play a central role in research papers for conveying ideas, yet they\nare often notoriously complex and labor-intensive to create. Although diagrams\nare presented as images, standard image generative models struggle to produce\nclear diagrams with well-defined structure. We argue that a promising direction\nis to generate demonstration diagrams directly in textual form as SVGs, which\ncan leverage recent advances in large language models (LLMs). However, due to\nthe complexity of components and the multimodal nature of diagrams,\nsufficiently discriminative and explainable metrics for evaluating the quality\nof LLM-generated diagrams remain lacking. In this paper, we propose\nDiagramEval, a novel evaluation metric designed to assess demonstration\ndiagrams generated by LLMs. Specifically, DiagramEval conceptualizes diagrams\nas graphs, treating text elements as nodes and their connections as directed\nedges, and evaluates diagram quality using two new groups of metrics: node\nalignment and path alignment. For the first time, we effectively evaluate\ndiagrams produced by state-of-the-art LLMs on recent research literature,\nquantitatively demonstrating the validity of our metrics. Furthermore, we show\nhow the enhanced explainability of our proposed metrics offers valuable\ninsights into the characteristics of LLM-generated diagrams. Code:\nhttps://github.com/ulab-uiuc/diagram-eval.",
      "authors": [
        "Chumeng Liang",
        "Jiaxuan You"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25761v2",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25761v2"
    },
    {
      "url": "https://arxiv.org/abs/2510.25744v2",
      "title": "Completion $\\neq$ Collaboration: Scaling Collaborative Effort with Agents",
      "abstract": "Current evaluations of agents remain centered around one-shot task\ncompletion, failing to account for the inherently iterative and collaborative\nnature of many real-world problems, where human goals are often underspecified\nand evolve. We argue for a shift from building and assessing task completion\nagents to developing collaborative agents, assessed not only by the quality of\ntheir final outputs but by how well they engage with and enhance human effort\nthroughout the problem-solving process. To support this shift, we introduce\ncollaborative effort scaling, a framework that captures how an agent's utility\ngrows with increasing user involvement. Through case studies and simulated\nevaluations, we show that state-of-the-art agents often underperform in\nmulti-turn, real-world scenarios, revealing a missing ingredient in agent\ndesign: the ability to sustain engagement and scaffold user understanding.\nCollaborative effort scaling offers a lens for diagnosing agent behavior and\nguiding development toward more effective interactions.",
      "authors": [
        "Shannon Zejiang Shen",
        "Valerie Chen",
        "Ken Gu",
        "Alexis Ross",
        "Zixian Ma",
        "Jillian Ross",
        "Alex Gu",
        "Chenglei Si",
        "Wayne Chi",
        "Andi Peng",
        "Jocelyn J Shen",
        "Ameet Talwalkar",
        "Tongshuang Wu",
        "David Sontag"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25744v2",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25744v2"
    },
    {
      "url": "https://arxiv.org/abs/2510.25741v1",
      "title": "Scaling Latent Reasoning via Looped Language Models",
      "abstract": "Modern LLMs are trained to \"think\" primarily via explicit text generation,\nsuch as chain-of-thought (CoT), which defers reasoning to post-training and\nunder-leverages pre-training data. We present and open-source Ouro, named after\nthe recursive Ouroboros, a family of pre-trained Looped Language Models\n(LoopLM) that instead build reasoning into the pre-training phase through (i)\niterative computation in latent space, (ii) an entropy-regularized objective\nfor learned depth allocation, and (iii) scaling to 7.7T tokens. Ouro 1.4B and\n2.6B models enjoy superior performance that match the results of up to 12B SOTA\nLLMs across a wide range of benchmarks. Through controlled experiments, we show\nthis advantage stems not from increased knowledge capacity, but from superior\nknowledge manipulation capabilities. We also show that LoopLM yields reasoning\ntraces more aligned with final outputs than explicit CoT. We hope our results\nshow the potential of LoopLM as a novel scaling direction in the reasoning era.\nOur model could be found in: http://ouro-llm.github.io.",
      "authors": [
        "Rui-Jie Zhu",
        "Zixuan Wang",
        "Kai Hua",
        "Tianyu Zhang",
        "Ziniu Li",
        "Haoran Que",
        "Boyi Wei",
        "Zixin Wen",
        "Fan Yin",
        "He Xing",
        "Lu Li",
        "Jiajun Shi",
        "Kaijing Ma",
        "Shanda Li",
        "Taylor Kergan",
        "Andrew Smith",
        "Xingwei Qu",
        "Mude Hui",
        "Bohong Wu",
        "Qiyang Min",
        "Hongzhi Huang",
        "Xun Zhou",
        "Wei Ye",
        "Jiaheng Liu",
        "Jian Yang",
        "Yunfeng Shi",
        "Chenghua Lin",
        "Enduo Zhao",
        "Tianle Cai",
        "Ge Zhang",
        "Wenhao Huang",
        "Yoshua Bengio",
        "Jason Eshraghian"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25741v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25741v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25732v1",
      "title": "The Limits of Obliviate: Evaluating Unlearning in LLMs via Stimulus-Knowledge Entanglement-Behavior Framework",
      "abstract": "Unlearning in large language models (LLMs) is crucial for managing sensitive\ndata and correcting misinformation, yet evaluating its effectiveness remains an\nopen problem. We investigate whether persuasive prompting can recall factual\nknowledge from deliberately unlearned LLMs across models ranging from 2.7B to\n13B parameters (OPT-2.7B, LLaMA-2-7B, LLaMA-3.1-8B, LLaMA-2-13B). Drawing from\nACT-R and Hebbian theory (spreading activation theories), as well as\ncommunication principles, we introduce Stimulus-Knowledge Entanglement-Behavior\nFramework (SKeB), which models information entanglement via domain graphs and\ntests whether factual recall in unlearned models is correlated with persuasive\nframing. We develop entanglement metrics to quantify knowledge activation\npatterns and evaluate factuality, non-factuality, and hallucination in outputs.\nOur results show persuasive prompts substantially enhance factual knowledge\nrecall (14.8% baseline vs. 24.5% with authority framing), with effectiveness\ninversely correlated to model size (128% recovery in 2.7B vs. 15% in 13B). SKeB\nprovides a foundation for assessing unlearning completeness, robustness, and\noverall behavior in LLMs.",
      "authors": [
        "Aakriti Shah",
        "Thai Le"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25732v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.7; I.2.6; I.2.4; G.2.2"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25732v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25726v1",
      "title": "The Tool Decathlon: Benchmarking Language Agents for Diverse, Realistic, and Long-Horizon Task Execution",
      "abstract": "Real-world language agents must handle complex, multi-step workflows across\ndiverse Apps. For instance, an agent may manage emails by coordinating with\ncalendars and file systems, or monitor a production database to detect\nanomalies and generate reports following an operating manual. However, existing\nlanguage agent benchmarks often focus on narrow domains or simplified tasks\nthat lack the diversity, realism, and long-horizon complexity required to\nevaluate agents' real-world performance. To address this gap, we introduce the\nTool Decathlon (dubbed as Toolathlon), a benchmark for language agents offering\ndiverse Apps and tools, realistic environment setup, and reliable\nexecution-based evaluation. Toolathlon spans 32 software applications and 604\ntools, ranging from everyday platforms such as Google Calendar and Notion to\nprofessional ones like WooCommerce, Kubernetes, and BigQuery. Most of the tools\nare based on a high-quality set of Model Context Protocol (MCP) servers that we\nmay have revised or implemented ourselves. Unlike prior works, which primarily\nensure functional realism but offer limited environment state diversity, we\nprovide realistic initial environment states from real software, such as Canvas\ncourses with dozens of students or real financial spreadsheets. This benchmark\nincludes 108 manually sourced or crafted tasks in total, requiring interacting\nwith multiple Apps over around 20 turns on average to complete. Each task is\nstrictly verifiable through dedicated evaluation scripts. Comprehensive\nevaluation of SOTA models highlights their significant shortcomings: the\nbest-performing model, Claude-4.5-Sonnet, achieves only a 38.6% success rate\nwith 20.2 tool calling turns on average, while the top open-weights model\nDeepSeek-V3.2-Exp reaches 20.1%. We expect Toolathlon to drive the development\nof more capable language agents for real-world, long-horizon task execution.",
      "authors": [
        "Junlong Li",
        "Wenshuo Zhao",
        "Jian Zhao",
        "Weihao Zeng",
        "Haoze Wu",
        "Xiaochen Wang",
        "Rui Ge",
        "Yuxuan Cao",
        "Yuzhen Huang",
        "Wei Liu",
        "Junteng Liu",
        "Zhaochen Su",
        "Yiyang Guo",
        "Fan Zhou",
        "Lueyang Zhang",
        "Juan Michelini",
        "Xingyao Wang",
        "Xiang Yue",
        "Shuyan Zhou",
        "Graham Neubig",
        "Junxian He"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25726v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25726v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25701v1",
      "title": "Interpreting LLMs as Credit Risk Classifiers: Do Their Feature Explanations Align with Classical ML?",
      "abstract": "Large Language Models (LLMs) are increasingly explored as flexible\nalternatives to classical machine learning models for classification tasks\nthrough zero-shot prompting. However, their suitability for structured tabular\ndata remains underexplored, especially in high-stakes financial applications\nsuch as financial risk assessment. This study conducts a systematic comparison\nbetween zero-shot LLM-based classifiers and LightGBM, a state-of-the-art\ngradient-boosting model, on a real-world loan default prediction task. We\nevaluate their predictive performance, analyze feature attributions using SHAP,\nand assess the reliability of LLM-generated self-explanations. While LLMs are\nable to identify key financial risk indicators, their feature importance\nrankings diverge notably from LightGBM, and their self-explanations often fail\nto align with empirical SHAP attributions. These findings highlight the\nlimitations of LLMs as standalone models for structured financial risk\nprediction and raise concerns about the trustworthiness of their self-generated\nexplanations. Our results underscore the need for explainability audits,\nbaseline comparisons with interpretable models, and human-in-the-loop oversight\nwhen deploying LLMs in risk-sensitive financial environments.",
      "authors": [
        "Saeed AlMarri",
        "Kristof Juhasz",
        "Mathieu Ravaut",
        "Gautier Marti",
        "Hamdan Al Ahbabi",
        "Ibrahim Elfadel"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25701v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25701v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25817v1",
      "title": "A Survey on Efficient Large Language Model Training: From Data-centric Perspectives",
      "abstract": "Post-training of Large Language Models (LLMs) is crucial for unlocking their\ntask generalization potential and domain-specific capabilities. However, the\ncurrent LLM post-training paradigm faces significant data challenges, including\nthe high costs of manual annotation and diminishing marginal returns on data\nscales. Therefore, achieving data-efficient post-training has become a key\nresearch question. In this paper, we present the first systematic survey of\ndata-efficient LLM post-training from a data-centric perspective. We propose a\ntaxonomy of data-efficient LLM post-training methods, covering data selection,\ndata quality enhancement, synthetic data generation, data distillation and\ncompression, and self-evolving data ecosystems. We summarize representative\napproaches in each category and outline future research directions. By\nexamining the challenges in data-efficient LLM post-training, we highlight open\nproblems and propose potential research avenues. We hope our work inspires\nfurther exploration into maximizing the potential of data utilization in\nlarge-scale model training. Paper List:\nhttps://github.com/luo-junyu/Awesome-Data-Efficient-LLM",
      "authors": [
        "Junyu Luo",
        "Bohan Wu",
        "Xiao Luo",
        "Zhiping Xiao",
        "Yiqiao Jin",
        "Rong-Cheng Tu",
        "Nan Yin",
        "Yifan Wang",
        "Jingyang Yuan",
        "Wei Ju",
        "Ming Zhang"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25817v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25817v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25694v1",
      "title": "Process-Level Trajectory Evaluation for Environment Configuration in Software Engineering Agents",
      "abstract": "Large language model-based agents show promise for software engineering, but\nenvironment configuration remains a bottleneck due to heavy manual effort and\nscarce large-scale, high-quality datasets. Existing benchmarks assess only\nend-to-end build/test success, obscuring where and why agents succeed or fail.\nWe introduce the Environment Configuration Diagnosis Benchmark, Enconda-bench,\nwhich provides process-level trajectory assessment of fine-grained agent\ncapabilities during environment setup-planning, perception-driven error\ndiagnosis, feedback-driven repair, and action to execute final environment\nconfiguration. Our task instances are automatically constructed by injecting\nrealistic README errors and are validated in Docker for scalable, high-quality\nevaluation. Enconda-bench combines process-level analysis with end-to-end\nexecutability to enable capability assessments beyond aggregate success rates.\nEvaluations across state-of-the-art LLMs and agent frameworks show that while\nagents can localize errors, they struggle to translate feedback into effective\ncorrections, limiting end-to-end performance. To our knowledge, Enconda-bench\nis the first framework to provide process-level internal capability assessment\nfor environment configuration, offering actionable insights for improving\nsoftware engineering agents.",
      "authors": [
        "Jiayi Kuang",
        "Yinghui Li",
        "Xin Zhang",
        "Yangning Li",
        "Di Yin",
        "Xing Sun",
        "Ying Shen",
        "Philip S. Yu"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25694v1",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25694v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25682v2",
      "title": "PairUni: Pairwise Training for Unified Multimodal Language Models",
      "abstract": "Unified vision-language models (UVLMs) must perform both understanding and\ngeneration within a single architecture, but these tasks rely on heterogeneous\ndata and supervision, making it difficult to balance them during reinforcement\nlearning (RL). We propose PairUni, a unified framework that reorganizes data\ninto understanding-generation (UG) pairs and aligns optimization accordingly.\nWe first use GPT-o3 to augment single-task data, generating captions for\nunderstanding samples and question-answer (QA) pairs for generation samples,\nforming aligned pairs from the same instance. Additionally, for each generation\nsample, we retrieve a semantically related understanding example to form a\nretrieved pair, linking different but related data points. These paired\nstructures expose cross-task semantic correspondences and support consistent\npolicy learning. To leverage this structure, we present Pair-GPRO, a pair-aware\nvariant based on Group Relative Policy Optimization. It assigns a similarity\nscore to each pair to modulate the advantage, strengthening learning from\nwell-aligned examples and reducing task interference. We curate a high-quality\ndataset of 16K UG pairs named PairUG for RL fine-tuning and evaluate PairUni on\nthe powerful Janus-Pro UVLMs. Our approach achieves balanced improvements on\nvarious UVLMs, outperforming strong UVLM RL baselines. Codes are available at\nhttps://github.com/Haochen-Wang409/PairUni.",
      "authors": [
        "Jiani Zheng",
        "Zhiyang Teng",
        "Xiangtai Li",
        "Anran Wang",
        "Yu Tian",
        "Kunpeng Qiu",
        "Ye Tian",
        "Haochen Wang",
        "Zhuochen Wang"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25682v2",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25682v2"
    },
    {
      "url": "https://arxiv.org/abs/2510.25677v1",
      "title": "ZK-SenseLM: Verifiable Large-Model Wireless Sensing with Selective Abstention and Zero-Knowledge Attestation",
      "abstract": "ZK-SenseLM is a secure and auditable wireless sensing framework that pairs a\nlarge-model encoder for Wi-Fi channel state information (and optionally mmWave\nradar or RFID) with a policy-grounded decision layer and end-to-end\nzero-knowledge proofs of inference. The encoder uses masked spectral\npretraining with phase-consistency regularization, plus a light cross-modal\nalignment that ties RF features to compact, human-interpretable policy tokens.\nTo reduce unsafe actions under distribution shift, we add a calibrated\nselective-abstention head; the chosen risk-coverage operating point is\nregistered and bound into the proof. We implement a four-stage proving\npipeline: (C1) feature sanity and commitment, (C2) threshold and version\nbinding, (C3) time-window binding, and (C4) PLONK-style proofs that the\nquantized network, given the committed window, produced the logged action and\nconfidence. Micro-batched proving amortizes cost across adjacent windows, and a\ngateway option offloads proofs from low-power devices. The system integrates\nwith differentially private federated learning and on-device personalization\nwithout weakening verifiability: model hashes and the registered threshold are\npart of each public statement. Across activity, presence or intrusion,\nrespiratory proxy, and RF fingerprinting tasks, ZK-SenseLM improves macro-F1\nand calibration, yields favorable coverage-risk curves under perturbations, and\nrejects tamper and replay with compact proofs and fast verification.",
      "authors": [
        "Hasan Akgul",
        "Mari Eplik",
        "Javier Rojas",
        "Aina Binti Abdullah",
        "Pieter van der Merwe"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25677v1",
      "categories": [
        "cs.CR",
        "cs.CL",
        "C.2.1; D.4.6; E.3; I.2.6; I.5.4"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25677v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25816v1",
      "title": "Beyond Long Context: When Semantics Matter More than Tokens",
      "abstract": "Electronic Health Records (EHR) store clinical documentation as base64\nencoded attachments in FHIR DocumentReference resources, which makes semantic\nquestion answering difficult. Traditional vector database methods often miss\nnuanced clinical relationships. The Clinical Entity Augmented Retrieval (CLEAR)\nmethod, introduced by Lopez et al. 2025, uses entity aware retrieval and\nachieved improved performance with an F1 score of 0.90 versus 0.86 for\nembedding based retrieval, while using over 70 percent fewer tokens. We\ndeveloped a Clinical Notes QA Evaluation Platform to validate CLEAR against\nzero shot large context inference and traditional chunk based retrieval\naugmented generation. The platform was tested on 12 clinical notes ranging from\n10,000 to 65,000 tokens representing realistic EHR content. CLEAR achieved a\n58.3 percent win rate, an average semantic similarity of 0.878, and used 78\npercent fewer tokens than wide context processing. The largest performance\ngains occurred on long notes, with a 75 percent win rate for documents\nexceeding 65,000 tokens. These findings confirm that entity aware retrieval\nimproves both efficiency and accuracy in clinical natural language processing.\nThe evaluation framework provides a reusable and transparent benchmark for\nassessing clinical question answering systems where semantic precision and\ncomputational efficiency are critical.",
      "authors": [
        "Tarun Kumar Chawdhury",
        "Jon D. Duke"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25816v1",
      "categories": [
        "cs.CL",
        "cs.LG",
        "68T50, 68T07",
        "I.2.7; H.3.3"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25816v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25628v1",
      "title": "EHR-R1: A Reasoning-Enhanced Foundational Language Model for Electronic Health Record Analysis",
      "abstract": "Electronic Health Records (EHRs) contain rich yet complex information, and\ntheir automated analysis is critical for clinical decision-making. Despite\nrecent advances of large language models (LLMs) in clinical workflows, their\nability to analyze EHRs remains limited due to narrow task coverage and lack of\nEHR-oriented reasoning capabilities. This paper aims to bridge the gap,\nspecifically, we present EHR-Ins, a large-scale, comprehensive EHR reasoning\ninstruction dataset, comprising 300k high-quality reasoning cases and 4M\nnon-reasoning cases across 42 distinct EHR tasks. Its core innovation is a\nthinking-graph-driven framework that enables to generate high-quality reasoning\ndata at scale. Based on it, we develop EHR-R1, a series of reasoning-enhanced\nLLMs with up to 72B parameters tailored for EHR analysis. Through a multi-stage\ntraining paradigm, including domain adaptation, reasoning enhancement, and\nreinforcement learning, EHR-R1 systematically acquires domain knowledge and\ndiverse reasoning capabilities, enabling accurate and robust EHR analysis.\nLastly, we introduce EHR-Bench, a new benchmark curated from MIMIC-IV, spanning\n42 tasks, to comprehensively assess reasoning and prediction across EHR\nscenarios. In experiments, we show that the resulting EHR-R1 consistently\noutperforms state-of-the-art commercial and open-source LLMs (including\nDeepSeek-V3 and GPT-4o), surpassing GPT-4o by over 30 points on MIMIC-Bench and\nachieving a 10\\% higher zero-shot AUROC on EHRSHOT. Collectively, EHR-Ins,\nEHR-R1, and EHR-Bench have significantly advanced the development for more\nreliable and clinically relevant EHR analysis.",
      "authors": [
        "Yusheng Liao",
        "Chaoyi Wu",
        "Junwei Liu",
        "Shuyang Jiang",
        "Pengcheng Qiu",
        "Haowen Wang",
        "Yun Yue",
        "Shuai Zhen",
        "Jian Wang",
        "Qianrui Fan",
        "Jinjie Gu",
        "Ya Zhang",
        "Yanfeng Wang",
        "Yu Wang",
        "Weidi Xie"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25628v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25628v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25626v1",
      "title": "Are Language Models Efficient Reasoners? A Perspective from Logic Programming",
      "abstract": "Modern language models (LMs) exhibit strong deductive reasoning capabilities,\nyet standard evaluations emphasize correctness while overlooking a key aspect\nof human-like reasoning: efficiency. In real-world reasoning scenarios, much of\nthe available information is irrelevant, and effective deductive inference\nrequires identifying and ignoring such distractions. We propose a framework for\nassessing LM reasoning efficiency through the lens of logic programming,\nintroducing a simple method to align proofs written in natural language -- as\ngenerated by an LM -- with shortest proofs found by executing the logic\nprogram. Efficiency is quantified by measuring how well a model avoids\nunnecessary inference. Empirically, we construct a dataset of math word\nproblems injected with various number of irrelevant axioms that vary in\nsemantic overlap with the goal theorem. We find that current LMs show marked\naccuracy declines under such conditions -- even with minimal, domain-consistent\ndistractions -- and the proofs they generate frequently exhibit detours through\nirrelevant inferences.",
      "authors": [
        "Andreas Opedal",
        "Yanick Zengaffinen",
        "Haruki Shirakami",
        "Clemente Pasti",
        "Mrinmaya Sachan",
        "Abulhair Saparov",
        "Ryan Cotterell",
        "Bernhard Schölkopf"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25626v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.LO"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25626v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25623v2",
      "title": "Evaluating the Role of Verifiers in Test-Time Scaling for Legal Reasoning Tasks",
      "abstract": "Test-time scaling (TTS) techniques can improve the performance of large\nlanguage models (LLMs) at the expense of additional computation and latency.\nWhile TTS has proven effective in formal domains such as mathematics and\nprogramming, its value in argumentative domains such as law remains\nunderexplored. We present an empirical study of verifier-based TTS methods for\nlegal multiple-choice QA (MCQA) across five benchmarks. Using a family of 7\nreward models, we evaluate both outcome-level (Best-of-$N$) and process-level\n(tree search) verification under realistic low-$N$ budgets. Our analysis\nsystematically investigates how verifier utility is affected by key properties\nsuch as domain specialization, model size, and supervision type\n(process-supervised PRMs vs. outcome-only ORMs), even when applied across\ndifferent roles.",
      "authors": [
        "Davide Romano",
        "Jonathan Schwarz",
        "Daniele Giofré"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25623v2",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25623v2"
    },
    {
      "url": "https://arxiv.org/abs/2510.25621v1",
      "title": "FARSIQA: Faithful and Advanced RAG System for Islamic Question Answering",
      "abstract": "The advent of Large Language Models (LLMs) has revolutionized Natural\nLanguage Processing, yet their application in high-stakes, specialized domains\nlike religious question answering is hindered by challenges like hallucination\nand unfaithfulness to authoritative sources. This issue is particularly\ncritical for the Persian-speaking Muslim community, where accuracy and\ntrustworthiness are paramount. Existing Retrieval-Augmented Generation (RAG)\nsystems, relying on simplistic single-pass pipelines, fall short on complex,\nmulti-hop queries requiring multi-step reasoning and evidence aggregation. To\naddress this gap, we introduce FARSIQA, a novel, end-to-end system for Faithful\nAdvanced Question Answering in the Persian Islamic domain. FARSIQA is built\nupon our innovative FAIR-RAG architecture: a Faithful, Adaptive, Iterative\nRefinement framework for RAG. FAIR-RAG employs a dynamic, self-correcting\nprocess: it adaptively decomposes complex queries, assesses evidence\nsufficiency, and enters an iterative loop to generate sub-queries,\nprogressively filling information gaps. Operating on a curated knowledge base\nof over one million authoritative Islamic documents, FARSIQA demonstrates\nsuperior performance. Rigorous evaluation on the challenging IslamicPCQA\nbenchmark shows state-of-the-art performance: the system achieves a remarkable\n97.0% in Negative Rejection - a 40-point improvement over baselines - and a\nhigh Answer Correctness score of 74.3%. Our work establishes a new standard for\nPersian Islamic QA and validates that our iterative, adaptive architecture is\ncrucial for building faithful, reliable AI systems in sensitive domains.",
      "authors": [
        "Mohammad Aghajani Asl",
        "Behrooz Minaei Bidgoli"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25621v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "68T50, 68T05, 68T30",
        "I.2.7; H.3.3"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25621v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25595v1",
      "title": "Communication and Verification in LLM Agents towards Collaboration under Information Asymmetry",
      "abstract": "While Large Language Model (LLM) agents are often approached from the angle\nof action planning/generation to accomplish a goal (e.g., given by language\ndescriptions), their abilities to collaborate with each other to achieve a\njoint goal are not well explored. To address this limitation, this paper\nstudies LLM agents in task collaboration, particularly under the condition of\ninformation asymmetry, where agents have disparities in their knowledge and\nskills and need to work together to complete a shared task. We extend Einstein\nPuzzles, a classical symbolic puzzle, to a table-top game. In this game, two\nLLM agents must reason, communicate, and act to satisfy spatial and relational\nconstraints required to solve the puzzle. We apply a fine-tuning-plus-verifier\nframework in which LLM agents are equipped with various communication\nstrategies and verification signals from the environment. Empirical results\nhighlight the critical importance of aligned communication, especially when\nagents possess both information-seeking and -providing capabilities.\nInterestingly, agents without communication can still achieve high task\nperformance; however, further analysis reveals a lack of true rule\nunderstanding and lower trust from human evaluators. Instead, by integrating an\nenvironment-based verifier, we enhance agents' ability to comprehend task rules\nand complete tasks, promoting both safer and more interpretable collaboration\nin AI systems. https://github.com/Roihn/EinsteinPuzzles",
      "authors": [
        "Run Peng",
        "Ziqiao Ma",
        "Amy Pang",
        "Sikai Li",
        "Zhang Xi-Jia",
        "Yingzhuo Yu",
        "Cristian-Paul Bara",
        "Joyce Chai"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25595v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25595v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25577v1",
      "title": "Lost in Phonation: Voice Quality Variation as an Evaluation Dimension for Speech Foundation Models",
      "abstract": "Recent advances in speech foundation models (SFMs) have enabled the direct\nprocessing of spoken language from raw audio, bypassing intermediate textual\nrepresentations. This capability allows SFMs to be exposed to, and potentially\nrespond to, rich paralinguistic variations embedded in the input speech signal.\nOne under-explored dimension of paralinguistic variation is voice quality,\nencompassing phonation types such as creaky and breathy voice. These phonation\ntypes are known to influence how listeners infer affective state, stance and\nsocial meaning in speech. Existing benchmarks for speech understanding largely\nrely on multiple-choice question answering (MCQA) formats, which are prone to\nfailure and therefore unreliable in capturing the nuanced ways paralinguistic\nfeatures influence model behaviour. In this paper, we probe SFMs through\nopen-ended generation tasks and speech emotion recognition, evaluating whether\nmodel behaviours are consistent across different phonation inputs. We introduce\na new parallel dataset featuring synthesized modifications to voice quality,\ndesigned to evaluate SFM responses to creaky and breathy voice. Our work\nprovides the first examination of SFM sensitivity to these particular\nnon-lexical aspects of speech perception.",
      "authors": [
        "Harm Lameris",
        "Shree Harsha Bokkahalli Satish",
        "Joakim Gustafson",
        "Éva Székely"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25577v1",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25577v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25557v1",
      "title": "Hybrid Quantum-Classical Recurrent Neural Networks",
      "abstract": "We present a hybrid quantum-classical recurrent neural network (QRNN)\narchitecture in which the entire recurrent core is realized as a parametrized\nquantum circuit (PQC) controlled by a classical feedforward network. The hidden\nstate is the quantum state of an $n$-qubit PQC, residing in an exponentially\nlarge Hilbert space $\\mathbb{C}^{2^n}$. The PQC is unitary by construction,\nmaking the hidden-state evolution norm-preserving without external constraints.\nAt each timestep, mid-circuit readouts are combined with the input embedding\nand processed by the feedforward network, which provides explicit classical\nnonlinearity. The outputs parametrize the PQC, which updates the hidden state\nvia unitary dynamics. The QRNN is compact and physically consistent, and it\nunifies (i) unitary recurrence as a high-capacity memory, (ii) partial\nobservation via mid-circuit measurements, and (iii) nonlinear classical control\nfor input-conditioned parametrization. We evaluate the model in simulation with\nup to 14 qubits on sentiment analysis, MNIST, permuted MNIST, copying memory,\nand language modeling, adopting projective measurements as a limiting case to\nobtain mid-circuit readouts while maintaining a coherent recurrent quantum\nmemory. We further devise a soft attention mechanism over the mid-circuit\nreadouts in a sequence-to-sequence model and show its effectiveness for machine\ntranslation. To our knowledge, this is the first model (RNN or otherwise)\ngrounded in quantum operations to achieve competitive performance against\nstrong classical baselines across a broad class of sequence-learning tasks.",
      "authors": [
        "Wenduan Xu"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25557v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "quant-ph"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25557v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25536v2",
      "title": "TwinVoice: A Multi-dimensional Benchmark Towards Digital Twins via LLM Persona Simulation",
      "abstract": "Large Language Models (LLMs) are exhibiting emergent human-like abilities and\nare increasingly envisioned as the foundation for simulating an individual's\ncommunication style, behavioral tendencies, and personality traits. However,\ncurrent evaluations of LLM-based persona simulation remain limited: most rely\non synthetic dialogues, lack systematic frameworks, and lack analysis of the\ncapability requirement. To address these limitations, we introduce TwinVoice, a\ncomprehensive benchmark for assessing persona simulation across diverse\nreal-world contexts. TwinVoice encompasses three dimensions: Social Persona\n(public social interactions), Interpersonal Persona (private dialogues), and\nNarrative Persona (role-based expression). It further decomposes the evaluation\nof LLM performance into six fundamental capabilities, including opinion\nconsistency, memory recall, logical reasoning, lexical fidelity, persona tone,\nand syntactic style. Experimental results reveal that while advanced models\nachieve moderate accuracy in persona simulation, they still fall short of\ncapabilities such as syntactic style and memory recall. Consequently, the\naverage performance achieved by LLMs remains considerably below the human\nbaseline.",
      "authors": [
        "Bangde Du",
        "Minghao Guo",
        "Songming He",
        "Ziyi Ye",
        "Xi Zhu",
        "Weihang Su",
        "Shuqi Zhu",
        "Yujia Zhou",
        "Yongfeng Zhang",
        "Qingyao Ai",
        "Yiqun Liu"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25536v2",
      "categories": [
        "cs.CL",
        "I.2.7; I.2.6; I.2.0"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25536v2"
    },
    {
      "url": "https://arxiv.org/abs/2510.25460v1",
      "title": "Fine-Tuned Language Models for Domain-Specific Summarization and Tagging",
      "abstract": "This paper presents a pipeline integrating fine-tuned large language models\n(LLMs) with named entity recognition (NER) for efficient domain-specific text\nsummarization and tagging. The authors address the challenge posed by rapidly\nevolving sub-cultural languages and slang, which complicate automated\ninformation extraction and law enforcement monitoring. By leveraging the LLaMA\nFactory framework, the study fine-tunes LLMs on both generalpurpose and custom\ndomain-specific datasets, particularly in the political and security domains.\nThe models are evaluated using BLEU and ROUGE metrics, demonstrating that\ninstruction fine-tuning significantly enhances summarization and tagging\naccuracy, especially for specialized corpora. Notably, the LLaMA3-8B-Instruct\nmodel, despite its initial limitations in Chinese comprehension, outperforms\nits Chinese-trained counterpart after domainspecific fine-tuning, suggesting\nthat underlying reasoning capabilities can transfer across languages. The\npipeline enables concise summaries and structured entity tagging, facilitating\nrapid document categorization and distribution. This approach proves scalable\nand adaptable for real-time applications, supporting efficient information\nmanagement and the ongoing need to capture emerging language trends. The\nintegration of LLMs and NER offers a robust solution for transforming\nunstructured text into actionable insights, crucial for modern knowledge\nmanagement and security operations.",
      "authors": [
        "Jun Wang",
        "Fuming Lin",
        "Yuyu Chen"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25460v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25460v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25441v1",
      "title": "Grounded in Reality: Learning and Deploying Proactive LLM from Offline Logs",
      "abstract": "Large Language Models (LLMs) excel as passive responders, but teaching them\nto be proactive, goal-oriented partners, a critical capability in high-stakes\ndomains, remains a major challenge. Current paradigms either myopically\noptimize single-turn attributes or rely on brittle, high-cost user simulators,\ncreating a persistent ``reality gap''. To bridge this gap, we introduce\n\\texttt{Learn-to-Ask}, a general, simulator-free framework for learning and\ndeploying proactive dialogue agents \\textit{directly from offline expert data},\nbypassing the need to model complex user dynamics. Our key insight is to\nreframe the offline policy learning problem by leveraging the \\textbf{observed\nfuture} of each expert trajectory. This allows us to infer a dense,\nturn-by-turn reward signal grounded in the expert's revealed strategy,\ndecomposing the intractable long-horizon problem into a series of supervised\nlearning tasks, and training a policy to output a structured \\texttt{(action,\nstate_assessment)} tuple, governing both \\textbf{what to ask} and, crucially,\n\\textbf{when to stop}. To ensure reward fidelity, our Automated Grader\nCalibration pipeline systematically purges noise from the LLM-based reward\nmodel with minimal human supervision. Empirically, we demonstrate the efficacy\nof \\texttt{Learn-to-Ask} in a real-world medical dataset, using LLMs of varying\nsizes up to 32B. Our approach culminates in the successful deployment of LLMs\ninto a live, large-scale online AI service. In rigorous in-house evaluations,\nour model was launched and achieved performance even superior to human experts,\nproving our framework's ability to translate offline data into tangible,\nreal-world impact. We hope this work provides a practical and economically\nviable blueprint for transforming passive LLMs into proactive, goal-oriented\nLLM applications.",
      "authors": [
        "Fei Wei",
        "Daoyuan Chen",
        "Ce Wang",
        "Yilun Huang",
        "Yushuo Chen",
        "Xuchen Pan",
        "Yaliang Li",
        "Bolin Ding"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25441v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25441v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25440v1",
      "title": "More than a Moment: Towards Coherent Sequences of Audio Descriptions",
      "abstract": "Audio Descriptions (ADs) convey essential on-screen information, allowing\nvisually impaired audiences to follow videos. To be effective, ADs must form a\ncoherent sequence that helps listeners to visualise the unfolding scene, rather\nthan describing isolated moments. However, most automatic methods generate each\nAD independently, often resulting in repetitive, incoherent descriptions. To\naddress this, we propose a training-free method, CoherentAD, that first\ngenerates multiple candidate descriptions for each AD time interval, and then\nperforms auto-regressive selection across the sequence to form a coherent and\ninformative narrative. To evaluate AD sequences holistically, we introduce a\nsequence-level metric, StoryRecall, which measures how well the predicted ADs\nconvey the ground truth narrative, alongside repetition metrics that capture\nthe redundancy across consecutive AD outputs. Our method produces coherent AD\nsequences with enhanced narrative understanding, outperforming prior approaches\nthat rely on independent generations.",
      "authors": [
        "Eshika Khandelwal",
        "Junyu Xie",
        "Tengda Han",
        "Max Bain",
        "Arsha Nagrani",
        "Andrew Zisserman",
        "Gül Varol",
        "Makarand Tapaswi"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25440v1",
      "categories": [
        "cs.CV",
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25440v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25434v1",
      "title": "A Critical Study of Automatic Evaluation in Sign Language Translation",
      "abstract": "Automatic evaluation metrics are crucial for advancing sign language\ntranslation (SLT). Current SLT evaluation metrics, such as BLEU and ROUGE, are\nonly text-based, and it remains unclear to what extent text-based metrics can\nreliably capture the quality of SLT outputs. To address this gap, we\ninvestigate the limitations of text-based SLT evaluation metrics by analyzing\nsix metrics, including BLEU, chrF, and ROUGE, as well as BLEURT on the one\nhand, and large language model (LLM)-based evaluators such as G-Eval and GEMBA\nzero-shot direct assessment on the other hand. Specifically, we assess the\nconsistency and robustness of these metrics under three controlled conditions:\nparaphrasing, hallucinations in model outputs, and variations in sentence\nlength. Our analysis highlights the limitations of lexical overlap metrics and\ndemonstrates that while LLM-based evaluators better capture semantic\nequivalence often missed by conventional metrics, they can also exhibit bias\ntoward LLM-paraphrased translations. Moreover, although all metrics are able to\ndetect hallucinations, BLEU tends to be overly sensitive, whereas BLEURT and\nLLM-based evaluators are comparatively lenient toward subtle cases. This\nmotivates the need for multimodal evaluation frameworks that extend beyond\ntext-based metrics to enable a more holistic assessment of SLT outputs.",
      "authors": [
        "Shakib Yazdani",
        "Yasser Hamidullah",
        "Cristina España-Bonet",
        "Eleftherios Avramidis",
        "Josef van Genabith"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25434v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25434v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25432v1",
      "title": "Depth and Autonomy: A Framework for Evaluating LLM Applications in Social Science Research",
      "abstract": "Large language models (LLMs) are increasingly utilized by researchers across\na wide range of domains, and qualitative social science is no exception;\nhowever, this adoption faces persistent challenges, including interpretive\nbias, low reliability, and weak auditability. We introduce a framework that\nsituates LLM usage along two dimensions, interpretive depth and autonomy,\nthereby offering a straightforward way to classify LLM applications in\nqualitative research and to derive practical design recommendations. We present\nthe state of the literature with respect to these two dimensions, based on all\npublished social science papers available on Web of Science that use LLMs as a\ntool and not strictly as the subject of study. Rather than granting models\nexpansive freedom, our approach encourages researchers to decompose tasks into\nmanageable segments, much as they would when delegating work to capable\nundergraduate research assistants. By maintaining low levels of autonomy and\nselectively increasing interpretive depth only where warranted and under\nsupervision, one can plausibly reap the benefits of LLMs while preserving\ntransparency and reliability.",
      "authors": [
        "Ali Sanaei",
        "Ali Rajabzadeh"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25432v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25432v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25427v1",
      "title": "RLMEval: Evaluating Research-Level Neural Theorem Proving",
      "abstract": "Despite impressive results on curated benchmarks, the practical impact of\nlarge language models (LLMs) on research-level neural theorem proving and proof\nautoformalization is still limited. We introduce RLMEval, an evaluation suite\nfor these tasks, focusing on research-level mathematics from real-world Lean\nformalization projects. RLMEval targets the evaluation of neural theorem\nproving and proof autoformalization on challenging research-level theorems by\nleveraging real Lean Blueprint formalization projects. Our evaluation of\nstate-of-the-art models on RLMEval, comprising 613 theorems from 6 Lean\nprojects, reveals a significant gap: progress on existing benchmarks does not\nreadily translate to these more realistic settings, with the best model\nachieving only a 10.3 % pass rate. RLMEval provides a new, challenging\nbenchmark designed to guide and accelerate progress in automated reasoning for\nformal mathematics.",
      "authors": [
        "Auguste Poiroux",
        "Antoine Bosselut",
        "Viktor Kunčak"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25427v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25427v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25426v1",
      "title": "Implicature in Interaction: Understanding Implicature Improves Alignment in Human-LLM Interaction",
      "abstract": "The rapid advancement of Large Language Models (LLMs) is positioning language\nat the core of human-computer interaction (HCI). We argue that advancing HCI\nrequires attention to the linguistic foundations of interaction, particularly\nimplicature (meaning conveyed beyond explicit statements through shared\ncontext) which is essential for human-AI (HAI) alignment. This study examines\nLLMs' ability to infer user intent embedded in context-driven prompts and\nwhether understanding implicature improves response generation. Results show\nthat larger models approximate human interpretations more closely, while\nsmaller models struggle with implicature inference. Furthermore,\nimplicature-based prompts significantly enhance the perceived relevance and\nquality of responses across models, with notable gains in smaller models.\nOverall, 67.6% of participants preferred responses with implicature-embedded\nprompts to literal ones, highlighting a clear preference for contextually\nnuanced communication. Our work contributes to understanding how linguistic\ntheory can be used to address the alignment problem by making HAI interaction\nmore natural and contextually grounded.",
      "authors": [
        "Asutosh Hota",
        "Jussi P. P. Jokinen"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25426v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25426v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25413v1",
      "title": "Seeing, Signing, and Saying: A Vision-Language Model-Assisted Pipeline for Sign Language Data Acquisition and Curation from Social Media",
      "abstract": "Most existing sign language translation (SLT) datasets are limited in scale,\nlack multilingual coverage, and are costly to curate due to their reliance on\nexpert annotation and controlled recording setup. Recently, Vision Language\nModels (VLMs) have demonstrated strong capabilities as evaluators and real-time\nassistants. Despite these advancements, their potential remains untapped in the\ncontext of sign language dataset acquisition. To bridge this gap, we introduce\nthe first automated annotation and filtering framework that utilizes VLMs to\nreduce reliance on manual effort while preserving data quality. Our method is\napplied to TikTok videos across eight sign languages and to the already curated\nYouTube-SL-25 dataset in German Sign Language for the purpose of additional\nevaluation. Our VLM-based pipeline includes a face visibility detection, a sign\nactivity recognition, a text extraction from video content, and a judgment step\nto validate alignment between video and text, implementing generic filtering,\nannotation and validation steps. Using the resulting corpus, TikTok-SL-8, we\nassess the performance of two off-the-shelf SLT models on our filtered dataset\nfor German and American Sign Languages, with the goal of establishing baselines\nand evaluating the robustness of recent models on automatically extracted,\nslightly noisy data. Our work enables scalable, weakly supervised pretraining\nfor SLT and facilitates data acquisition from social media.",
      "authors": [
        "Shakib Yazdani",
        "Yasser Hamidullah",
        "Cristina España-Bonet",
        "Josef van Genabith"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25413v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25413v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25412v1",
      "title": "Serve Programs, Not Prompts",
      "abstract": "Current large language model (LLM) serving systems, primarily designed for\ntext completion, are neither efficient nor adaptable for increasingly complex\nLLM applications due to their inflexible design. We propose a new LLM serving\nsystem architecture that serves programs instead of prompts to address this\nproblem. These programs, called LLM Inference Programs (LIPs), allow users to\ncustomize token prediction and KV cache management at runtime and to offload\nparts of their application logic, such as tool execution, to the server. We\ndescribe an example of this architecture through a system named Symphony, which\nfunctions as an operating system for LIPs. Symphony exposes LLM model\ncomputations via system calls and virtualizes KV cache with a dedicated file\nsystem, while ensuring GPU efficiency with a two-level process scheduling\nscheme. Symphony has the potential to open the door to a more efficient and\nextensible ecosystem for LLM applications.",
      "authors": [
        "In Gim",
        "Lin Zhong"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25412v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25412v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25409v2",
      "title": "BhashaBench V1: A Comprehensive Benchmark for the Quadrant of Indic Domains",
      "abstract": "The rapid advancement of large language models(LLMs) has intensified the need\nfor domain and culture specific evaluation. Existing benchmarks are largely\nAnglocentric and domain-agnostic, limiting their applicability to India-centric\ncontexts. To address this gap, we introduce BhashaBench V1, the first\ndomain-specific, multi-task, bilingual benchmark focusing on critical Indic\nknowledge systems. BhashaBench V1 contains 74,166 meticulously curated\nquestion-answer pairs, with 52,494 in English and 21,672 in Hindi, sourced from\nauthentic government and domain-specific exams. It spans four major domains:\nAgriculture, Legal, Finance, and Ayurveda, comprising 90+ subdomains and\ncovering 500+ topics, enabling fine-grained evaluation. Evaluation of 29+ LLMs\nreveals significant domain and language specific performance gaps, with\nespecially large disparities in low-resource domains. For instance, GPT-4o\nachieves 76.49% overall accuracy in Legal but only 59.74% in Ayurveda. Models\nconsistently perform better on English content compared to Hindi across all\ndomains. Subdomain-level analysis shows that areas such as Cyber Law,\nInternational Finance perform relatively well, while Panchakarma, Seed Science,\nand Human Rights remain notably weak. BhashaBench V1 provides a comprehensive\ndataset for evaluating large language models across India's diverse knowledge\ndomains. It enables assessment of models' ability to integrate domain-specific\nknowledge with bilingual understanding. All code, benchmarks, and resources are\npublicly available to support open research.",
      "authors": [
        "Vijay Devane",
        "Mohd Nauman",
        "Bhargav Patel",
        "Aniket Mahendra Wakchoure",
        "Yogeshkumar Sant",
        "Shyam Pawar",
        "Viraj Thakur",
        "Ananya Godse",
        "Sunil Patra",
        "Neha Maurya",
        "Suraj Racha",
        "Nitish Kamal Singh",
        "Ajay Nagpal",
        "Piyush Sawarkar",
        "Kundeshwar Vijayrao Pundalik",
        "Rohit Saluja",
        "Ganesh Ramakrishnan"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25409v2",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25409v2"
    },
    {
      "url": "https://arxiv.org/abs/2510.25384v1",
      "title": "Roleplaying with Structure: Synthetic Therapist-Client Conversation Generation from Questionnaires",
      "abstract": "The development of AI for mental health is hindered by a lack of authentic\ntherapy dialogues, due to strict privacy regulations and the fact that clinical\nsessions were historically rarely recorded. We present an LLM-driven pipeline\nthat generates synthetic counseling dialogues based on structured client\nprofiles and psychological questionnaires. Grounded on the principles of\nCognitive Behavioral Therapy (CBT), our method creates synthetic therapeutic\nconversations for clinical disorders such as anxiety and depression. Our\nframework, SQPsych (Structured Questionnaire-based Psychotherapy), converts\nstructured psychological input into natural language dialogues through\ntherapist-client simulations. Due to data governance policies and privacy\nrestrictions prohibiting the transmission of clinical questionnaire data to\nthird-party services, previous methodologies relying on proprietary models are\ninfeasible in our setting. We address this limitation by generating a\nhigh-quality corpus using open-weight LLMs, validated through human expert\nevaluation and LLM-based assessments. Our SQPsychLLM models fine-tuned on\nSQPsychConv achieve strong performance on counseling benchmarks, surpassing\nbaselines in key therapeutic skills. Our findings highlight the potential of\nsynthetic data to enable scalable, data-secure, and clinically informed AI for\nmental health support. We will release our code, models, and corpus at\nhttps://ai-mh.github.io/SQPsych",
      "authors": [
        "Doan Nam Long Vu",
        "Rui Tan",
        "Lena Moench",
        "Svenja Jule Francke",
        "Daniel Woiwod",
        "Florian Thomas-Odenthal",
        "Sanna Stroth",
        "Tilo Kircher",
        "Christiane Hermann",
        "Udo Dannlowski",
        "Hamidreza Jamalabadi",
        "Shaoxiong Ji"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25384v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25384v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25378v1",
      "title": "Hallucinations in Bibliographic Recommendation: Citation Frequency as a Proxy for Training Data Redundancy",
      "abstract": "Large language models (LLMs) have been increasingly applied to a wide range\nof tasks, from natural language understanding to code generation. While they\nhave also been used to assist in bibliographic recommendation, the\nhallucination of non-existent papers remains a major issue. Building on prior\nstudies, this study hypothesizes that an LLM's ability to correctly produce\nbibliographic information depends on whether the underlying knowledge is\ngenerated or memorized, with highly cited papers (i.e., more frequently appear\nin the training corpus) showing lower hallucination rates. We therefore assume\ncitation count as a proxy for training data redundancy (i.e., the frequency\nwith which a given bibliographic record is repeatedly represented in the\npretraining corpus) and investigate how citation frequency affects hallucinated\nreferences in LLM outputs. Using GPT-4.1, we generated and manually verified\n100 bibliographic records across twenty computer-science domains, and measured\nfactual consistency via cosine similarity between generated and authentic\nmetadata. The results revealed that (i) hallucination rates vary across\nresearch domains, (ii) citation count is strongly correlated with factual\naccuracy, and (iii) bibliographic information becomes almost verbatimly\nmemorized beyond approximately 1,000 citations. These findings suggest that\nhighly cited papers are nearly verbatimly retained in the model, indicating a\nthreshold where generalization shifts into memorization.",
      "authors": [
        "Junichiro Niimi"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25378v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25378v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25370v1",
      "title": "Monitoring Transformative Technological Convergence Through LLM-Extracted Semantic Entity Triple Graphs",
      "abstract": "Forecasting transformative technologies remains a critical but challenging\ntask, particularly in fast-evolving domains such as Information and\nCommunication Technologies (ICTs). Traditional expert-based methods struggle to\nkeep pace with short innovation cycles and ambiguous early-stage terminology.\nIn this work, we propose a novel, data-driven pipeline to monitor the emergence\nof transformative technologies by identifying patterns of technological\nconvergence.\n  Our approach leverages advances in Large Language Models (LLMs) to extract\nsemantic triples from unstructured text and construct a large-scale graph of\ntechnology-related entities and relations. We introduce a new method for\ngrouping semantically similar technology terms (noun stapling) and develop\ngraph-based metrics to detect convergence signals. The pipeline includes\nmulti-stage filtering, domain-specific keyword clustering, and a temporal trend\nanalysis of topic co-occurence.\n  We validate our methodology on two complementary datasets: 278,625 arXiv\npreprints (2017--2024) to capture early scientific signals, and 9,793 USPTO\npatent applications (2018-2024) to track downstream commercial developments.\nOur results demonstrate that the proposed pipeline can identify both\nestablished and emerging convergence patterns, offering a scalable and\ngeneralizable framework for technology forecasting grounded in full-text\nanalysis.",
      "authors": [
        "Alexander Sternfeld",
        "Andrei Kucharavy",
        "Dimitri Percia David",
        "Alain Mermoud",
        "Julian Jang-Jaccard",
        "Nathan Monnet"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25370v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25370v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25364v1",
      "title": "CLASS-IT: Conversational and Lecture-Aligned Small-Scale Instruction Tuning for BabyLMs",
      "abstract": "This work investigates whether small-scale LMs can benefit from instruction\ntuning. We compare conversational and question-answering instruction tuning\ndatasets, applied either in a merged or sequential curriculum, using\ndecoder-only models with 100M and 140M parameters. Evaluation spans both\nfine-tuning (SuperGLUE) and zero-shot (BLiMP, EWoK, WUGs, entity tracking, and\npsycholinguistic correlation) settings. Results show that instruction tuning\nyields small but consistent gains in fine-tuning scenarios, with sequential\ncurricula outperforming merged data; however, improvements do not consistently\ntransfer to zero-shot tasks, suggesting a trade-off between interaction-focused\nadaptation and broad linguistic generalization. These results highlight both\nthe potential and the constraints of adapting human-inspired learning\nstrategies to low-resource LMs, and point toward hybrid, curriculum-based\napproaches for enhancing generalization under ecological training limits.",
      "authors": [
        "Luca Capone",
        "Alessandro Bondielli",
        "Alessandro Lenci"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25364v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25364v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25356v1",
      "title": "Not ready for the bench: LLM legal interpretation is unstable and out of step with human judgments",
      "abstract": "Legal interpretation frequently involves assessing how a legal text, as\nunderstood by an 'ordinary' speaker of the language, applies to the set of\nfacts characterizing a legal dispute in the U.S. judicial system. Recent\nscholarship has proposed that legal practitioners add large language models\n(LLMs) to their interpretive toolkit. This work offers an empirical argument\nagainst LLM interpretation as recently practiced by legal scholars and federal\njudges. Our investigation in English shows that models do not provide stable\ninterpretive judgments: varying the question format can lead the model to\nwildly different conclusions. Moreover, the models show weak to moderate\ncorrelation with human judgment, with large variance across model and question\nvariant, suggesting that it is dangerous to give much credence to the\nconclusions produced by generative AI.",
      "authors": [
        "Abhishek Purushothama",
        "Junghyun Min",
        "Brandon Waldon",
        "Nathan Schneider"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25356v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25356v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25333v1",
      "title": "CRMWeaver: Building Powerful Business Agent via Agentic RL and Shared Memories",
      "abstract": "Recent years have witnessed the rapid development of LLM-based agents, which\nshed light on using language agents to solve complex real-world problems. A\nprominent application lies in business agents, which interact with databases\nand internal knowledge bases via tool calls to fulfill diverse user\nrequirements. However, this domain is characterized by intricate data\nrelationships and a wide range of heterogeneous tasks, from statistical data\nqueries to knowledge-based question-answering. To address these challenges, we\npropose CRMWeaver, a novel approach that enhances business agents in such\ncomplex settings. To acclimate the agentic model to intricate business\nenvironments, we employ a synthesis data generation and RL-based paradigm\nduring training, which significantly improves the model's ability to handle\ncomplex data and varied tasks. During inference, a shared memories mechanism is\nintroduced, prompting the agent to learn from task guidelines in similar\nproblems, thereby further boosting its effectiveness and generalization,\nespecially in unseen scenarios. We validate the efficacy of our approach on the\nCRMArena-Pro dataset, where our lightweight model achieves competitive results\nin both B2B and B2C business scenarios, underscoring its practical value for\nreal-world applications.",
      "authors": [
        "Yilong Lai",
        "Yipin Yang",
        "Jialong Wu",
        "Fengran Mo",
        "Zhenglin Wang",
        "Ting Liang",
        "Jianguo Lin",
        "Keping Yang"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25333v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25333v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25320v1",
      "title": "GAP: Graph-Based Agent Planning with Parallel Tool Use and Reinforcement Learning",
      "abstract": "Autonomous agents powered by large language models (LLMs) have shown\nimpressive capabilities in tool manipulation for complex task-solving. However,\nexisting paradigms such as ReAct rely on sequential reasoning and execution,\nfailing to exploit the inherent parallelism among independent sub-tasks. This\nsequential bottleneck leads to inefficient tool utilization and suboptimal\nperformance in multi-step reasoning scenarios. We introduce Graph-based Agent\nPlanning (GAP), a novel framework that explicitly models inter-task\ndependencies through graph-based planning to enable adaptive parallel and\nserial tool execution. Our approach trains agent foundation models to decompose\ncomplex tasks into dependency-aware sub-task graphs, autonomously determining\nwhich tools can be executed in parallel and which must follow sequential\ndependencies. This dependency-aware orchestration achieves substantial\nimprovements in both execution efficiency and task accuracy. To train GAP, we\nconstruct a high-quality dataset of graph-based planning traces derived from\nthe Multi-Hop Question Answering (MHQA) benchmark. We employ a two-stage\ntraining strategy: supervised fine-tuning (SFT) on the curated dataset,\nfollowed by reinforcement learning (RL) with a correctness-based reward\nfunction on strategically sampled queries where tool-based reasoning provides\nmaximum value. Experimental results on MHQA datasets demonstrate that GAP\nsignificantly outperforms traditional ReAct baselines, particularly on\nmulti-step retrieval tasks, while achieving dramatic improvements in tool\ninvocation efficiency through intelligent parallelization. The project page is\navailable at: https://github.com/WJQ7777/Graph-Agent-Planning.",
      "authors": [
        "Jiaqi Wu",
        "Qinlao Zhao",
        "Zefeng Chen",
        "Kai Qin",
        "Yifei Zhao",
        "Xueqian Wang",
        "Yuhang Yao"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25320v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25320v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25310v1",
      "title": "Parrot: A Training Pipeline Enhances Both Program CoT and Natural Language CoT for Reasoning",
      "abstract": "Natural language chain-of-thought (N-CoT) and Program chain-of-thought\n(P-CoT) have emerged as two primary paradigms for large language models (LLMs)\nto solve mathematical reasoning problems. Current research typically endeavors\nto achieve unidirectional enhancement: P-CoT enhanced N-CoT or N-CoT enhanced\nP-CoT. In this paper, we seek to fully unleash the two paradigms' strengths for\nmutual enhancement and ultimately achieve simultaneous improvements. We conduct\na detailed analysis of the error types across two paradigms, based on which we\npropose Parrot, a novel training pipeline for mathematical problems: 1) Three\ntarget-designed subtasks integrate sequential P-CoT and N-CoT generation. 2) A\nsubtask hybrid training strategy to facilitate natural language semantic\ntransferability. 3) The converted N-CoT auxiliary reward is designed to\nalleviate the sparse rewards in P-CoT optimization. Extensive experiments\ndemonstrate that Parrot significantly enhances both the performance of N-CoT\nand P-CoT, especially on N-CoT. Using Parrot SFT, the N-CoT performance of\nLLaMA2 and CodeLLaMA achieve gains of +21.87 and +21.48 on MathQA over the RL\nbaseline, which is resource-intensive.",
      "authors": [
        "Senjie Jin",
        "Lu Chen",
        "Zhiheng Xi",
        "Yuhui Wang",
        "Sirui Song",
        "Yuhao Zhou",
        "Xinbo Zhang",
        "Peng Sun",
        "Hong Lu",
        "Tao Gui",
        "Qi Zhang",
        "Xuanjing Huang"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25310v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25310v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25303v1",
      "title": "Teaching Sarcasm: Few-Shot Multimodal Sarcasm Detection via Distillation to a Parameter-Efficient Student",
      "abstract": "Multimodal sarcasm detection is challenging, especially in low-resource\nsettings where subtle image-text contradictions are hard to learn due to scarce\nannotated data, which hinders the model's performance. Parameter-efficient\nfine-tuning (PEFT) methods like adapters, LoRA, and prompt tuning reduce\noverfitting but struggle to reach optimal performance due to limited\nsupervision from few-shot data. We propose PEKD, a unified framework that\nenhances PEFT methods via distillation from an expert model trained on\nlarge-scale sarcasm data, which acts as the teacher. To mitigate unreliable\nsignals from the teacher, we introduce an entropy-aware gating mechanism that\ndynamically adjusts the distillation strength based on teacher confidence.\nExperiments on two public datasets demonstrate that our PEKD framework enables\nPEFT methods to outperform both prior parameter-efficient approaches and large\nmultimodal models, achieving strong results in the few-shot scenario. The\nframework is modular and adaptable to a wide range of multimodal models and\ntasks.",
      "authors": [
        "Soumyadeep Jana",
        "Sanasam Ranbir Singh"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25303v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25303v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25273v1",
      "title": "Adapting Small Language Models to Low-Resource Domains: A Case Study in Hindi Tourism QA",
      "abstract": "Domain-specific question answering in low-resource languages faces two key\nchallenges: scarcity of annotated datasets and limited domain knowledge in\ngeneral-purpose language models. In this work, we present a multi-stage\nfinetuning strategy to adapt lightweight language models to the Hindi tourism\ndomain by leveraging both original and synthetic training data. Synthetic\nquestion-answer pairs are generated using large LLMs (LLaMA-70B, Phi-14B) and\nused to augment the limited original dataset. We explore several training\nmethodologies and analyse their impact on domain generalisation. Our results\ndemonstrate that large models can efficiently generate synthetic data, while\nsmall models can effectively adapt to it, offering a scalable pathway for\nlow-resource, domain-specific QA.",
      "authors": [
        "Sandipan Majhi",
        "Paheli Bhattacharya"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25273v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25273v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25232v1",
      "title": "From Medical Records to Diagnostic Dialogues: A Clinical-Grounded Approach and Dataset for Psychiatric Comorbidity",
      "abstract": "Psychiatric comorbidity is clinically significant yet challenging due to the\ncomplexity of multiple co-occurring disorders. To address this, we develop a\nnovel approach integrating synthetic patient electronic medical record (EMR)\nconstruction and multi-agent diagnostic dialogue generation. We create 502\nsynthetic EMRs for common comorbid conditions using a pipeline that ensures\nclinical relevance and diversity. Our multi-agent framework transfers the\nclinical interview protocol into a hierarchical state machine and context tree,\nsupporting over 130 diagnostic states while maintaining clinical standards.\nThrough this rigorous process, we construct PsyCoTalk, the first large-scale\ndialogue dataset supporting comorbidity, containing 3,000 multi-turn diagnostic\ndialogues validated by psychiatrists. This dataset enhances diagnostic accuracy\nand treatment planning, offering a valuable resource for psychiatric\ncomorbidity research. Compared to real-world clinical transcripts, PsyCoTalk\nexhibits high structural and linguistic fidelity in terms of dialogue length,\ntoken distribution, and diagnostic reasoning strategies. Licensed psychiatrists\nconfirm the realism and diagnostic validity of the dialogues. This dataset\nenables the development and evaluation of models capable of multi-disorder\npsychiatric screening in a single conversational pass.",
      "authors": [
        "Tianxi Wan",
        "Jiaming Luo",
        "Siyuan Chen",
        "Kunyao Lan",
        "Jianhua Chen",
        "Haiyang Geng",
        "Mengyue Wu"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25232v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25232v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25224v1",
      "title": "ProMediate: A Socio-cognitive framework for evaluating proactive agents in multi-party negotiation",
      "abstract": "While Large Language Models (LLMs) are increasingly used in agentic\nframeworks to assist individual users, there is a growing need for agents that\ncan proactively manage complex, multi-party collaboration. Systematic\nevaluation methods for such proactive agents remain scarce, limiting progress\nin developing AI that can effectively support multiple people together.\nNegotiation offers a demanding testbed for this challenge, requiring\nsocio-cognitive intelligence to navigate conflicting interests between multiple\nparticipants and multiple topics and build consensus. Here, we present\nProMediate, the first framework for evaluating proactive AI mediator agents in\ncomplex, multi-topic, multi-party negotiations. ProMediate consists of two core\ncomponents: (i) a simulation testbed based on realistic negotiation cases and\ntheory-driven difficulty levels (ProMediate-Easy, ProMediate-Medium, and\nProMediate-Hard), with a plug-and-play proactive AI mediator grounded in\nsocio-cognitive mediation theories, capable of flexibly deciding when and how\nto intervene; and (ii) a socio-cognitive evaluation framework with a new suite\nof metrics to measure consensus changes, intervention latency, mediator\neffectiveness, and intelligence. Together, these components establish a\nsystematic framework for assessing the socio-cognitive intelligence of\nproactive AI agents in multi-party settings. Our results show that a socially\nintelligent mediator agent outperforms a generic baseline, via faster,\nbetter-targeted interventions. In the ProMediate-Hard setting, our social\nmediator increases consensus change by 3.6 percentage points compared to the\ngeneric baseline (10.65\\% vs 7.01\\%) while being 77\\% faster in response\n(15.98s vs. 3.71s). In conclusion, ProMediate provides a rigorous,\ntheory-grounded testbed to advance the development of proactive, socially\nintelligent agents.",
      "authors": [
        "Ziyi Liu",
        "Bahar Sarrafzadeh",
        "Pei Zhou",
        "Longqi Yang",
        "Jieyu Zhao",
        "Ashish Sharma"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25224v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25224v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25805v1",
      "title": "Ideology-Based LLMs for Content Moderation",
      "abstract": "Large language models (LLMs) are increasingly used in content moderation\nsystems, where ensuring fairness and neutrality is essential. In this study, we\nexamine how persona adoption influences the consistency and fairness of harmful\ncontent classification across different LLM architectures, model sizes, and\ncontent modalities (language vs. vision). At first glance, headline performance\nmetrics suggest that personas have little impact on overall classification\naccuracy. However, a closer analysis reveals important behavioral shifts.\nPersonas with different ideological leanings display distinct propensities to\nlabel content as harmful, showing that the lens through which a model \"views\"\ninput can subtly shape its judgments. Further agreement analyses highlight that\nmodels, particularly larger ones, tend to align more closely with personas from\nthe same political ideology, strengthening within-ideology consistency while\nwidening divergence across ideological groups. To show this effect more\ndirectly, we conducted an additional study on a politically targeted task,\nwhich confirmed that personas not only behave more coherently within their own\nideology but also exhibit a tendency to defend their perspective while\ndownplaying harmfulness in opposing views. Together, these findings highlight\nhow persona conditioning can introduce subtle ideological biases into LLM\noutputs, raising concerns about the use of AI systems that may reinforce\npartisan perspectives under the guise of neutrality.",
      "authors": [
        "Stefano Civelli",
        "Pietro Bernardelle",
        "Nardiena A. Pratama",
        "Gianluca Demartini"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25805v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25805v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25804v1",
      "title": "Beyond Length: Quantifying Long-Range Information for Long-Context LLM Pretraining Data",
      "abstract": "Long-context language models unlock advanced capabilities in reasoning, code\ngeneration, and document summarization by leveraging dependencies across\nextended spans of text. However, a significant portion of readily available\nlong-text data lacks meaningful long-distance dependencies; most spans can be\npredicted using only local context. Training on such data is inefficient,\nmaking careful data selection crucial. Therefore, we introduce LongFilter, a\nframework for curating training data tailored to long-context pretraining.\nLongFilter measures the information gain provided by extended context by\ncontrasting model predictions under long-context versus short-context settings,\nthereby identifying samples where long-range dependencies are essential.\nExperiments with LLaMA-3-8B, extending its context length from 8K to 64K, show\nthat LongFilter efficiently selects high-quality data and yields substantial\nimprovements on benchmarks such as HELMET, LongBench, and RULER.",
      "authors": [
        "Haoran Deng",
        "Yingyu Lin",
        "Zhenghao Lin",
        "Xiao Liu",
        "Yizhou Sun",
        "Yi-An Ma",
        "Yeyun Gong"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25804v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25804v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25206v1",
      "title": "RAVR: Reference-Answer-guided Variational Reasoning for Large Language Models",
      "abstract": "Reinforcement learning (RL) can refine the reasoning abilities of large\nlanguage models (LLMs), but critically depends on a key prerequisite: the LLM\ncan already generate high-utility reasoning paths with non-negligible\nprobability. For tasks beyond the LLM's current competence, such reasoning path\ncan be hard to sample, and learning risks reinforcing familiar but suboptimal\nreasoning. We are motivated by the insight from cognitive science that Why is\nthis the answer is often an easier question than What is the answer, as it\navoids the heavy cognitive load of open-ended exploration, opting instead for\nexplanatory reconstruction-systematically retracing the reasoning that links a\nquestion to its answer. We show that LLMs can similarly leverage answers to\nderive high-quality reasoning paths. We formalize this phenomenon and prove\nthat conditioning on answer provably increases the expected utility of sampled\nreasoning paths, thereby transforming intractable problems into learnable ones.\nBuilding on this insight, we introduce RAVR (Reference-Answer-guided\nVariational Reasoning), an end-to-end framework that uses answer-conditioned\nreasoning as a variational surrogate for question-only reasoning. Experiments\nin both general and math domains demonstrate consistent improvements over\nstrong baselines. We further analyze the reasoning behavior and find that RAVR\nreduces hesitation, strengthens conclusion consolidation, and promotes\nproblem-specific strategies in reasoning.",
      "authors": [
        "Tianqianjin Lin",
        "Xi Zhao",
        "Xingyao Zhang",
        "Rujiao Long",
        "Yi Xu",
        "Zhuoren Jiang",
        "Wenbo Su",
        "Bo Zheng"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25206v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "I.2.7"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25206v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25187v1",
      "title": "Testing Cross-Lingual Text Comprehension In LLMs Using Next Sentence Prediction",
      "abstract": "While large language models are trained on massive datasets, this data is\nheavily skewed towards English. Does their impressive performance reflect\ngenuine ability or just this data advantage? To find out, we tested them in a\nsetting where they could not rely on data abundance: low-resource languages.\nBuilding on prior work Agarwal et al. (2025) that used Next Sentence Prediction\n(NSP) as a test, we created a large-scale benchmark with 10,000 questions each\nfor English (a high-resource language), Swahili (medium-resource), and Hausa\n(low-resource). We then tested several top models, including GPT-4 Turbo,\nGemini 1.5 Flash, and LLaMA 3 70B, to see how their performance holds up. The\nresults painted a clear picture of how levels of language resources impact\noutcomes. While all models excelled in English, their accuracy dropped in\nSwahili and fell sharply in Hausa, with LLaMA 3 struggling the most. The story\nbecame even more interesting when we introduced Chain-of-Thought (CoT)\nprompting. For the struggling LLaMA 3, CoT acted as a helpful guide,\nsignificantly boosting its accuracy. However, for the more capable GPT-4 and\nGemini, the same technique often backfired, leading to a kind of \"overthinking\"\nthat hurt their results in the cross-lingual context. This reveals that\nChain-of-Thought is not a universal solution; its effectiveness depends heavily\non the model's baseline capability and the specific context of the task. Our\nframework pinpoints LLM weaknesses, highlights when CoT helps or hinders\ncross-lingual NSP performance, and factors influencing their decisions.",
      "authors": [
        "Ritesh Sunil Chavan",
        "Jack Mostow"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25187v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25187v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25160v2",
      "title": "Model-Document Protocol for AI Search",
      "abstract": "AI search depends on linking large language models (LLMs) with vast external\nknowledge sources. Yet web pages, PDF files, and other raw documents are not\ninherently LLM-ready: they are long, noisy, and unstructured. Conventional\nretrieval methods treat these documents as verbatim text and return raw\npassages, leaving the burden of fragment assembly and contextual reasoning to\nthe LLM. This gap underscores the need for a new retrieval paradigm that\nredefines how models interact with documents.\n  We introduce the Model-Document Protocol (MDP), a general framework that\nformalizes how raw text is bridged to LLMs through consumable knowledge\nrepresentations. Rather than treating retrieval as passage fetching, MDP\ndefines multiple pathways that transform unstructured documents into\ntask-specific, LLM-ready inputs. These include agentic reasoning, which curates\nraw evidence into coherent context; memory grounding, which accumulates\nreusable notes to enrich reasoning; and structured leveraging, which encodes\ndocuments into formal representations such as graphs or key-value caches. All\nthree pathways share the same goal: ensuring that what reaches the LLM is not\nraw fragments but compact, structured knowledge directly consumable for\nreasoning.\n  As an instantiation, we present MDP-Agent, which realizes the protocol\nthrough an agentic process: constructing document-level gist memories for\nglobal coverage, performing diffusion-based exploration with vertical\nexploitation to uncover layered dependencies, and applying map-reduce style\nsynthesis to integrate large-scale evidence into compact yet sufficient\ncontext. Experiments on information-seeking benchmarks demonstrate that\nMDP-Agent outperforms baselines, validating both the soundness of the MDP\nframework and the effectiveness of its agentic instantiation.",
      "authors": [
        "Hongjin Qian",
        "Zheng Liu"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25160v2",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25160v2"
    },
    {
      "url": "https://arxiv.org/abs/2510.25150v1",
      "title": "Explainable Disentanglement on Discrete Speech Representations for Noise-Robust ASR",
      "abstract": "Discrete audio representations are gaining traction in speech modeling due to\ntheir interpretability and compatibility with large language models, but are\nnot always optimized for noisy or real-world environments. Building on existing\nworks that quantize Whisper embeddings for speech-to-unit modeling, we propose\ndisentangling semantic speech content from background noise in the latent\nspace. Our end-to-end model separates clean speech in the form of codebook\ntokens, while extracting interpretable noise vectors as quantization residue\nwhich are supervised via a lightweight classifier. We show that our approach\nimproves alignment between clean/noisy speech and text, producing speech tokens\nthat display a high degree of noiseinvariance, and improves ASR performance.\nKeeping Whisper frozen, we show an 82% reduction in error rate compared to\nWhisper, and 35% improvement over baseline methods on the VBDemand test set.\nFurther analyses show that the learned token space generalizes well to both\nseen and unseen acoustic conditions.",
      "authors": [
        "Shreyas Gopal",
        "Ashutosh Anshul",
        "Haoyang Li",
        "Yue Heng Yeo",
        "Hexin Liu",
        "Eng Siong Chng"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25150v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25150v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25801v1",
      "title": "Metis-SPECS: Decoupling Multimodal Learning via Self-distilled Preference-based Cold Start",
      "abstract": "Reinforcement learning (RL) with verifiable rewards has recently catalyzed a\nwave of \"MLLM-r1\" approaches that bring RL to vision language models. Most\nrepresentative paradigms begin with a cold start, typically employing\nsupervised fine-tuning (SFT), to initialize the policy before RL. However,\nSFT-based cold start adopts the reasoning paradigm intertwined with task\nsolution and output format, which may induce instruction-style overfitting,\nweakens out-of-distribution generalization, and ultimately affects downstream\nRL. We revisit the cold start along two views, its training method and data\nconstruction, and introduce the Generalization Factor (GF) coefficient to\nquantify the generalization capability under different methods. Our empirical\nstudy finds that preference-based training methods (e.g. DPO) generalizes\nbetter than SFT-based methods in cold start. Motivated by this, we propose\nSPECS-a Self-distilled, Preference-based Cold Start framework that decouples\nmultimodal learning: (1) generates introspective preference data pairs via\nself-distillation, avoiding reliance on larger teachers or manual annotation;\n(2) performs preference-based training to learn, focusing on shallow,\ntransferable surface-form criteria (format, structure, style) rather than\nmemorizing content; and (3) hands off to RL with verifiable rewards for deep\nreasoning results. Experimental results across multiple multimodal benchmarks\nshow that our decoupling learning framework yields consistent performance gains\nover strong baselines, improving MEGA-Bench by 4.1% and MathVista by 12.2%.\nAdditional experiments indicate that SPECS contributes to reducing\nin-distribution \"stuckness,\" improving exploration, stabilizing training, and\nraising the performance ceiling.",
      "authors": [
        "Kun Chen",
        "Peng Shi",
        "Haibo Qiu",
        "Zhixiong Zeng",
        "Siqi Yang",
        "Wenji Mao",
        "Lin Ma"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25801v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25801v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25799v1",
      "title": "LISTEN to Your Preferences: An LLM Framework for Multi-Objective Selection",
      "abstract": "Human experts often struggle to select the best option from a large set of\nitems with multiple competing objectives, a process bottlenecked by the\ndifficulty of formalizing complex, implicit preferences. To address this, we\nintroduce LISTEN, a framework that leverages a Large Language Model (LLM) as a\nzero-shot preference oracle, guided only by an expert's high-level priorities\nin natural language. To operate within LLM constraints like context windows and\ninference costs, we propose two iterative algorithms: LISTEN-U, which uses the\nLLM to refine a parametric utility function, and LISTEN-T, a non-parametric\nmethod that performs tournament-style selections over small batches of\nsolutions. Evaluated on diverse tasks including flight booking, shopping, and\nexam scheduling, our results show LISTEN-U excels when preferences are\nparametrically aligned (a property we measure with a novel concordance metric),\nwhile LISTEN-T offers more robust performance. This work explores a promising\ndirection for steering complex multi-objective decisions directly with natural\nlanguage, reducing the cognitive burden of traditional preference elicitation.",
      "authors": [
        "Adam S. Jovine",
        "Tinghan Ye",
        "Francis Bahk",
        "Jingjing Wang",
        "David B. Shmoys",
        "Peter I. Frazier"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25799v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25799v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25798v1",
      "title": "MemEIC: A Step Toward Continual and Compositional Knowledge Editing",
      "abstract": "The dynamic nature of information necessitates continuously updating large\nvision-language models (LVLMs). While recent knowledge editing techniques hint\nat promising directions, they often focus on editing a single modality (vision\nor language) in isolation. This prevalent practice neglects the inherent\nmultimodality of LVLMs and the continuous nature of knowledge updates,\npotentially leading to suboptimal editing outcomes when considering the\ninterplay between modalities and the need for ongoing knowledge refinement. To\naddress these limitations, we propose MemEIC, a novel method for Continual and\nCompositional Knowledge Editing (CCKE) in LVLMs. MemEIC enables compositional\nediting of both visual and textual knowledge sequentially. Our approach employs\na hybrid external-internal editor featuring a dual external memory for\ncross-modal evidence retrieval and dual LoRA adapters that facilitate\ndisentangled parameter updates for each modality. A key component is a\nbrain-inspired knowledge connector, activated selectively for compositional\nreasoning, that integrates information across different modalities. Experiments\ndemonstrate that MemEIC significantly improves performance on complex\nmultimodal questions and effectively preserves prior edits, setting a new\nbenchmark for CCKE in LVLMs.",
      "authors": [
        "Jin Seong",
        "Jiyun Park",
        "Wencke Liermann",
        "Hongseok Choi",
        "Yoonji Nam",
        "Hyun Kim",
        "Soojong Lim",
        "Namhoon Lee"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25798v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25798v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25117v1",
      "title": "A Survey on Unlearning in Large Language Models",
      "abstract": "The advancement of Large Language Models (LLMs) has revolutionized natural\nlanguage processing, yet their training on massive corpora poses significant\nrisks, including the memorization of sensitive personal data, copyrighted\nmaterial, and knowledge that could facilitate malicious activities. To mitigate\nthese issues and align with legal and ethical standards such as the \"right to\nbe forgotten\", machine unlearning has emerged as a critical technique to\nselectively erase specific knowledge from LLMs without compromising their\noverall performance. This survey provides a systematic review of over 180\npapers on LLM unlearning published since 2021, focusing exclusively on\nlarge-scale generative models. Distinct from prior surveys, we introduce novel\ntaxonomies for both unlearning methods and evaluations. We clearly categorize\nmethods into training-time, post-training, and inference-time based on the\ntraining stage at which unlearning is applied. For evaluations, we not only\nsystematically compile existing datasets and metrics but also critically\nanalyze their advantages, disadvantages, and applicability, providing practical\nguidance to the research community. In addition, we discuss key challenges and\npromising future research directions. Our comprehensive overview aims to inform\nand guide the ongoing development of secure and reliable LLMs.",
      "authors": [
        "Ruichen Qiu",
        "Jiajun Tan",
        "Jiayue Pu",
        "Honglin Wang",
        "Xiao-Shan Gao",
        "Fei Sun"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25117v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25117v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25116v1",
      "title": "Pretraining Strategies using Monolingual and Parallel Data for Low-Resource Machine Translation",
      "abstract": "This research article examines the effectiveness of various pretraining\nstrategies for developing machine translation models tailored to low-resource\nlanguages. Although this work considers several low-resource languages,\nincluding Afrikaans, Swahili, and Zulu, the translation model is specifically\ndeveloped for Lingala, an under-resourced African language, building upon the\npretraining approach introduced by Reid and Artetxe (2021), originally designed\nfor high-resource languages. Through a series of comprehensive experiments, we\nexplore different pretraining methodologies, including the integration of\nmultiple languages and the use of both monolingual and parallel data during the\npretraining phase. Our findings indicate that pretraining on multiple languages\nand leveraging both monolingual and parallel data significantly enhance\ntranslation quality. This study offers valuable insights into effective\npretraining strategies for low-resource machine translation, helping to bridge\nthe performance gap between high-resource and low-resource languages. The\nresults contribute to the broader goal of developing more inclusive and\naccurate NLP models for marginalized communities and underrepresented\npopulations. The code and datasets used in this study are publicly available to\nfacilitate further research and ensure reproducibility, with the exception of\ncertain data that may no longer be accessible due to changes in public\navailability.",
      "authors": [
        "Idriss Nguepi Nguefack",
        "Mara Finkelstein",
        "Toadoum Sari Sakayo"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25116v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25116v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25110v1",
      "title": "DEBATE: A Large-Scale Benchmark for Role-Playing LLM Agents in Multi-Agent, Long-Form Debates",
      "abstract": "Accurately modeling opinion change through social interactions is crucial for\naddressing issues like misinformation and polarization. While role-playing\nlarge language models (LLMs) offer a promising way to simulate human-like\ninteractions, existing research shows that single-agent alignment does not\nguarantee authentic multi-agent group dynamics. Current LLM role-play setups\noften produce unnatural dynamics (e.g., premature convergence), without an\nempirical benchmark to measure authentic human opinion trajectories. To bridge\nthis gap, we introduce DEBATE, the first large-scale empirical benchmark\nexplicitly designed to evaluate the authenticity of the interaction between\nmulti-agent role-playing LLMs. DEBATE contains 29,417 messages from multi-round\ndebate conversations among over 2,792 U.S.-based participants discussing 107\ncontroversial topics, capturing both publicly-expressed messages and\nprivately-reported opinions. Using DEBATE, we systematically evaluate and\nidentify critical discrepancies between simulated and authentic group dynamics.\nWe further demonstrate DEBATE's utility for aligning LLMs with human behavior\nthrough supervised fine-tuning, achieving improvements in surface-level metrics\n(e.g., ROUGE-L and message length) while highlighting limitations in deeper\nsemantic alignment (e.g., semantic similarity). Our findings highlight both the\npotential and current limitations of role-playing LLM agents for realistically\nsimulating human-like social dynamics.",
      "authors": [
        "Yun-Shiuan Chuang",
        "Ruixuan Tu",
        "Chengtao Dai",
        "Smit Vasani",
        "Binwei Yao",
        "Michael Henry Tessler",
        "Sijia Yang",
        "Dhavan Shah",
        "Robert Hawkins",
        "Junjie Hu",
        "Timothy T. Rogers"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25110v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25110v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25101v1",
      "title": "KnowCoder-A1: Incentivizing Agentic Reasoning Capability with Outcome Supervision for KBQA",
      "abstract": "Knowledge Base Question Answering (KBQA) aims to answer natural-language\nquestions over a structured Knowledge Base (KB). Recent work improves KBQA by\nadopting an agentic reasoning paradigm, in which Large Language Models (LLMs)\niteratively decompose a question, generate its corresponding logical queries,\nand interact with the KB to derive the answer. However, these methods typically\nfine-tune LLMs on reasoning trajectories synthesized via process supervision,\nwhich offers weak incentives for exploration and thus fails to strengthen the\nagentic reasoning ability. In this paper, we propose KnowCoder-A1, an LLM that\ncan autonomously perform agentic reasoning on KBs to obtain answers. To\nincentivize autonomous exploration, KnowCoder-A1 trains the LLM under\noutcome-only supervision via a multi-stage curriculum reinforcement learning\nwith an easy-to-hard curriculum. To establish foundational agentic\ncapabilities, KnowCoder-A1 first fine-tunes the LLM on a small set of\nhigh-quality trajectories obtained through outcome-based rejection sampling.\nThen, to alleviate the reward sparsity inherent in outcome-only supervision, it\napplies multi-stage curriculum RL with reward schedules that progress from easy\nto hard. Trained with outcome-only supervision, KnowCoder-A1 exhibits powerful\nreasoning behaviors and consistently outperforms prior approaches across three\nmainstream datasets. Notably, on the zero-shot subset of GrailQA, KnowCoder-A1\nachieves up to an 11.1% relative improvement while using only one-twelfth of\nthe training data, demonstrating strong agentic reasoning capabilities.",
      "authors": [
        "Zhuo Chen",
        "Fei Wang",
        "Zixuan Li",
        "Zhao Zhang",
        "Weiwei Ding",
        "Chuanguang Yang",
        "Yongjun Xu",
        "Xiaolong Jin",
        "Jiafeng Guo"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25101v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25101v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25087v1",
      "title": "BioCoref: Benchmarking Biomedical Coreference Resolution with LLMs",
      "abstract": "Coreference resolution in biomedical texts presents unique challenges due to\ncomplex domain-specific terminology, high ambiguity in mention forms, and\nlong-distance dependencies between coreferring expressions. In this work, we\npresent a comprehensive evaluation of generative large language models (LLMs)\nfor coreference resolution in the biomedical domain. Using the CRAFT corpus as\nour benchmark, we assess the LLMs' performance with four prompting experiments\nthat vary in their use of local, contextual enrichment, and domain-specific\ncues such as abbreviations and entity dictionaries. We benchmark these\napproaches against a discriminative span-based encoder, SpanBERT, to compare\nthe efficacy of generative versus discriminative methods. Our results\ndemonstrate that while LLMs exhibit strong surface-level coreference\ncapabilities, especially when supplemented with domain-grounding prompts, their\nperformance remains sensitive to long-range context and mentions ambiguity.\nNotably, the LLaMA 8B and 17B models show superior precision and F1 scores\nunder entity-augmented prompting, highlighting the potential of lightweight\nprompt engineering for enhancing LLM utility in biomedical NLP tasks.",
      "authors": [
        "Nourah M Salem",
        "Elizabeth White",
        "Michael Bada",
        "Lawrence Hunter"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25087v1",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25087v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25797v1",
      "title": "Enhancing Underwater Object Detection through Spatio-Temporal Analysis and Spatial Attention Networks",
      "abstract": "This study examines the effectiveness of spatio-temporal modeling and the\nintegration of spatial attention mechanisms in deep learning models for\nunderwater object detection. Specifically, in the first phase, the performance\nof temporal-enhanced YOLOv5 variant T-YOLOv5 is evaluated, in comparison with\nthe standard YOLOv5. For the second phase, an augmented version of T-YOLOv5 is\ndeveloped, through the addition of a Convolutional Block Attention Module\n(CBAM). By examining the effectiveness of the already pre-existing YOLOv5 and\nT-YOLOv5 models and of the newly developed T-YOLOv5 with CBAM. With CBAM, the\nresearch highlights how temporal modeling improves detection accuracy in\ndynamic marine environments, particularly under conditions of sudden movements,\npartial occlusions, and gradual motion. The testing results showed that YOLOv5\nachieved a mAP@50-95 of 0.563, while T-YOLOv5 and T-YOLOv5 with CBAM\noutperformed with mAP@50-95 scores of 0.813 and 0.811, respectively,\nhighlighting their superior accuracy and generalization in detecting complex\nobjects. The findings demonstrate that T-YOLOv5 significantly enhances\ndetection reliability compared to the standard model, while T-YOLOv5 with CBAM\nfurther improves performance in challenging scenarios, although there is a loss\nof accuracy when it comes to simpler scenarios.",
      "authors": [
        "Sai Likhith Karri",
        "Ansh Saxena"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25797v1",
      "categories": [
        "cs.CV",
        "cs.CL",
        "cs.RO"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25797v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25069v1",
      "title": "TOPol: Capturing and Explaining Multidimensional Semantic Polarity Fields and Vectors",
      "abstract": "Traditional approaches to semantic polarity in computational linguistics\ntreat sentiment as a unidimensional scale, overlooking the multidimensional\nstructure of language. This work introduces TOPol (Topic-Orientation POLarity),\na semi-unsupervised framework for reconstructing and interpreting\nmultidimensional narrative polarity fields under human-on-the-loop (HoTL)\ndefined contextual boundaries (CBs). The framework embeds documents using a\ntransformer-based large language model (tLLM), applies neighbor-tuned UMAP\nprojection, and segments topics via Leiden partitioning. Given a CB between\ndiscourse regimes A and B, TOPol computes directional vectors between\ncorresponding topic-boundary centroids, yielding a polarity field that\nquantifies fine-grained semantic displacement during regime shifts. This\nvectorial representation enables assessing CB quality and detecting polarity\nchanges, guiding HoTL CB refinement. To interpret identified polarity vectors,\nthe tLLM compares their extreme points and produces contrastive labels with\nestimated coverage. Robustness analyses show that only CB definitions (the main\nHoTL-tunable parameter) significantly affect results, confirming methodological\nstability. We evaluate TOPol on two corpora: (i) U.S. Central Bank speeches\naround a macroeconomic breakpoint, capturing non-affective semantic shifts, and\n(ii) Amazon product reviews across rating strata, where affective polarity\naligns with NRC valence. Results demonstrate that TOPol consistently captures\nboth affective and non-affective polarity transitions, providing a scalable,\ngeneralizable, and interpretable framework for context-sensitive\nmultidimensional discourse analysis.",
      "authors": [
        "Gabin Taibi",
        "Lucia Gomez"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25069v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25069v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25064v1",
      "title": "Can LLMs Estimate Cognitive Complexity of Reading Comprehension Items?",
      "abstract": "Estimating the cognitive complexity of reading comprehension (RC) items is\ncrucial for assessing item difficulty before it is administered to learners.\nUnlike syntactic and semantic features, such as passage length or semantic\nsimilarity between options, cognitive features that arise during answer\nreasoning are not readily extractable using existing NLP tools and have\ntraditionally relied on human annotation. In this study, we examine whether\nlarge language models (LLMs) can estimate the cognitive complexity of RC items\nby focusing on two dimensions-Evidence Scope and Transformation Level-that\nindicate the degree of cognitive burden involved in reasoning about the answer.\nOur experimental results demonstrate that LLMs can approximate the cognitive\ncomplexity of items, indicating their potential as tools for prior difficulty\nanalysis. Further analysis reveals a gap between LLMs' reasoning ability and\ntheir metacognitive awareness: even when they produce correct answers, they\nsometimes fail to correctly identify the features underlying their own\nreasoning process.",
      "authors": [
        "Seonjeong Hwang",
        "Hyounghun Kim",
        "Gary Geunbae Lee"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25064v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25064v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25055v1",
      "title": "GAPMAP: Mapping Scientific Knowledge Gaps in Biomedical Literature Using Large Language Models",
      "abstract": "Scientific progress is driven by the deliberate articulation of what remains\nunknown. This study investigates the ability of large language models (LLMs) to\nidentify research knowledge gaps in the biomedical literature. We define two\ncategories of knowledge gaps: explicit gaps, clear declarations of missing\nknowledge; and implicit gaps, context-inferred missing knowledge. While prior\nwork has focused mainly on explicit gap detection, we extend this line of\nresearch by addressing the novel task of inferring implicit gaps. We conducted\ntwo experiments on almost 1500 documents across four datasets, including a\nmanually annotated corpus of biomedical articles. We benchmarked both\nclosed-weight models (from OpenAI) and open-weight models (Llama and Gemma 2)\nunder paragraph-level and full-paper settings. To address the reasoning of\nimplicit gaps inference, we introduce \\textbf{\\small TABI}, a Toulmin-Abductive\nBucketed Inference scheme that structures reasoning and buckets inferred\nconclusion candidates for validation. Our results highlight the robust\ncapability of LLMs in identifying both explicit and implicit knowledge gaps.\nThis is true for both open- and closed-weight models, with larger variants\noften performing better. This suggests a strong ability of LLMs for\nsystematically identifying candidate knowledge gaps, which can support\nearly-stage research formulation, policymakers, and funding decisions. We also\nreport observed failure modes and outline directions for robust deployment,\nincluding domain adaptation, human-in-the-loop verification, and benchmarking\nacross open- and closed-weight models.",
      "authors": [
        "Nourah M Salem",
        "Elizabeth White",
        "Michael Bada",
        "Lawrence Hunter"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25055v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25055v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25054v2",
      "title": "Evaluating Emotion Recognition in Spoken Language Models on Emotionally Incongruent Speech",
      "abstract": "Advancements in spoken language processing have driven the development of\nspoken language models (SLMs), designed to achieve universal audio\nunderstanding by jointly learning text and audio representations for a wide\nrange of tasks. Although promising results have been achieved, there is growing\ndiscussion regarding these models' generalization capabilities and the extent\nto which they truly integrate audio and text modalities in their internal\nrepresentations. In this work, we evaluate four SLMs on the task of speech\nemotion recognition using a dataset of emotionally incongruent speech samples,\na condition under which the semantic content of the spoken utterance conveys\none emotion while speech expressiveness conveys another. Our results indicate\nthat SLMs rely predominantly on textual semantics rather than speech emotion to\nperform the task, indicating that text-related representations largely dominate\nover acoustic representations. We release both the code and the Emotionally\nIncongruent Synthetic Speech dataset (EMIS) to the community.",
      "authors": [
        "Pedro Corrêa",
        "João Lima",
        "Victor Moreno",
        "Lucas Ueda",
        "Paula Dornhofer Paro Costa"
      ],
      "date": "2025-10-29",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25054v2",
      "categories": [
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25054v2"
    },
    {
      "url": "https://arxiv.org/abs/2510.25017v1",
      "title": "StorageXTuner: An LLM Agent-Driven Automatic Tuning Framework for Heterogeneous Storage Systems",
      "abstract": "Automatically configuring storage systems is hard: parameter spaces are large\nand conditions vary across workloads, deployments, and versions. Heuristic and\nML tuners are often system specific, require manual glue, and degrade under\nchanges. Recent LLM-based approaches help but usually treat tuning as a\nsingle-shot, system-specific task, which limits cross-system reuse, constrains\nexploration, and weakens validation. We present StorageXTuner, an LLM\nagent-driven auto-tuning framework for heterogeneous storage engines.\nStorageXTuner separates concerns across four agents - Executor (sandboxed\nbenchmarking), Extractor (performance digest), Searcher (insight-guided\nconfiguration exploration), and Reflector (insight generation and management).\nThe design couples an insight-driven tree search with layered memory that\npromotes empirically validated insights and employs lightweight checkers to\nguard against unsafe actions. We implement a prototype and evaluate it on\nRocksDB, LevelDB, CacheLib, and MySQL InnoDB with YCSB, MixGraph, and TPC-H/C.\nRelative to out-of-the-box settings and to ELMo-Tune, StorageXTuner reaches up\nto 575% and 111% higher throughput, reduces p99 latency by as much as 88% and\n56%, and converges with fewer trials.",
      "authors": [
        "Qi Lin",
        "Zhenyu Zhang",
        "Viraj Thakkar",
        "Zhenjie Sun",
        "Mai Zheng",
        "Zhichao Cao"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25017v1",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25017v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25013v1",
      "title": "Emergence of Minimal Circuits for Indirect Object Identification in Attention-Only Transformers",
      "abstract": "Mechanistic interpretability aims to reverse-engineer large language models\n(LLMs) into human-understandable computational circuits. However, the\ncomplexity of pretrained models often obscures the minimal mechanisms required\nfor specific reasoning tasks. In this work, we train small, attention-only\ntransformers from scratch on a symbolic version of the Indirect Object\nIdentification (IOI) task -- a benchmark for studying coreference -- like\nreasoning in transformers. Surprisingly, a single-layer model with only two\nattention heads achieves perfect IOI accuracy, despite lacking MLPs and\nnormalization layers. Through residual stream decomposition, spectral analysis,\nand embedding interventions, we find that the two heads specialize into\nadditive and contrastive subcircuits that jointly implement IOI resolution.\nFurthermore, we show that a two-layer, one-head model achieves similar\nperformance by composing information across layers through query-value\ninteractions. These results demonstrate that task-specific training induces\nhighly interpretable, minimal circuits, offering a controlled testbed for\nprobing the computational foundations of transformer reasoning.",
      "authors": [
        "Rabin Adhikari"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25013v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25013v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.24992v1",
      "title": "POWSM: A Phonetic Open Whisper-Style Speech Foundation Model",
      "abstract": "Recent advances in spoken language processing have led to substantial\nprogress in phonetic tasks such as automatic speech recognition (ASR), phone\nrecognition (PR), grapheme-to-phoneme conversion (G2P), and phoneme-to-grapheme\nconversion (P2G). Despite their conceptual similarity, these tasks have largely\nbeen studied in isolation, each relying on task-specific architectures and\ndatasets. In this paper, we introduce POWSM (Phonetic Open Whisper-style Speech\nModel), the first unified framework capable of jointly performing multiple\nphone-related tasks. POWSM enables seamless conversion between audio, text\n(graphemes), and phones, opening up new possibilities for universal and\nlow-resource speech processing. Our model outperforms or matches specialized PR\nmodels of similar size (Wav2Vec2Phoneme and ZIPA) while jointly supporting G2P,\nP2G, and ASR. Our training data, code and models are released to foster open\nscience.",
      "authors": [
        "Chin-Jou Li",
        "Kalvin Chang",
        "Shikhar Bharadwaj",
        "Eunjung Yeo",
        "Kwanghee Choi",
        "Jian Zhu",
        "David Mortensen",
        "Shinji Watanabe"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24992v1",
      "categories": [
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24992v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.24966v1",
      "title": "Sequences of Logits Reveal the Low Rank Structure of Language Models",
      "abstract": "A major problem in the study of large language models is to understand their\ninherent low-dimensional structure. We introduce an approach to study the\nlow-dimensional structure of language models at a model-agnostic level: as\nsequential probabilistic models. We first empirically demonstrate that a wide\nrange of modern language models exhibit low-rank structure: in particular,\nmatrices built from the model's logits for varying sets of prompts and\nresponses have low approximate rank. We then show that this low-rank structure\ncan be leveraged for generation -- in particular, we can generate a response to\na target prompt using a linear combination of the model's outputs on unrelated,\nor even nonsensical prompts.\n  On the theoretical front, we observe that studying the approximate rank of\nlanguage models in the sense discussed above yields a simple universal\nabstraction whose theoretical predictions parallel our experiments. We then\nanalyze the representation power of the abstraction and give provable learning\nguarantees.",
      "authors": [
        "Noah Golowich",
        "Allen Liu",
        "Abhishek Shetty"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24966v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "stat.ML"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24966v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.24963v1",
      "title": "Language Model Behavioral Phases are Consistent Across Architecture, Training Data, and Scale",
      "abstract": "We show that across architecture (Transformer vs. Mamba vs. RWKV), training\ndataset (OpenWebText vs. The Pile), and scale (14 million parameters to 12\nbillion parameters), autoregressive language models exhibit highly consistent\npatterns of change in their behavior over the course of pretraining. Based on\nour analysis of over 1,400 language model checkpoints on over 110,000 tokens of\nEnglish, we find that up to 98% of the variance in language model behavior at\nthe word level can be explained by three simple heuristics: the unigram\nprobability (frequency) of a given word, the $n$-gram probability of the word,\nand the semantic similarity between the word and its context. Furthermore, we\nsee consistent behavioral phases in all language models, with their predicted\nprobabilities for words overfitting to those words' $n$-gram probabilities for\nincreasing $n$ over the course of training. Taken together, these results\nsuggest that learning in neural language models may follow a similar trajectory\nirrespective of model details.",
      "authors": [
        "James A. Michaelov",
        "Roger P. Levy",
        "Benjamin K. Bergen"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24963v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24963v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.24942v1",
      "title": "Finding Culture-Sensitive Neurons in Vision-Language Models",
      "abstract": "Despite their impressive performance, vision-language models (VLMs) still\nstruggle on culturally situated inputs. To understand how VLMs process\nculturally grounded information, we study the presence of culture-sensitive\nneurons, i.e. neurons whose activations show preferential sensitivity to inputs\nassociated with particular cultural contexts. We examine whether such neurons\nare important for culturally diverse visual question answering and where they\nare located. Using the CVQA benchmark, we identify neurons of culture\nselectivity and perform causal tests by deactivating the neurons flagged by\ndifferent identification methods. Experiments on three VLMs across 25 cultural\ngroups demonstrate the existence of neurons whose ablation disproportionately\nharms performance on questions about the corresponding cultures, while having\nminimal effects on others. Moreover, we propose a new margin-based selector -\nContrastive Activation Selection (CAS), and show that it outperforms existing\nprobability- and entropy-based methods in identifying culture-sensitive\nneurons. Finally, our layer-wise analyses reveals that such neurons tend to\ncluster in certain decoder layers. Overall, our findings shed new light on the\ninternal organization of multimodal representations.",
      "authors": [
        "Xiutian Zhao",
        "Rochelle Choenni",
        "Rohit Saxena",
        "Ivan Titov"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24942v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24942v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.24940v1",
      "title": "SemCoT: Accelerating Chain-of-Thought Reasoning through Semantically-Aligned Implicit Tokens",
      "abstract": "The verbosity of Chain-of-Thought (CoT) reasoning hinders its mass deployment\nin efficiency-critical applications. Recently, implicit CoT approaches have\nemerged, which encode reasoning steps within LLM's hidden embeddings (termed\n``implicit reasoning'') rather than explicit tokens. This approach accelerates\nCoT by reducing the reasoning length and bypassing some LLM components.\nHowever, existing implicit CoT methods face two significant challenges: (1)\nthey fail to preserve the semantic alignment between the implicit reasoning\n(when transformed to natural language) and the ground-truth reasoning,\nresulting in a significant CoT performance degradation, and (2) they focus on\nreducing the length of the implicit reasoning; however, they neglect the\nconsiderable time cost for an LLM to generate one individual implicit reasoning\ntoken. To tackle these challenges, we propose a novel semantically-aligned\nimplicit CoT framework termed SemCoT. In particular, for the first challenge,\nwe design a contrastively trained sentence transformer that evaluates semantic\nalignment between implicit and explicit reasoning, which is used to enforce\nsemantic preservation during implicit reasoning optimization. To address the\nsecond challenge, we introduce an efficient implicit reasoning generator by\nfinetuning a lightweight language model using knowledge distillation. This\ngenerator is guided by our sentence transformer to distill ground-truth\nreasoning into semantically aligned implicit reasoning, while also optimizing\nfor accuracy. SemCoT is the first approach that enhances CoT efficiency by\njointly optimizing token-level generation speed and preserving semantic\nalignment with ground-truth reasoning. Extensive experiments demonstrate the\nsuperior performance of SemCoT compared to state-of-the-art methods in both\nefficiency and effectiveness. Our code can be found at\nhttps://github.com/YinhanHe123/SemCoT/.",
      "authors": [
        "Yinhan He",
        "Wendy Zheng",
        "Yaochen Zhu",
        "Zaiyi Zheng",
        "Lin Su",
        "Sriram Vasudevan",
        "Qi Guo",
        "Liangjie Hong",
        "Jundong Li"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24940v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24940v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.24934v1",
      "title": "Disaggregation Reveals Hidden Training Dynamics: The Case of Agreement Attraction",
      "abstract": "Language models generally produce grammatical text, but they are more likely\nto make errors in certain contexts. Drawing on paradigms from\npsycholinguistics, we carry out a fine-grained analysis of those errors in\ndifferent syntactic contexts. We demonstrate that by disaggregating over the\nconditions of carefully constructed datasets and comparing model performance on\neach over the course of training, it is possible to better understand the\nintermediate stages of grammatical learning in language models. Specifically,\nwe identify distinct phases of training where language model behavior aligns\nwith specific heuristics such as word frequency and local context rather than\ngeneralized grammatical rules. We argue that taking this approach to analyzing\nlanguage model behavior more generally can serve as a powerful tool for\nunderstanding the intermediate learning phases, overall training dynamics, and\nthe specific generalizations learned by language models.",
      "authors": [
        "James A. Michaelov",
        "Catherine Arnett"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24934v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24934v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.24932v1",
      "title": "RiddleBench: A New Generative Reasoning Benchmark for LLMs",
      "abstract": "Large Language Models have demonstrated strong performance on many\nestablished reasoning benchmarks. However, these benchmarks primarily evaluate\nstructured skills like quantitative problem-solving, leaving a gap in assessing\nflexible, multifaceted reasoning abilities that are central to human\nintelligence. These abilities require integrating logical deduction with\nspatial awareness and constraint satisfaction, which current evaluations do not\nmeasure well. To address this, we introduce RiddleBench, a benchmark of 1,737\nchallenging puzzles in English designed to probe these core reasoning\ncapabilities. Evaluation of state-of-the-art models on RiddleBench shows\nfundamental weaknesses. Even top proprietary models like Gemini 2.5 Pro, o3,\nand Claude 4 Sonnet achieve accuracy just above 60% (60.30%, 63.37%, and\n63.16%). Analysis further reveals deep failures, including hallucination\ncascades (accepting flawed reasoning from other models) and poor\nself-correction due to a strong self-confirmation bias. Their reasoning is also\nfragile, with performance degrading significantly when constraints are\nreordered or irrelevant information is introduced. RiddleBench functions as a\ndiagnostic tool for these issues and as a resource for guiding the development\nof more robust and reliable language models.",
      "authors": [
        "Deepon Halder",
        "Alan Saji",
        "Thanmay Jayakumar",
        "Ratish Puduppully",
        "Anoop Kunchukuttan",
        "Raj Dabre"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24932v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24932v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.24891v1",
      "title": "Idea2Plan: Exploring AI-Powered Research Planning",
      "abstract": "Large language models (LLMs) have demonstrated significant potential to\naccelerate scientific discovery as valuable tools for analyzing data,\ngenerating hypotheses, and supporting innovative approaches in various\nscientific fields. In this work, we investigate how LLMs can handle the\ntransition from conceptual research ideas to well-structured research plans.\nEffective research planning not only supports scientists in advancing their\nresearch but also represents a crucial capability for the development of\nautonomous research agents. Despite its importance, the field lacks a\nsystematic understanding of LLMs' research planning capability. To rigorously\nmeasure this capability, we introduce the Idea2Plan task and Idea2Plan Bench, a\nbenchmark built from 200 ICML 2025 Spotlight and Oral papers released after\nmajor LLM training cutoffs. Each benchmark instance includes a research idea\nand a grading rubric capturing the key components of valid plans. We further\npropose Idea2Plan JudgeEval, a complementary benchmark to assess the\nreliability of LLM-based judges against expert annotations. Experimental\nresults show that GPT-5 and GPT-5-mini achieve the strongest performance on the\nbenchmark, though substantial headroom remains for future improvement. Our\nstudy provides new insights into LLMs' capability for research planning and lay\nthe groundwork for future progress.",
      "authors": [
        "Jin Huang",
        "Silviu Cucerzan",
        "Sujay Kumar Jauhar",
        "Ryen W. White"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24891v1",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24891v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.24870v1",
      "title": "Seeing Through the MiRAGE: Evaluating Multimodal Retrieval Augmented Generation",
      "abstract": "We introduce MiRAGE, an evaluation framework for retrieval-augmented\ngeneration (RAG) from multimodal sources. As audiovisual media becomes a\nprevalent source of information online, it is essential for RAG systems to\nintegrate information from these sources into generation. However, existing\nevaluations for RAG are text-centric, limiting their applicability to\nmultimodal, reasoning intensive settings because they don't verify information\nagainst sources. MiRAGE is a claim-centric approach to multimodal RAG\nevaluation, consisting of InfoF1, evaluating factuality and information\ncoverage, and CiteF1, measuring citation support and completeness. We show that\nMiRAGE, when applied by humans, strongly aligns with extrinsic quality\njudgments. We additionally introduce automatic variants of MiRAGE and three\nprominent TextRAG metrics -- ACLE, ARGUE, and RAGAS -- demonstrating the\nlimitations of text-centric work and laying the groundwork for automatic\nevaluation. We release open-source implementations and outline how to assess\nmultimodal RAG.",
      "authors": [
        "Alexander Martin",
        "William Walden",
        "Reno Kriz",
        "Dengjia Zhang",
        "Kate Sanders",
        "Eugene Yang",
        "Chihsheng Jin",
        "Benjamin Van Durme"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24870v1",
      "categories": [
        "cs.CL",
        "cs.CV",
        "cs.IR"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24870v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.24856v1",
      "title": "Do Large Language Models Grasp The Grammar? Evidence from Grammar-Book-Guided Probing in Luxembourgish",
      "abstract": "Grammar refers to the system of rules that governs the structural\norganization and the semantic relations among linguistic units such as\nsentences, phrases, and words within a given language. In natural language\nprocessing, there remains a notable scarcity of grammar focused evaluation\nprotocols, a gap that is even more pronounced for low-resource languages.\nMoreover, the extent to which large language models genuinely comprehend\ngrammatical structure, especially the mapping between syntactic structures and\nmeanings, remains under debate. To investigate this issue, we propose a Grammar\nBook Guided evaluation pipeline intended to provide a systematic and\ngeneralizable framework for grammar evaluation consisting of four key stages,\nand in this work we take Luxembourgish as a case study. The results show a weak\npositive correlation between translation performance and grammatical\nunderstanding, indicating that strong translations do not necessarily imply\ndeep grammatical competence. Larger models perform well overall due to their\nsemantic strength but remain weak in morphology and syntax, struggling\nparticularly with Minimal Pair tasks, while strong reasoning ability offers a\npromising way to enhance their grammatical understanding.",
      "authors": [
        "Lujun Li",
        "Yewei Song",
        "Lama Sleem",
        "Yiqun Wang",
        "Yangjie Xu",
        "Cedric Lothritz",
        "Niccolo Gentile",
        "Radu State",
        "Tegawende F. Bissyande",
        "Jacques Klein"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24856v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24856v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.24707v1",
      "title": "MetricX-25 and GemSpanEval: Google Translate Submissions to the WMT25 Evaluation Shared Task",
      "abstract": "In this paper, we present our submissions to the unified WMT25 Translation\nEvaluation Shared Task. For the Quality Score Prediction subtask, we create a\nnew generation of MetricX with improvements in the input format and the\ntraining protocol, while for the Error Span Detection subtask we develop a new\nmodel, GemSpanEval, trained to predict error spans along with their severities\nand categories. Both systems are based on the state-of-the-art multilingual\nopen-weights model Gemma 3, fine-tuned on publicly available WMT data. We\ndemonstrate that MetricX-25, adapting Gemma 3 to an encoder-only architecture\nwith a regression head on top, can be trained to effectively predict both MQM\nand ESA quality scores, and significantly outperforms its predecessor. Our\ndecoder-only GemSpanEval model, on the other hand, we show to be competitive in\nerror span detection with xCOMET, a strong encoder-only sequence-tagging\nbaseline. With error span detection formulated as a generative task, we\ninstruct the model to also output the context for each predicted error span,\nthus ensuring that error spans are identified unambiguously.",
      "authors": [
        "Juraj Juraska",
        "Tobias Domhan",
        "Mara Finkelstein",
        "Tetsuji Nakagawa",
        "Geza Kovacs",
        "Daniel Deutsch",
        "Pidong Wang",
        "Markus Freitag"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24707v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24707v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.24706v1",
      "title": "ComboBench: Can LLMs Manipulate Physical Devices to Play Virtual Reality Games?",
      "abstract": "Virtual Reality (VR) games require players to translate high-level semantic\nactions into precise device manipulations using controllers and head-mounted\ndisplays (HMDs). While humans intuitively perform this translation based on\ncommon sense and embodied understanding, whether Large Language Models (LLMs)\ncan effectively replicate this ability remains underexplored. This paper\nintroduces a benchmark, ComboBench, evaluating LLMs' capability to translate\nsemantic actions into VR device manipulation sequences across 262 scenarios\nfrom four popular VR games: Half-Life: Alyx, Into the Radius, Moss: Book II,\nand Vivecraft. We evaluate seven LLMs, including GPT-3.5, GPT-4, GPT-4o,\nGemini-1.5-Pro, LLaMA-3-8B, Mixtral-8x7B, and GLM-4-Flash, compared against\nannotated ground truth and human performance. Our results reveal that while\ntop-performing models like Gemini-1.5-Pro demonstrate strong task decomposition\ncapabilities, they still struggle with procedural reasoning and spatial\nunderstanding compared to humans. Performance varies significantly across\ngames, suggesting sensitivity to interaction complexity. Few-shot examples\nsubstantially improve performance, indicating potential for targeted\nenhancement of LLMs' VR manipulation capabilities. We release all materials at\nhttps://sites.google.com/view/combobench.",
      "authors": [
        "Shuqing Li",
        "Jiayi Yan",
        "Chenyu Niu",
        "Jen-tse Huang",
        "Yun Peng",
        "Wenxuan Wang",
        "Yepang Liu",
        "Michael R. Lyu"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24706v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC",
        "cs.SE"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24706v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.24702v1",
      "title": "Agent Data Protocol: Unifying Datasets for Diverse, Effective Fine-tuning of LLM Agents",
      "abstract": "Public research results on large-scale supervised finetuning of AI agents\nremain relatively rare, since the collection of agent training data presents\nunique challenges. In this work, we argue that the bottleneck is not a lack of\nunderlying data sources, but that a large variety of data is fragmented across\nheterogeneous formats, tools, and interfaces. To this end, we introduce the\nagent data protocol (ADP), a light-weight representation language that serves\nas an \"interlingua\" between agent datasets in diverse formats and unified agent\ntraining pipelines downstream. The design of ADP is expressive enough to\ncapture a large variety of tasks, including API/tool use, browsing, coding,\nsoftware engineering, and general agentic workflows, while remaining simple to\nparse and train on without engineering at a per-dataset level. In experiments,\nwe unified a broad collection of 13 existing agent training datasets into ADP\nformat, and converted the standardized ADP data into training-ready formats for\nmultiple agent frameworks. We performed SFT on these data, and demonstrated an\naverage performance gain of ~20% over corresponding base models, and delivers\nstate-of-the-art or near-SOTA performance on standard coding, browsing, tool\nuse, and research benchmarks, without domain-specific tuning. All code and data\nare released publicly, in the hope that ADP could help lower the barrier to\nstandardized, scalable, and reproducible agent training.",
      "authors": [
        "Yueqi Song",
        "Ketan Ramaneti",
        "Zaid Sheikh",
        "Ziru Chen",
        "Boyu Gou",
        "Tianbao Xie",
        "Yiheng Xu",
        "Danyang Zhang",
        "Apurva Gandhi",
        "Fan Yang",
        "Joseph Liu",
        "Tianyue Ou",
        "Zhihao Yuan",
        "Frank Xu",
        "Shuyan Zhou",
        "Xingyao Wang",
        "Xiang Yue",
        "Tao Yu",
        "Huan Sun",
        "Yu Su",
        "Graham Neubig"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24702v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24702v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.24701v1",
      "title": "Tongyi DeepResearch Technical Report",
      "abstract": "We present Tongyi DeepResearch, an agentic large language model, which is\nspecifically designed for long-horizon, deep information-seeking research\ntasks. To incentivize autonomous deep research agency, Tongyi DeepResearch is\ndeveloped through an end-to-end training framework that combines agentic\nmid-training and agentic post-training, enabling scalable reasoning and\ninformation seeking across complex tasks. We design a highly scalable data\nsynthesis pipeline that is fully automatic, without relying on costly human\nannotation, and empowers all training stages. By constructing customized\nenvironments for each stage, our system enables stable and consistent\ninteractions throughout. Tongyi DeepResearch, featuring 30.5 billion total\nparameters, with only 3.3 billion activated per token, achieves\nstate-of-the-art performance across a range of agentic deep research\nbenchmarks, including Humanity's Last Exam, BrowseComp, BrowseComp-ZH,\nWebWalkerQA, xbench-DeepSearch, FRAMES and xbench-DeepSearch-2510. We\nopen-source the model, framework, and complete solutions to empower the\ncommunity.",
      "authors": [
        "Tongyi DeepResearch Team",
        "Baixuan Li",
        "Bo Zhang",
        "Dingchu Zhang",
        "Fei Huang",
        "Guangyu Li",
        "Guoxin Chen",
        "Huifeng Yin",
        "Jialong Wu",
        "Jingren Zhou",
        "Kuan Li",
        "Liangcai Su",
        "Litu Ou",
        "Liwen Zhang",
        "Pengjun Xie",
        "Rui Ye",
        "Wenbiao Yin",
        "Xinmiao Yu",
        "Xinyu Wang",
        "Xixi Wu",
        "Xuanzhong Chen",
        "Yida Zhao",
        "Zhen Zhang",
        "Zhengwei Tao",
        "Zhongwang Zhang",
        "Zile Qiao",
        "Chenxi Wang",
        "Donglei Yu",
        "Gang Fu",
        "Haiyang Shen",
        "Jiayin Yang",
        "Jun Lin",
        "Junkai Zhang",
        "Kui Zeng",
        "Li Yang",
        "Hailong Yin",
        "Maojia Song",
        "Ming Yan",
        "Peng Xia",
        "Qian Xiao",
        "Rui Min",
        "Ruixue Ding",
        "Runnan Fang",
        "Shaowei Chen",
        "Shen Huang",
        "Shihang Wang",
        "Shihao Cai",
        "Weizhou Shen",
        "Xiaobin Wang",
        "Xin Guan",
        "Xinyu Geng",
        "Yingcheng Shi",
        "Yuning Wu",
        "Zhuo Chen",
        "Zijian Li",
        "Yong Jiang"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24701v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG",
        "cs.MA"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24701v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.24698v1",
      "title": "ParallelMuse: Agentic Parallel Thinking for Deep Information Seeking",
      "abstract": "Parallel thinking expands exploration breadth, complementing the deep\nexploration of information-seeking (IS) agents to further enhance\nproblem-solving capability. However, conventional parallel thinking faces two\nkey challenges in this setting: inefficiency from repeatedly rolling out from\nscratch, and difficulty in integrating long-horizon reasoning trajectories\nduring answer generation, as limited context capacity prevents full\nconsideration of the reasoning process. To address these issues, we propose\nParallelMuse, a two-stage paradigm designed for deep IS agents. The first\nstage, Functionality-Specified Partial Rollout, partitions generated sequences\ninto functional regions and performs uncertainty-guided path reuse and\nbranching to enhance exploration efficiency. The second stage, Compressed\nReasoning Aggregation, exploits reasoning redundancy to losslessly compress\ninformation relevant to answer derivation and synthesize a coherent final\nanswer. Experiments across multiple open-source agents and benchmarks\ndemonstrate up to 62% performance improvement with a 10--30% reduction in\nexploratory token consumption.",
      "authors": [
        "Baixuan Li",
        "Dingchu Zhang",
        "Jialong Wu",
        "Wenbiao Yin",
        "Zhengwei Tao",
        "Yida Zhao",
        "Liwen Zhang",
        "Haiyang Shen",
        "Runnan Fang",
        "Pengjun Xie",
        "Jingren Zhou",
        "Yong Jiang"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24698v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24698v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.24699v1",
      "title": "AgentFold: Long-Horizon Web Agents with Proactive Context Management",
      "abstract": "LLM-based web agents show immense promise for information seeking, yet their\neffectiveness on long-horizon tasks is hindered by a fundamental trade-off in\ncontext management. Prevailing ReAct-based agents suffer from context\nsaturation as they accumulate noisy, raw histories, while methods that fixedly\nsummarize the full history at each step risk the irreversible loss of critical\ndetails. Addressing these, we introduce AgentFold, a novel agent paradigm\ncentered on proactive context management, inspired by the human cognitive\nprocess of retrospective consolidation. AgentFold treats its context as a\ndynamic cognitive workspace to be actively sculpted, rather than a passive log\nto be filled. At each step, it learns to execute a `folding' operation, which\nmanages its historical trajectory at multiple scales: it can perform granular\ncondensations to preserve vital, fine-grained details, or deep consolidations\nto abstract away entire multi-step sub-tasks. The results on prominent\nbenchmarks are striking: with simple supervised fine-tuning (without continual\npre-training or RL), our AgentFold-30B-A3B agent achieves 36.2% on BrowseComp\nand 47.3% on BrowseComp-ZH. Notably, this performance not only surpasses or\nmatches open-source models of a dramatically larger scale, such as the\nDeepSeek-V3.1-671B-A37B, but also surpasses leading proprietary agents like\nOpenAI's o4-mini.",
      "authors": [
        "Rui Ye",
        "Zhongwang Zhang",
        "Kuan Li",
        "Huifeng Yin",
        "Zhengwei Tao",
        "Yida Zhao",
        "Liangcai Su",
        "Liwen Zhang",
        "Zile Qiao",
        "Xinyu Wang",
        "Pengjun Xie",
        "Fei Huang",
        "Siheng Chen",
        "Jingren Zhou",
        "Yong Jiang"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24699v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24699v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.24697v1",
      "title": "WebLeaper: Empowering Efficiency and Efficacy in WebAgent via Enabling Info-Rich Seeking",
      "abstract": "Large Language Model (LLM)-based agents have emerged as a transformative\napproach for open-ended problem solving, with information seeking (IS) being a\ncore capability that enables autonomous reasoning and decision-making. While\nprior research has largely focused on improving retrieval depth, we observe\nthat current IS agents often suffer from low search efficiency, which in turn\nconstrains overall performance. A key factor underlying this inefficiency is\nthe sparsity of target entities in training tasks, which limits opportunities\nfor agents to learn and generalize efficient search behaviors. To address these\nchallenges, we propose WebLeaper, a framework for constructing high-coverage IS\ntasks and generating efficient solution trajectories. We formulate IS as a\ntree-structured reasoning problem, enabling a substantially larger set of\ntarget entities to be embedded within a constrained context. Leveraging curated\nWikipedia tables, we propose three variants for synthesizing IS tasks, Basic,\nUnion, and Reverse-Union, to systematically increase both IS efficiency and\nefficacy. Finally, we curate training trajectories by retaining only those that\nare simultaneously accurate and efficient, ensuring that the model is optimized\nfor both correctness and search performance. Extensive experiments on both\nbasic and comprehensive settings, conducted on five IS benchmarks, BrowserComp,\nGAIA, xbench-DeepSearch, WideSearch, and Seal-0, demonstrate that our method\nconsistently achieves improvements in both effectiveness and efficiency over\nstrong baselines.",
      "authors": [
        "Zhengwei Tao",
        "Haiyang Shen",
        "Baixuan Li",
        "Wenbiao Yin",
        "Jialong Wu",
        "Kuan Li",
        "Zhongwang Zhang",
        "Huifeng Yin",
        "Rui Ye",
        "Liwen Zhang",
        "Xinyu Wang",
        "Pengjun Xie",
        "Jingren Zhou",
        "Yong Jiang"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24697v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24697v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.24695v1",
      "title": "AgentFrontier: Expanding the Capability Frontier of LLM Agents with ZPD-Guided Data Synthesis",
      "abstract": "Training large language model agents on tasks at the frontier of their\ncapabilities is key to unlocking advanced reasoning. We introduce a data\nsynthesis approach inspired by the educational theory of the Zone of Proximal\nDevelopment (ZPD), which defines this frontier as tasks an LLM cannot solve\nalone but can master with guidance. To operationalize this, we present the\nAgentFrontier Engine, an automated pipeline that synthesizes high-quality,\nmultidisciplinary data situated precisely within the LLM's ZPD. This engine\nsupports both continued pre-training with knowledge-intensive data and targeted\npost-training on complex reasoning tasks. From the same framework, we derive\nthe ZPD Exam, a dynamic and automated benchmark designed to evaluate agent\ncapabilities on these frontier tasks. We train AgentFrontier-30B-A3B model on\nour synthesized data, which achieves state-of-the-art results on demanding\nbenchmarks like Humanity's Last Exam, even surpassing some leading proprietary\nagents. Our work demonstrates that a ZPD-guided approach to data synthesis\noffers a scalable and effective path toward building more capable LLM agents.",
      "authors": [
        "Xuanzhong Chen",
        "Zile Qiao",
        "Guoxin Chen",
        "Liangcai Su",
        "Zhen Zhang",
        "Xinyu Wang",
        "Pengjun Xie",
        "Fei Huang",
        "Jingren Zhou",
        "Yong Jiang"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24695v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24695v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.24694v1",
      "title": "Repurposing Synthetic Data for Fine-grained Search Agent Supervision",
      "abstract": "LLM-based search agents are increasingly trained on entity-centric synthetic\ndata to solve complex, knowledge-intensive tasks. However, prevailing training\nmethods like Group Relative Policy Optimization (GRPO) discard this rich entity\ninformation, relying instead on sparse, outcome-based rewards. This critical\nlimitation renders them unable to distinguish informative \"near-miss\"\nsamples-those with substantially correct reasoning but a flawed final\nanswer-from complete failures, thus discarding valuable learning signals. We\naddress this by leveraging the very entities discarded during training. Our\nempirical analysis reveals a strong positive correlation between the number of\nground-truth entities identified during an agent's reasoning process and final\nanswer accuracy. Building on this insight, we introduce Entity-aware Group\nRelative Policy Optimization (E-GRPO), a novel framework that formulates a\ndense entity-aware reward function. E-GRPO assigns partial rewards to incorrect\nsamples proportional to their entity match rate, enabling the model to\neffectively learn from these \"near-misses\". Experiments on diverse\nquestion-answering (QA) and deep research benchmarks show that E-GRPO\nconsistently and significantly outperforms the GRPO baseline. Furthermore, our\nanalysis reveals that E-GRPO not only achieves superior accuracy but also\ninduces more efficient reasoning policies that require fewer tool calls,\ndemonstrating a more effective and sample-efficient approach to aligning search\nagents.",
      "authors": [
        "Yida Zhao",
        "Kuan Li",
        "Xixi Wu",
        "Liwen Zhang",
        "Dingchu Zhang",
        "Baixuan Li",
        "Maojia Song",
        "Zhuo Chen",
        "Chenxi Wang",
        "Xinyu Wang",
        "Kewei Tu",
        "Pengjun Xie",
        "Jingren Zhou",
        "Yong Jiang"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24694v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24694v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.24693v1",
      "title": "STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence",
      "abstract": "Despite rapid progress in Multi-modal Large Language Models and Large\nAudio-Language Models, existing audio benchmarks largely test semantics that\ncan be recovered from text captions, masking deficits in fine-grained\nperceptual reasoning. We formalize audio 4D intelligence that is defined as\nreasoning over sound dynamics in time and 3D space, and introduce STAR-Bench to\nmeasure it. STAR-Bench combines a Foundational Acoustic Perception setting (six\nattributes under absolute and relative regimes) with a Holistic Spatio-Temporal\nReasoning setting that includes segment reordering for continuous and discrete\nprocesses and spatial tasks spanning static localization, multi-source\nrelations, and dynamic trajectories. Our data curation pipeline uses two\nmethods to ensure high-quality samples. For foundational tasks, we use\nprocedurally synthesized and physics-simulated audio. For holistic data, we\nfollow a four-stage process that includes human annotation and final selection\nbased on human performance. Unlike prior benchmarks where caption-only\nanswering reduces accuracy slightly, STAR-Bench induces far larger drops\n(-31.5\\% temporal, -35.2\\% spatial), evidencing its focus on linguistically\nhard-to-describe cues. Evaluating 19 models reveals substantial gaps compared\nwith humans and a capability hierarchy: closed-source models are bottlenecked\nby fine-grained perception, while open-source models lag across perception,\nknowledge, and reasoning. Our STAR-Bench provides critical insights and a clear\npath forward for developing future models with a more robust understanding of\nthe physical world.",
      "authors": [
        "Zihan Liu",
        "Zhikang Niu",
        "Qiuyang Xiao",
        "Zhisheng Zheng",
        "Ruoqi Yuan",
        "Yuhang Zang",
        "Yuhang Cao",
        "Xiaoyi Dong",
        "Jianze Liang",
        "Xie Chen",
        "Leilei Sun",
        "Dahua Lin",
        "Jiaqi Wang"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24693v1",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24693v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.24684v1",
      "title": "SPICE: Self-Play In Corpus Environments Improves Reasoning",
      "abstract": "Self-improving systems require environmental interaction for continuous\nadaptation. We introduce SPICE (Self-Play In Corpus Environments), a\nreinforcement learning framework where a single model acts in two roles: a\nChallenger that mines documents from a large corpus to generate diverse\nreasoning tasks, and a Reasoner that solves them. Through adversarial dynamics,\nthe Challenger creates an automatic curriculum at the frontier of the\nReasoner's capability, while corpus grounding provides the rich,\nnear-inexhaustible external signal necessary for sustained improvement. Unlike\nexisting ungrounded self-play methods that offer more limited benefits, SPICE\nachieves consistent gains across mathematical (+8.9%) and general reasoning\n(+9.8%) benchmarks on multiple model families. Our analysis reveals how\ndocument grounding is a key ingredient in SPICE to continuously generate its\nown increasingly challenging goals and achieve them, enabling sustained\nself-improvement.",
      "authors": [
        "Bo Liu",
        "Chuanyang Jin",
        "Seungone Kim",
        "Weizhe Yuan",
        "Wenting Zhao",
        "Ilia Kulikov",
        "Xian Li",
        "Sainbayar Sukhbaatar",
        "Jack Lanchantin",
        "Jason Weston"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24684v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24684v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.24677v1",
      "title": "Dissecting Role Cognition in Medical LLMs via Neuronal Ablation",
      "abstract": "Large language models (LLMs) have gained significant traction in medical\ndecision support systems, particularly in the\n  context of medical question answering and role-playing simulations. A common\npractice, Prompt-Based Role Playing (PBRP),\n  instructs models to adopt different clinical roles (e.g., medical students,\nresidents, attending physicians) to simulate varied\n  professional behaviors. However, the impact of such role prompts on model\nreasoning capabilities remains unclear. This\n  study introduces the RP-Neuron-Activated Evaluation Framework(RPNA) to\nevaluate whether role prompts induce distinct,\n  role-specific cognitive processes in LLMs or merely modify linguistic style.\nWe test this framework on three medical QA\n  datasets, employing neuron ablation and representation analysis techniques to\nassess changes in reasoning pathways. Our\n  results demonstrate that role prompts do not significantly enhance the\nmedical reasoning abilities of LLMs. Instead, they\n  primarily affect surface-level linguistic features, with no evidence of\ndistinct reasoning pathways or cognitive differentiation\n  across clinical roles. Despite superficial stylistic changes, the core\ndecision-making mechanisms of LLMs remain uniform\n  across roles, indicating that current PBRP methods fail to replicate the\ncognitive complexity found in real-world medical\n  practice. This highlights the limitations of role-playing in medical AI and\nemphasizes the need for models that simulate genuine\n  cognitive processes rather than linguistic imitation.We have released the\nrelated code in the following repository:https:\n  //github.com/IAAR-Shanghai/RolePlay_LLMDoctor",
      "authors": [
        "Xun Liang",
        "Huayi Lai",
        "Hanyu Wang",
        "Wentao Zhang",
        "Linfeng Zhang",
        "Yanfang Chen",
        "Feiyu Xiong",
        "Zhiyu Li"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24677v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24677v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.24668v1",
      "title": "InteractComp: Evaluating Search Agents With Ambiguous Queries",
      "abstract": "Language agents have demonstrated remarkable potential in web search and\ninformation retrieval. However, these search agents assume user queries are\ncomplete and unambiguous, an assumption that diverges from reality where users\nbegin with incomplete queries requiring clarification through interaction. Yet\nmost agents lack interactive mechanisms during the search process, and existing\nbenchmarks cannot assess this capability. To address this gap, we introduce\nInteractComp, a benchmark designed to evaluate whether search agents can\nrecognize query ambiguity and actively interact to resolve it during search.\nFollowing the principle of easy to verify, interact to disambiguate, we\nconstruct 210 expert-curated questions across 9 domains through a\ntarget-distractor methodology that creates genuine ambiguity resolvable only\nthrough interaction. Evaluation of 17 models reveals striking failure: the best\nmodel achieves only 13.73% accuracy despite 71.50% with complete context,\nexposing systematic overconfidence rather than reasoning deficits. Forced\ninteraction produces dramatic gains, demonstrating latent capability current\nstrategies fail to engage. Longitudinal analysis shows interaction capabilities\nstagnated over 15 months while search performance improved seven-fold,\nrevealing a critical blind spot. This stagnation, coupled with the immediate\nfeedback inherent to search tasks, makes InteractComp a valuable resource for\nboth evaluating and training interaction capabilities in search agents. The\ncode is available at https://github.com/FoundationAgents/InteractComp.",
      "authors": [
        "Mingyi Deng",
        "Lijun Huang",
        "Yani Fan",
        "Jiayi Zhang",
        "Fashen Ren",
        "Jinyi Bai",
        "Fuzhen Yang",
        "Dayi Miao",
        "Zhaoyang Yu",
        "Yifan Wu",
        "Yanfei Zhang",
        "Fengwei Teng",
        "Yingjia Wan",
        "Song Hu",
        "Yude Li",
        "Xin Jin",
        "Conghao Hu",
        "Haoyu Li",
        "Qirui Fu",
        "Tai Zhong",
        "Xinyu Wang",
        "Xiangru Tang",
        "Nan Tang",
        "Chenglin Wu",
        "Yuyu Luo"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24668v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24668v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.24664v1",
      "title": "MQM Re-Annotation: A Technique for Collaborative Evaluation of Machine Translation",
      "abstract": "Human evaluation of machine translation is in an arms race with translation\nmodel quality: as our models get better, our evaluation methods need to be\nimproved to ensure that quality gains are not lost in evaluation noise. To this\nend, we experiment with a two-stage version of the current state-of-the-art\ntranslation evaluation paradigm (MQM), which we call MQM re-annotation. In this\nsetup, an MQM annotator reviews and edits a set of pre-existing MQM\nannotations, that may have come from themselves, another human annotator, or an\nautomatic MQM annotation system. We demonstrate that rater behavior in\nre-annotation aligns with our goals, and that re-annotation results in\nhigher-quality annotations, mostly due to finding errors that were missed\nduring the first pass.",
      "authors": [
        "Parker Riley",
        "Daniel Deutsch",
        "Mara Finkelstein",
        "Colten DiIanni",
        "Juraj Juraska",
        "Markus Freitag"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24664v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24664v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.24654v1",
      "title": "Evolving Diagnostic Agents in a Virtual Clinical Environment",
      "abstract": "In this paper, we present a framework for training large language models\n(LLMs) as diagnostic agents with reinforcement learning, enabling them to\nmanage multi-turn diagnostic processes, adaptively select examinations, and\ncommit to final diagnoses. Unlike instruction-tuned models trained on static\ncase summaries, our method acquires diagnostic strategies through interactive\nexploration and outcome-based feedback. Our contributions are fourfold: (i) We\npresent DiagGym, a diagnostics world model trained with electronic health\nrecords that emits examination outcomes conditioned on patient history and\nrecommended examination, serving as a virtual clinical environment for\nrealistic diagnosis training and evaluation; (ii) We train DiagAgent via\nend-to-end, multi-turn reinforcement learning to learn diagnostic policies that\noptimize both information yield and diagnostic accuracy; (iii) We introduce\nDiagBench, a diagnostic benchmark comprising 750 cases with physician-validated\nexamination recommendations and 99 cases annotated with 973 physician-written\nrubrics on diagnosis process; (iv) we demonstrate superior performance across\ndiverse diagnostic settings. DiagAgent significantly outperforms 10\nstate-of-the-art LLMs, including DeepSeek-v3 and GPT-4o, as well as two\nprompt-engineered agents. In single-turn settings, DiagAgent achieves 9.34%\nhigher diagnostic accuracy and 44.03% improvement in examination recommendation\nhit ratio. In end-to-end settings, it delivers 15.12% increase in diagnostic\naccuracy and 23.09% boost in examination recommendation F1 score. In\nrubric-based evaluation, it surpasses the next-best model, Claude-sonnet-4, by\n7.1% in weighted rubric score. These findings indicate that learning policies\nin interactive clinical environments confers dynamic and clinically meaningful\ndiagnostic management abilities unattainable through passive training alone.",
      "authors": [
        "Pengcheng Qiu",
        "Chaoyi Wu",
        "Junwei Liu",
        "Qiaoyu Zheng",
        "Yusheng Liao",
        "Haowen Wang",
        "Yun Yue",
        "Qianrui Fan",
        "Shuai Zhen",
        "Jian Wang",
        "Jinjie Gu",
        "Yanfeng Wang",
        "Ya Zhang",
        "Weidi Xie"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24654v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24654v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.24652v1",
      "title": "Optimizing Retrieval for RAG via Reinforced Contrastive Learning",
      "abstract": "As retrieval-augmented generation (RAG) becomes increasingly widespread, the\nrole of information retrieval (IR) is shifting from retrieving information for\nhuman users to retrieving contextual knowledge for artificial intelligence (AI)\nsystems, where relevance becomes difficult to define or annotate beforehand. To\naddress this challenge, we propose R3, a Retrieval framework optimized for RAG\nthrough trialand-feedback Reinforced contrastive learning. Unlike prior\napproaches that rely on annotated or synthetic data for supervised fine-tuning,\nR3 enables the retriever to dynamically explore and optimize relevance within\nthe RAG environment. During training, the retrieved results interact with the\nenvironment to produce contrastive signals that automatically guide the\nretriever's self-improvement. Extensive experiments across diverse tasks\ndemonstrate that R3 improves RAG performance by 5.2% over the original\nretriever and surpasses state-of-the-art retrievers by 4.9%, while achieving\ncomparable results to LLM-augmented retrieval and RAG systems built on\npost-trained or instruction-tuned LLMs. It is both efficient and practical,\nrequiring only 4 GPUs and completing training within a single day.",
      "authors": [
        "Jiawei Zhou",
        "Lei Chen"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24652v1",
      "categories": [
        "cs.CL",
        "cs.IR"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24652v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.24647v1",
      "title": "Quantifying the Effects of Word Length, Frequency, and Predictability on Dyslexia",
      "abstract": "We ask where, and under what conditions, dyslexic reading costs arise in a\nlarge-scale naturalistic reading dataset. Using eye-tracking aligned to\nword-level features (word length, frequency, and predictability), we model how\neach feature influences dyslexic time costs. We find that all three features\nrobustly change reading times in both typical and dyslexic readers, and that\ndyslexic readers show stronger sensitivities to each, especially\npredictability. Counterfactual manipulations of these features substantially\nnarrow the dyslexic-control gap by about one third, with predictability showing\nthe strongest effect, followed by length and frequency. These patterns align\nwith dyslexia theories that posit heightened demands on linguistic working\nmemory and phonological encoding, and they motivate further work on lexical\ncomplexity and parafoveal preview benefits to explain the remaining gap. In\nshort, we quantify when extra dyslexic costs arise, how large they are, and\noffer actionable guidance for interventions and computational models for\ndyslexics.",
      "authors": [
        "Hugo Rydel-Johnston",
        "Alex Kafkas"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24647v1",
      "categories": [
        "cs.CL",
        "q-bio.NC"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24647v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.24636v2",
      "title": "OpenReward: Learning to Reward Long-form Agentic Tasks via Reinforcement Learning",
      "abstract": "Reward models (RMs) have become essential for aligning large language models\n(LLMs), serving as scalable proxies for human evaluation in both training and\ninference. However, existing RMs struggle on knowledge-intensive and long-form\ntasks, where evaluating correctness requires grounding beyond the model's\ninternal knowledge. This limitation hinders them from reliably discriminating\nsubtle quality differences, especially when external evidence is necessary. To\naddress this, we introduce OpenRM, a tool-augmented long-form reward model that\nsystematically judges open-ended responses by invoking external tools to gather\nrelevant evidence. We train OpenRM with Group Relative Policy Optimization\n(GRPO) on over 27K synthesized pairwise examples generated through a\ncontrollable data synthesis framework. The training objective jointly\nsupervises intermediate tool usage and final outcome accuracy, incentivizing\nour reward model to learn effective evidence-based judgment strategies.\nExtensive experiments on three newly-collected datasets and two widely-used\nbenchmarks demonstrate that OpenRM substantially outperforms existing reward\nmodeling approaches. As a further step, we integrate OpenRM into both\ninference-time response selection and training-time data selection. This yields\nconsistent gains in downstream LLM alignment tasks, highlighting the potential\nof tool-augmented reward models for scaling reliable long-form evaluation.",
      "authors": [
        "Ziyou Hu",
        "Zhengliang Shi",
        "Minghang Zhu",
        "Haitao Li",
        "Teng Sun",
        "Pengjie Ren",
        "Suzan Verberne",
        "Zhaochun Ren"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24636v2",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24636v2"
    },
    {
      "url": "https://arxiv.org/abs/2510.24628v1",
      "title": "\"Mm, Wat?\" Detecting Other-initiated Repair Requests in Dialogue",
      "abstract": "Maintaining mutual understanding is a key component in human-human\nconversation to avoid conversation breakdowns, in which repair, particularly\nOther-Initiated Repair (OIR, when one speaker signals trouble and prompts the\nother to resolve), plays a vital role. However, Conversational Agents (CAs)\nstill fail to recognize user repair initiation, leading to breakdowns or\ndisengagement. This work proposes a multimodal model to automatically detect\nrepair initiation in Dutch dialogues by integrating linguistic and prosodic\nfeatures grounded in Conversation Analysis. The results show that prosodic cues\ncomplement linguistic features and significantly improve the results of\npretrained text and audio embeddings, offering insights into how different\nfeatures interact. Future directions include incorporating visual cues,\nexploring multilingual and cross-context corpora to assess the robustness and\ngeneralizability.",
      "authors": [
        "Anh Ngo",
        "Nicolas Rollet",
        "Catherine Pelachaud",
        "Chloe Clavel"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24628v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24628v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.24626v1",
      "title": "Relative Scaling Laws for LLMs",
      "abstract": "Scaling laws describe how language models improve with additional data,\nparameters, and compute. While widely used, they are typically measured on\naggregate test sets. Aggregate evaluations yield clean trends but average over\nheterogeneous subpopulations, obscuring performance disparities. We introduce\nrelative scaling laws, which track how performance gaps between test\ndistributions evolve with scale rather than focusing solely on absolute error.\nUsing 255 decoder-only Transformers trained under matched-compute (IsoFLOP)\nbudgets from $10^{18}$--$10^{20}$ FLOPs on standard pretraining datasets, we\nfind diverse trajectories: academic domains on MMLU converge toward parity;\nregional English dialects shift depending on population size; and clusters of\nAI risk behaviours split, with capability- and influence-related risks\nincreasing during pretraining while adversarial risks do not. These results\nshow that although scaling improves overall performance, it is not a universal\nequalizer. To support further study, we release all model checkpoints from this\nwork to enable practitioners to measure relative alongside traditional scaling\nlaws, in order to better prioritize robustness challenges in light of the\nbitter lesson.",
      "authors": [
        "William Held",
        "David Hall",
        "Percy Liang",
        "Diyi Yang"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24626v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24626v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.24619v1",
      "title": "Zero-Shot Cross-Lingual Transfer using Prefix-Based Adaptation",
      "abstract": "With the release of new large language models (LLMs) like Llama and Mistral,\nzero-shot cross-lingual transfer has become increasingly feasible due to their\nmultilingual pretraining and strong generalization capabilities. However,\nadapting these decoder-only LLMs to new tasks across languages remains\nchallenging. While parameter-efficient fine-tuning (PeFT) techniques like\nLow-Rank Adaptation (LoRA) are widely used, prefix-based techniques such as\nsoft prompt tuning, prefix tuning, and Llama Adapter are less explored,\nespecially for zero-shot transfer in decoder-only models. We present a\ncomprehensive study of three prefix-based methods for zero-shot cross-lingual\ntransfer from English to 35+ high- and low-resource languages. Our analysis\nfurther explores transfer across linguistic families and scripts, as well as\nthe impact of scaling model sizes from 1B to 24B. With Llama 3.1 8B, prefix\nmethods outperform LoRA-baselines by up to 6% on the Belebele benchmark.\nSimilar improvements were observed with Mistral v0.3 7B as well. Despite using\nonly 1.23M learning parameters with prefix tuning, we achieve consistent\nimprovements across diverse benchmarks. These findings highlight the potential\nof prefix-based techniques as an effective and scalable alternative to LoRA,\nparticularly in low-resource multilingual settings.",
      "authors": [
        "Snegha A",
        "Sayambhu Sen",
        "Piyush Singh Pasi",
        "Abhishek Singhania",
        "Preethi Jyothi"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24619v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "I.2.7"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24619v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.24606v1",
      "title": "Long-Context Modeling with Dynamic Hierarchical Sparse Attention for On-Device LLMs",
      "abstract": "The quadratic cost of attention hinders the scalability of long-context LLMs,\nespecially in resource-constrained settings. Existing static sparse methods\nsuch as sliding windows or global tokens utilizes the sparsity of attention to\nreduce the cost of attention, but poorly adapts to the content-dependent\nvariations in attention due to their staticity. While previous work has\nproposed several dynamic approaches to improve flexibility, they still depend\non predefined templates or heuristic mechanisms. Such strategies reduce\ngenerality and prune tokens that remain contextually important, limiting their\naccuracy across diverse tasks. To tackle these bottlenecks of existing methods\nfor long-context modeling, we introduce Dynamic Hierarchical Sparse Attention\n(DHSA), a data-driven framework that dynamically predicts attention sparsity\nonline without retraining. Our proposed DHSA adaptively segments sequences into\nvariable-length chunks, then computes chunk representations by aggregating the\ntoken embeddings within each chunk. To avoid the bias introduced by varying\nchunk lengths, we apply length-normalized aggregation that scales the averaged\nembeddings by the square root of the chunk size. Finally, DHSA upsamples the\nchunk-level similarity scores to token level similarities to calculate\nimportance scores that determine which token-level interactions should be\npreserved. Our experiments on Gemma2 with Needle-in-a-Haystack Test and\nLongBench show that DHSA matches dense attention in accuracy, while reducing\nprefill latency by 20-60% and peak memory usage by 35%. Compared to other\nrepresentative baselines such as block sparse attention, DHSA achieves\nconsistently higher accuracy (6-18% relative gains) with comparable or lower\ncost, offering an efficient and adaptable solution for long-context on-device\nLLMs.",
      "authors": [
        "Siheng Xiong",
        "Joe Zou",
        "Faramarz Fekri",
        "Yae Jee Cho"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24606v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24606v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.24605v1",
      "title": "Diffusion LLM with Native Variable Generation Lengths: Let [EOS] Lead the Way",
      "abstract": "Diffusion-based large language models (dLLMs) have exhibited substantial\npotential for parallel text generation, which may enable more efficient\ngeneration compared to autoregressive models. However, current dLLMs suffer\nfrom fixed generation lengths, which indicates the generation lengths of dLLMs\nhave to be determined before decoding as a hyper-parameter, leading to issues\nin efficiency and flexibility. To solve these problems, in this work, we\npropose to train a diffusion LLM with native variable generation lengths,\nabbreviated as dLLM-Var. Concretely, we aim to train a model to accurately\npredict the [EOS] token in the generated text, which makes a dLLM be able to\nnatively infer in a block diffusion manner, while still maintaining the ability\nof global bi-directional (full) attention and high parallelism. Experiments on\nstandard benchmarks demonstrate that our method achieves a 30.1x speedup over\ntraditional dLLM inference paradigms and a 2.4x speedup relative to\nautoregressive models such as Qwen and Llama. Our method achieves higher\naccuracy and faster inference, elevating dLLMs beyond mere academic novelty and\nsupporting their practical use in real-world applications. Codes and models\nhave been released.",
      "authors": [
        "Yicun Yang",
        "Cong Wang",
        "Shaobo Wang",
        "Zichen Wen",
        "Biqing Qi",
        "Hanlin Xu",
        "Linfeng Zhang"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24605v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24605v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.24592v2",
      "title": "ReForm: Reflective Autoformalization with Prospective Bounded Sequence Optimization",
      "abstract": "Autoformalization, which translates natural language mathematics into\nmachine-verifiable formal statements, is critical for using formal mathematical\nreasoning to solve math problems stated in natural language. While Large\nLanguage Models can generate syntactically correct formal statements, they\noften fail to preserve the original problem's semantic intent. This limitation\narises from the LLM approaches' treating autoformalization as a simplistic\ntranslation task which lacks mechanisms for self-reflection and iterative\nrefinement that human experts naturally employ. To address these issues, we\npropose ReForm, a Reflective Autoformalization method that tightly integrates\nsemantic consistency evaluation into the autoformalization process. This\nenables the model to iteratively generate formal statements, assess its\nsemantic fidelity, and self-correct identified errors through progressive\nrefinement. To effectively train this reflective model, we introduce\nProspective Bounded Sequence Optimization (PBSO), which employs different\nrewards at different sequence positions to ensure that the model develops both\naccurate autoformalization and correct semantic validations, preventing\nsuperficial critiques that would undermine the purpose of reflection. Extensive\nexperiments across four autoformalization benchmarks demonstrate that ReForm\nachieves an average improvement of 22.6 percentage points over the strongest\nbaselines. To further ensure evaluation reliability, we introduce\nConsistencyCheck, a benchmark of 859 expert-annotated items that not only\nvalidates LLMs as judges but also reveals that autoformalization is inherently\ndifficult: even human experts produce semantic errors in up to 38.5% of cases.",
      "authors": [
        "Guoxin Chen",
        "Jing Wu",
        "Xinjie Chen",
        "Wayne Xin Zhao",
        "Ruihua Song",
        "Chengxi Li",
        "Kai Fan",
        "Dayiheng Liu",
        "Minpeng Liao"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24592v2",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24592v2"
    },
    {
      "url": "https://arxiv.org/abs/2510.24591v1",
      "title": "ReplicationBench: Can AI Agents Replicate Astrophysics Research Papers?",
      "abstract": "Frontier AI agents show increasing promise as scientific research assistants,\nand may eventually be useful for extended, open-ended research workflows.\nHowever, in order to use agents for novel research, we must first assess the\nunderlying faithfulness and correctness of their work. To evaluate agents as\nresearch assistants, we introduce ReplicationBench, an evaluation framework\nthat tests whether agents can replicate entire research papers drawn from the\nastrophysics literature. Astrophysics, where research relies heavily on\narchival data and computational study while requiring little real-world\nexperimentation, is a particularly useful testbed for AI agents in scientific\nresearch. We split each paper into tasks which require agents to replicate the\npaper's core contributions, including the experimental setup, derivations, data\nanalysis, and codebase. Each task is co-developed with the original paper\nauthors and targets a key scientific result, enabling objective evaluation of\nboth faithfulness (adherence to original methods) and correctness (technical\naccuracy of results). ReplicationBench is extremely challenging for current\nfrontier language models: even the best-performing language models score under\n20%. We analyze ReplicationBench trajectories in collaboration with domain\nexperts and find a rich, diverse set of failure modes for agents in scientific\nresearch. ReplicationBench establishes the first benchmark of paper-scale,\nexpert-validated astrophysics research tasks, reveals insights about agent\nperformance generalizable to other domains of data-driven science, and provides\na scalable framework for measuring AI agents' reliability in scientific\nresearch.",
      "authors": [
        "Christine Ye",
        "Sihan Yuan",
        "Suchetha Cooray",
        "Steven Dillmann",
        "Ian L. V. Roque",
        "Dalya Baron",
        "Philipp Frank",
        "Sergio Martin-Alvarez",
        "Nolan Koblischke",
        "Frank J Qu",
        "Diyi Yang",
        "Risa Wechsler",
        "Ioana Ciuca"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24591v1",
      "categories": [
        "cs.CL",
        "astro-ph.IM"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24591v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.24570v1",
      "title": "BEST-RQ-Based Self-Supervised Learning for Whisper Domain Adaptation",
      "abstract": "Automatic Speech Recognition (ASR) systems, despite large multilingual\ntraining, struggle in out-of-domain and low-resource scenarios where labeled\ndata is scarce. We propose BEARD (BEST-RQ Encoder Adaptation with Re-training\nand Distillation), a novel framework designed to adapt Whisper's encoder using\nunlabeled data. Unlike traditional self-supervised learning methods, BEARD\nuniquely combines a BEST-RQ objective with knowledge distillation from a frozen\nteacher encoder, ensuring the encoder's complementarity with the pre-trained\ndecoder. Our experiments focus on the ATCO2 corpus from the challenging Air\nTraffic Control (ATC) communications domain, characterized by non-native\nspeech, noise, and specialized phraseology. Using about 5,000 hours of\nuntranscribed speech for BEARD and 2 hours of transcribed speech for\nfine-tuning, the proposed approach significantly outperforms previous baseline\nand fine-tuned model, achieving a relative improvement of 12% compared to the\nfine-tuned model. To the best of our knowledge, this is the first work to use a\nself-supervised learning objective for domain adaptation of Whisper.",
      "authors": [
        "Raphaël Bagat",
        "Irina Illina",
        "Emmanuel Vincent"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24570v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24570v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25786v1",
      "title": "BlackboxNLP-2025 MIB Shared Task: Improving Circuit Faithfulness via Better Edge Selection",
      "abstract": "One of the main challenges in mechanistic interpretability is circuit\ndiscovery, determining which parts of a model perform a given task. We build on\nthe Mechanistic Interpretability Benchmark (MIB) and propose three key\nimprovements to circuit discovery. First, we use bootstrapping to identify\nedges with consistent attribution scores. Second, we introduce a simple\nratio-based selection strategy to prioritize strong positive-scoring edges,\nbalancing performance and faithfulness. Third, we replace the standard greedy\nselection with an integer linear programming formulation. Our methods yield\nmore faithful circuits and outperform prior approaches across multiple MIB\ntasks and models. Our code is available at:\nhttps://github.com/technion-cs-nlp/MIB-Shared-Task.",
      "authors": [
        "Yaniv Nikankin",
        "Dana Arad",
        "Itay Itzhak",
        "Anja Reusch",
        "Adi Simhi",
        "Gal Kesten-Pomeranz",
        "Yonatan Belinkov"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25786v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25786v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.24541v1",
      "title": "Open Korean Historical Corpus: A Millennia-Scale Diachronic Collection of Public Domain Texts",
      "abstract": "The history of the Korean language is characterized by a discrepancy between\nits spoken and written forms and a pivotal shift from Chinese characters to the\nHangul alphabet. However, this linguistic evolution has remained largely\nunexplored in NLP due to a lack of accessible historical corpora. To address\nthis gap, we introduce the Open Korean Historical Corpus, a large-scale, openly\nlicensed dataset spanning 1,300 years and 6 languages, as well as\nunder-represented writing systems like Korean-style Sinitic (Idu) and\nHanja-Hangul mixed script. This corpus contains 18 million documents and 5\nbillion tokens from 19 sources, ranging from the 7th century to 2025. We\nleverage this resource to quantitatively analyze major linguistic shifts: (1)\nIdu usage peaked in the 1860s before declining sharply; (2) the transition from\nHanja to Hangul was a rapid transformation starting around 1890; and (3) North\nKorea's lexical divergence causes modern tokenizers to produce up to 51 times\nhigher out-of-vocabulary rates. This work provides a foundational resource for\nquantitative diachronic analysis by capturing the history of the Korean\nlanguage. Moreover, it can serve as a pre-training corpus for large language\nmodels, potentially improving their understanding of Sino-Korean vocabulary in\nmodern Hangul as well as archaic writing systems.",
      "authors": [
        "Seyoung Song",
        "Nawon Kim",
        "Songeun Chae",
        "Kiwoong Park",
        "Jiho Jin",
        "Haneul Yoo",
        "Kyunghyun Cho",
        "Alice Oh"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24541v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24541v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.24538v1",
      "title": "Dark & Stormy: Modeling Humor in the Worst Sentences Ever Written",
      "abstract": "Textual humor is enormously diverse and computational studies need to account\nfor this range, including intentionally bad humor. In this paper, we curate and\nanalyze a novel corpus of sentences from the Bulwer-Lytton Fiction Contest to\nbetter understand \"bad\" humor in English. Standard humor detection models\nperform poorly on our corpus, and an analysis of literary devices finds that\nthese sentences combine features common in existing humor datasets (e.g., puns,\nirony) with metaphor, metafiction and simile. LLMs prompted to synthesize\ncontest-style sentences imitate the form but exaggerate the effect by\nover-using certain literary devices, and including far more novel\nadjective-noun bigrams than human writers. Data, code and analysis are\navailable at https://github.com/venkatasg/bulwer-lytton",
      "authors": [
        "Venkata S Govindarajan",
        "Laura Biester"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24538v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24538v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.24530v1",
      "title": "Levée d'ambiguïtés par grammaires locales",
      "abstract": "Many words are ambiguous in terms of their part of speech (POS). However,\nwhen a word appears in a text, this ambiguity is generally much reduced.\nDisambiguating POS involves using context to reduce the number of POS\nassociated with words, and is one of the main challenges of lexical tagging.\nThe problem of labeling words by POS frequently arises in natural language\nprocessing, for example for spelling correction, grammar or style checking,\nexpression recognition, text-to-speech conversion, text corpus analysis, etc.\nLexical tagging systems are thus useful as an initial component of many natural\nlanguage processing systems. A number of recent lexical tagging systems produce\nmultiple solutions when the text is lexically ambiguous or the uniquely correct\nsolution cannot be found. These contributions aim to guarantee a zero silence\nrate: the correct tag(s) for a word must never be discarded. This objective is\nunrealistic for systems that tag each word uniquely. This article concerns a\nlexical disambiguation method adapted to the objective of a zero silence rate\nand implemented in Silberztein's INTEX system (1993). We present here a formal\ndescription of this method. We show that to verify a local disambiguation\ngrammar in this framework, it is not sufficient to consider the transducer\npaths separately: one needs to verify their interactions. Similarly, if a\ncombination of multiple transducers is used, the result cannot be predicted by\nconsidering them in isolation. Furthermore, when examining the initial labeling\nof a text as produced by INTEX, ideas for disambiguation rules come\nspontaneously, but grammatical intuitions may turn out to be inaccurate, often\ndue to an unforeseen construction or ambiguity. If a zero silence rate is\ntargeted, local grammars must be carefully tested. This is where a detailed\nspecification of what a grammar will do once applied to texts would be\nnecessary.",
      "authors": [
        "Eric G. C. Laporte"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24530v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24530v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.24824v1",
      "title": "Parallel Loop Transformer for Efficient Test-Time Computation Scaling",
      "abstract": "Large Language Models (LLMs) are powerful but often too slow and costly for\nreal-world use during inference. Looped transformers save on parameters by\nreusing the same weights for multiple computational steps, or \"loops.\" However,\nthis approach has a major flaw: the loops run one after another, causing\ninference latency and memory requirements to increase with each added loop.\nThis makes them impractical for fast applications. To solve this problem, we\nintroduce the Parallel Loop Transformer (PLT). PLT is a new architecture that\ndelivers the performance benefits of a deep, looped model but with the low\nlatency of a standard, non-looped model. PLT works using two key techniques.\nFirst, Cross-Loop Parallelism (CLP) breaks the sequential dependency by\ncomputing different loops for different tokens at the same time, all within a\nsingle pass. Second, to prevent memory costs from growing, we use an Efficient\nRepresentation Enhancement strategy. This method shares the memory (KV cache)\nfrom the first loop with all other loops. It then uses a Gated Sliding-Window\nAttention (G-SWA) to combine this shared global information with local\ninformation, maintaining high accuracy. Our experiments show that PLT achieves\nthe high accuracy of a traditional looped model but with almost no extra\nlatency or memory cost compared to a standard transformer.",
      "authors": [
        "Bohong Wu",
        "Mengzhao Chen",
        "Xiang Luo",
        "Shen Yan",
        "Qifan Yu",
        "Fan Xia",
        "Tianqi Zhang",
        "Hongrui Zhan",
        "Zheng Zhong",
        "Xun Zhou",
        "Siyuan Qiao",
        "Xingyan Bin"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24824v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24824v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.24514v1",
      "title": "Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs",
      "abstract": "While Multimodal Large Language Models (MLLMs) excel at visual understanding,\nthey often struggle in complex scenarios that require visual planning and\nimagination. Inspired by how humans use sketching as a form of visual thinking\nto develop and communicate ideas, we introduce Latent Sketchpad, a framework\nthat equips MLLMs with an internal visual scratchpad. The internal visual\nrepresentations of MLLMs have traditionally been confined to perceptual\nunderstanding. We repurpose them to support generative visual thought without\ncompromising reasoning ability. Building on frontier MLLMs, our approach\nintegrates visual generation directly into their native autoregressive\nreasoning process. It allows the model to interleave textual reasoning with the\ngeneration of visual latents. These latents guide the internal thought process\nand can be translated into sketch images for interpretability. To realize this,\nwe introduce two components: a Context-Aware Vision Head autoregressively\nproduces visual representations, and a pretrained Sketch Decoder renders these\ninto human-interpretable images. We evaluate the framework on our new dataset\nMazePlanning. Experiments across various MLLMs show that Latent Sketchpad\ndelivers comparable or even superior reasoning performance to their backbone.\nIt further generalizes across distinct frontier MLLMs, including Gemma3 and\nQwen2.5-VL. By extending model's textual reasoning to visual thinking, our\nframework opens new opportunities for richer human-computer interaction and\nbroader applications. More details and resources are available on our project\npage: https://latent-sketchpad.github.io/.",
      "authors": [
        "Huanyu Zhang",
        "Wenshan Wu",
        "Chengzu Li",
        "Ning Shang",
        "Yan Xia",
        "Yangyu Huang",
        "Yifan Zhang",
        "Li Dong",
        "Zhang Zhang",
        "Liang Wang",
        "Tieniu Tan",
        "Furu Wei"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24514v1",
      "categories": [
        "cs.CV",
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24514v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.24505v1",
      "title": "CritiCal: Can Critique Help LLM Uncertainty or Confidence Calibration?",
      "abstract": "Accurate confidence calibration in Large Language Models (LLMs) is critical\nfor safe use in high-stakes domains, where clear verbalized confidence enhances\nuser trust. Traditional methods that mimic reference confidence expressions\noften fail to capture the reasoning needed for accurate confidence assessment.\nWe propose natural language critiques as a solution, ideally suited for\nconfidence calibration, as precise gold confidence labels are hard to obtain\nand often require multiple generations. This paper studies how natural language\ncritiques can enhance verbalized confidence, addressing: (1) What to critique:\nuncertainty (question-focused) or confidence (answer-specific)? Analysis shows\nconfidence suits multiple-choice tasks, while uncertainty excels in open-ended\nscenarios. (2) How to critique: self-critique or critique calibration training?\nWe propose Self-Critique, enabling LLMs to critique and optimize their\nconfidence beyond mere accuracy, and CritiCal, a novel Critique Calibration\ntraining method that leverages natural language critiques to improve confidence\ncalibration, moving beyond direct numerical optimization. Experiments show that\nCritiCal significantly outperforms Self-Critique and other competitive\nbaselines, even surpassing its teacher model, GPT-4o, in complex reasoning\ntasks. CritiCal also shows robust generalization in out-of-distribution\nsettings, advancing LLM's reliability.",
      "authors": [
        "Qing Zong",
        "Jiayu Liu",
        "Tianshi Zheng",
        "Chunyang Li",
        "Baixuan Xu",
        "Haochen Shi",
        "Weiqi Wang",
        "Zhaowei Wang",
        "Chunkit Chan",
        "Yangqiu Song"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24505v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24505v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.24488v1",
      "title": "A word association network methodology for evaluating implicit biases in LLMs compared to humans",
      "abstract": "As Large language models (LLMs) become increasingly integrated into our\nlives, their inherent social biases remain a pressing concern. Detecting and\nevaluating these biases can be challenging because they are often implicit\nrather than explicit in nature, so developing evaluation methods that assess\nthe implicit knowledge representations of LLMs is essential. We present a novel\nword association network methodology for evaluating implicit biases in LLMs\nbased on simulating semantic priming within LLM-generated word association\nnetworks. Our prompt-based approach taps into the implicit relational\nstructures encoded in LLMs, providing both quantitative and qualitative\nassessments of bias. Unlike most prompt-based evaluation methods, our method\nenables direct comparisons between various LLMs and humans, providing a\nvaluable point of reference and offering new insights into the alignment of\nLLMs with human cognition. To demonstrate the utility of our methodology, we\napply it to both humans and several widely used LLMs to investigate social\nbiases related to gender, religion, ethnicity, sexual orientation, and\npolitical party. Our results reveal both convergences and divergences between\nLLM and human biases, providing new perspectives on the potential risks of\nusing LLMs. Our methodology contributes to a systematic, scalable, and\ngeneralizable framework for evaluating and comparing biases across multiple\nLLMs and humans, advancing the goal of transparent and socially responsible\nlanguage technologies.",
      "authors": [
        "Katherine Abramski",
        "Giulio Rossetti",
        "Massimo Stella"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24488v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24488v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.24478v1",
      "title": "Talk2Ref: A Dataset for Reference Prediction from Scientific Talks",
      "abstract": "Scientific talks are a growing medium for disseminating research, and\nautomatically identifying relevant literature that grounds or enriches a talk\nwould be highly valuable for researchers and students alike. We introduce\nReference Prediction from Talks (RPT), a new task that maps long, and\nunstructured scientific presentations to relevant papers. To support research\non RPT, we present Talk2Ref, the first large-scale dataset of its kind,\ncontaining 6,279 talks and 43,429 cited papers (26 per talk on average), where\nrelevance is approximated by the papers cited in the talk's corresponding\nsource publication. We establish strong baselines by evaluating\nstate-of-the-art text embedding models in zero-shot retrieval scenarios, and\npropose a dual-encoder architecture trained on Talk2Ref. We further explore\nstrategies for handling long transcripts, as well as training for domain\nadaptation. Our results show that fine-tuning on Talk2Ref significantly\nimproves citation prediction performance, demonstrating both the challenges of\nthe task and the effectiveness of our dataset for learning semantic\nrepresentations from spoken scientific content. The dataset and trained models\nare released under an open license to foster future research on integrating\nspoken scientific communication into citation recommendation systems.",
      "authors": [
        "Frederik Broy",
        "Maike Züfle",
        "Jan Niehues"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24478v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24478v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.24476v1",
      "title": "Mitigating Hallucination in Large Language Models (LLMs): An Application-Oriented Survey on RAG, Reasoning, and Agentic Systems",
      "abstract": "Hallucination remains one of the key obstacles to the reliable deployment of\nlarge language models (LLMs), particularly in real-world applications. Among\nvarious mitigation strategies, Retrieval-Augmented Generation (RAG) and\nreasoning enhancement have emerged as two of the most effective and widely\nadopted approaches, marking a shift from merely suppressing hallucinations to\nbalancing creativity and reliability. However, their synergistic potential and\nunderlying mechanisms for hallucination mitigation have not yet been\nsystematically examined. This survey adopts an application-oriented perspective\nof capability enhancement to analyze how RAG, reasoning enhancement, and their\nintegration in Agentic Systems mitigate hallucinations. We propose a taxonomy\ndistinguishing knowledge-based and logic-based hallucinations, systematically\nexamine how RAG and reasoning address each, and present a unified framework\nsupported by real-world applications, evaluations, and benchmarks.",
      "authors": [
        "Yihan Li",
        "Xiyuan Fu",
        "Ghanshyam Verma",
        "Paul Buitelaar",
        "Mingming Liu"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24476v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24476v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.24469v1",
      "title": "Iterative Critique-Refine Framework for Enhancing LLM Personalization",
      "abstract": "Personalized text generation requires models not only to produce coherent\ntext but also to align with a target user's style, tone, and topical focus.\nExisting retrieval-augmented approaches such as LaMP and PGraphRAG enrich\nprofiles with user and neighbor histories, but they stop at generation and\noften yield outputs that drift in tone, topic, or style. We present PerFine, a\nunified, training-free critique-refine framework that enhances personalization\nthrough iterative, profile-grounded feedback. In each iteration, an LLM\ngenerator produces a draft conditioned on the retrieved profile, and a critic\nLLM - also conditioned on the same profile - provides structured feedback on\ntone, vocabulary, sentence structure, and topicality. The generator then\nrevises, while a novel knockout strategy retains the stronger draft across\niterations. We further study additional inference-time strategies such as\nBest-of-N and Topic Extraction to balance quality and efficiency. Across Yelp,\nGoodreads, and Amazon datasets, PerFine consistently improves personalization\nover PGraphRAG, with GEval gains of +7-13%, steady improvements over 3-5\nrefinement iterations, and scalability with increasing critic size. These\nresults highlight that post-hoc, profile-aware feedback offers a powerful\nparadigm for personalized LLM generation that is both training-free and\nmodel-agnostic.",
      "authors": [
        "Durga Prasad Maram",
        "Dhruvin Gandhi",
        "Zonghai Yao",
        "Gayathri Akkinapalli",
        "Franck Dernoncourt",
        "Yu Wang",
        "Ryan A. Rossi",
        "Nesreen K. Ahmed"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24469v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24469v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.24450v1",
      "title": "Charting the European LLM Benchmarking Landscape: A New Taxonomy and a Set of Best Practices",
      "abstract": "While new benchmarks for large language models (LLMs) are being developed\ncontinuously to catch up with the growing capabilities of new models and AI in\ngeneral, using and evaluating LLMs in non-English languages remains a\nlittle-charted landscape. We give a concise overview of recent developments in\nLLM benchmarking, and then propose a new taxonomy for the categorization of\nbenchmarks that is tailored to multilingual or non-English use scenarios. We\nfurther propose a set of best practices and quality standards that could lead\nto a more coordinated development of benchmarks for European languages. Among\nother recommendations, we advocate for a higher language and culture\nsensitivity of evaluation methods.",
      "authors": [
        "Špela Vintar",
        "Taja Kuzman Pungeršek",
        "Mojca Brglez",
        "Nikola Ljubešić"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24450v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24450v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.24446v1",
      "title": "SPARTA: Evaluating Reasoning Segmentation Robustness through Black-Box Adversarial Paraphrasing in Text Autoencoder Latent Space",
      "abstract": "Multimodal large language models (MLLMs) have shown impressive capabilities\nin vision-language tasks such as reasoning segmentation, where models generate\nsegmentation masks based on textual queries. While prior work has primarily\nfocused on perturbing image inputs, semantically equivalent textual\nparaphrases-crucial in real-world applications where users express the same\nintent in varied ways-remain underexplored. To address this gap, we introduce a\nnovel adversarial paraphrasing task: generating grammatically correct\nparaphrases that preserve the original query meaning while degrading\nsegmentation performance. To evaluate the quality of adversarial paraphrases,\nwe develop a comprehensive automatic evaluation protocol validated with human\nstudies. Furthermore, we introduce SPARTA-a black-box, sentence-level\noptimization method that operates in the low-dimensional semantic latent space\nof a text autoencoder, guided by reinforcement learning. SPARTA achieves\nsignificantly higher success rates, outperforming prior methods by up to 2x on\nboth the ReasonSeg and LLMSeg-40k datasets. We use SPARTA and competitive\nbaselines to assess the robustness of advanced reasoning segmentation models.\nWe reveal that they remain vulnerable to adversarial paraphrasing-even under\nstrict semantic and grammatical constraints. All code and data will be released\npublicly upon acceptance.",
      "authors": [
        "Viktoriia Zinkovich",
        "Anton Antonov",
        "Andrei Spiridonov",
        "Denis Shepelev",
        "Andrey Moskalenko",
        "Daria Pugacheva",
        "Elena Tutubalina",
        "Andrey Kuznetsov",
        "Vlad Shakhuro"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24446v1",
      "categories": [
        "cs.CL",
        "cs.CV"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24446v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.24442v1",
      "title": "Law in Silico: Simulating Legal Society with LLM-Based Agents",
      "abstract": "Since real-world legal experiments are often costly or infeasible, simulating\nlegal societies with Artificial Intelligence (AI) systems provides an effective\nalternative for verifying and developing legal theory, as well as supporting\nlegal administration. Large Language Models (LLMs), with their world knowledge\nand role-playing capabilities, are strong candidates to serve as the foundation\nfor legal society simulation. However, the application of LLMs to simulate\nlegal systems remains underexplored. In this work, we introduce Law in Silico,\nan LLM-based agent framework for simulating legal scenarios with individual\ndecision-making and institutional mechanisms of legislation, adjudication, and\nenforcement. Our experiments, which compare simulated crime rates with\nreal-world data, demonstrate that LLM-based agents can largely reproduce\nmacro-level crime trends and provide insights that align with real-world\nobservations. At the same time, micro-level simulations reveal that a\nwell-functioning, transparent, and adaptive legal system offers better\nprotection of the rights of vulnerable individuals.",
      "authors": [
        "Yiding Wang",
        "Yuxuan Chen",
        "Fanxu Meng",
        "Xifan Chen",
        "Xiaolei Yang",
        "Muhan Zhang"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24442v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CY",
        "cs.MA"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24442v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.24438v1",
      "title": "Can LLMs Write Faithfully? An Agent-Based Evaluation of LLM-generated Islamic Content",
      "abstract": "Large language models are increasingly used for Islamic guidance, but risk\nmisquoting texts, misapplying jurisprudence, or producing culturally\ninconsistent responses. We pilot an evaluation of GPT-4o, Ansari AI, and Fanar\non prompts from authentic Islamic blogs. Our dual-agent framework uses a\nquantitative agent for citation verification and six-dimensional scoring (e.g.,\nStructure, Islamic Consistency, Citations) and a qualitative agent for\nfive-dimensional side-by-side comparison (e.g., Tone, Depth, Originality).\nGPT-4o scored highest in Islamic Accuracy (3.93) and Citation (3.38), Ansari AI\nfollowed (3.68, 3.32), and Fanar lagged (2.76, 1.82). Despite relatively strong\nperformance, models still fall short in reliably producing accurate Islamic\ncontent and citations -- a paramount requirement in faith-sensitive writing.\nGPT-4o had the highest mean quantitative score (3.90/5), while Ansari AI led\nqualitative pairwise wins (116/200). Fanar, though trailing, introduces\ninnovations for Islamic and Arabic contexts. This study underscores the need\nfor community-driven benchmarks centering Muslim perspectives, offering an\nearly step toward more reliable AI in Islamic knowledge and other high-stakes\ndomains such as medicine, law, and journalism.",
      "authors": [
        "Abdullah Mushtaq",
        "Rafay Naeem",
        "Ezieddin Elmahjub",
        "Ibrahim Ghaznavi",
        "Shawqi Al-Maliki",
        "Mohamed Abdallah",
        "Ala Al-Fuqaha",
        "Junaid Qadir"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24438v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.MA"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24438v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.24434v1",
      "title": "LuxIT: A Luxembourgish Instruction Tuning Dataset from Monolingual Seed Data",
      "abstract": "The effectiveness of instruction-tuned Large Language Models (LLMs) is often\nlimited in low-resource linguistic settings due to a lack of high-quality\ntraining data. We introduce LuxIT, a novel, monolingual instruction tuning\ndataset for Luxembourgish developed to mitigate this challenge. We synthesize\nthe dataset from a corpus of native Luxembourgish texts, utilizing\nDeepSeek-R1-0528, chosen for its shown proficiency in Luxembourgish. Following\ngeneration, we apply a quality assurance process, employing an LLM-as-a-judge\napproach. To investigate the practical utility of the dataset, we fine-tune\nseveral smaller-scale LLMs on LuxIT. Subsequent benchmarking against their base\nmodels on Luxembourgish language proficiency examinations, however, yields\nmixed results, with performance varying significantly across different models.\nLuxIT represents a critical contribution to Luxembourgish natural language\nprocessing and offers a replicable monolingual methodology, though our findings\nhighlight the need for further research to optimize its application.",
      "authors": [
        "Julian Valline",
        "Cedric Lothritz",
        "Jordi Cabot"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24434v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24434v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.24427v2",
      "title": "SynthWorlds: Controlled Parallel Worlds for Disentangling Reasoning and Knowledge in Language Models",
      "abstract": "Evaluating the reasoning ability of language models (LMs) is complicated by\ntheir extensive parametric world knowledge, where benchmark performance often\nreflects factual recall rather than genuine reasoning. Existing datasets and\napproaches (e.g., temporal filtering, paraphrasing, adversarial substitution)\ncannot cleanly separate the two. We present SynthWorlds, a framework that\ndisentangles task reasoning complexity from factual knowledge. In SynthWorlds,\nwe construct parallel corpora representing two worlds with identical\ninterconnected structure: a real-mapped world, where models may exploit\nparametric knowledge, and a synthetic-mapped world, where such knowledge is\nmeaningless. On top of these corpora, we design two mirrored tasks as case\nstudies: multi-hop question answering and page navigation, which maintain equal\nreasoning difficulty across worlds. Experiments in parametric-only (e.g.,\nclosed-book QA) and knowledge-augmented (e.g., retrieval-augmented) LM settings\nreveal a persistent knowledge advantage gap, defined as the performance boost\nmodels gain from memorized parametric world knowledge. Knowledge acquisition\nand integration mechanisms reduce but do not eliminate this gap, highlighting\nopportunities for system improvements. Fully automatic and scalable,\nSynthWorlds provides a controlled environment for evaluating LMs in ways that\nwere previously challenging, enabling precise and testable comparisons of\nreasoning and memorization.",
      "authors": [
        "Ken Gu",
        "Advait Bhat",
        "Mike A Merrill",
        "Robert West",
        "Xin Liu",
        "Daniel McDuff",
        "Tim Althoff"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24427v2",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24427v2"
    },
    {
      "url": "https://arxiv.org/abs/2510.24425v1",
      "title": "Comprehensive and Efficient Distillation for Lightweight Sentiment Analysis Models",
      "abstract": "Recent efforts leverage knowledge distillation techniques to develop\nlightweight and practical sentiment analysis models. These methods are grounded\nin human-written instructions and large-scale user texts. Despite the promising\nresults, two key challenges remain: (1) manually written instructions are\nlimited in diversity and quantity, making them insufficient to ensure\ncomprehensive coverage of distilled knowledge; (2) large-scale user texts incur\nhigh computational cost, hindering the practicality of these methods. To this\nend, we introduce COMPEFFDIST, a comprehensive and efficient distillation\nframework for sentiment analysis. Our framework consists of two key modules:\nattribute-based automatic instruction construction and difficulty-based data\nfiltering, which correspondingly tackle the aforementioned challenges. Applying\nour method across multiple model series (Llama-3, Qwen-3, and Gemma-3), we\nenable 3B student models to match the performance of 20x larger teacher models\non most tasks. In addition, our approach greatly outperforms baseline methods\nin data efficiency, attaining the same performance level with only 10% of the\ndata.",
      "authors": [
        "Guangyu Xie",
        "Yice Zhang",
        "Jianzhu Bao",
        "Qianlong Wang",
        "Yang Sun",
        "Bingbing Wang",
        "Ruifeng Xu"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24425v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24425v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.24411v1",
      "title": "OS-Sentinel: Towards Safety-Enhanced Mobile GUI Agents via Hybrid Validation in Realistic Workflows",
      "abstract": "Computer-using agents powered by Vision-Language Models (VLMs) have\ndemonstrated human-like capabilities in operating digital environments like\nmobile platforms. While these agents hold great promise for advancing digital\nautomation, their potential for unsafe operations, such as system compromise\nand privacy leakage, is raising significant concerns. Detecting these safety\nconcerns across the vast and complex operational space of mobile environments\npresents a formidable challenge that remains critically underexplored. To\nestablish a foundation for mobile agent safety research, we introduce\nMobileRisk-Live, a dynamic sandbox environment accompanied by a safety\ndetection benchmark comprising realistic trajectories with fine-grained\nannotations. Built upon this, we propose OS-Sentinel, a novel hybrid safety\ndetection framework that synergistically combines a Formal Verifier for\ndetecting explicit system-level violations with a VLM-based Contextual Judge\nfor assessing contextual risks and agent actions. Experiments show that\nOS-Sentinel achieves 10%-30% improvements over existing approaches across\nmultiple metrics. Further analysis provides critical insights that foster the\ndevelopment of safer and more reliable autonomous mobile agents.",
      "authors": [
        "Qiushi Sun",
        "Mukai Li",
        "Zhoumianze Liu",
        "Zhihui Xie",
        "Fangzhi Xu",
        "Zhangyue Yin",
        "Kanzhi Cheng",
        "Zehao Li",
        "Zichen Ding",
        "Qi Liu",
        "Zhiyong Wu",
        "Zhuosheng Zhang",
        "Ben Kao",
        "Lingpeng Kong"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24411v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.HC"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24411v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.25784v1",
      "title": "zFLoRA: Zero-Latency Fused Low-Rank Adapters",
      "abstract": "Large language models (LLMs) are increasingly deployed with task-specific\nadapters catering to multiple downstream applications. In such a scenario, the\nadditional compute associated with these apparently insignificant number of\nadapter parameters (typically less than 1% of the base model) turns out to be\ndisproportionately significant during inference time (upto 2.5x times that of\nthe base model). In this paper, we propose a new zero-latency fused low-rank\nadapter (zFLoRA) that introduces zero or negligible latency overhead on top of\nthe base model. Experimental results on LLMs of size 1B, 3B and 7B show that\nzFLoRA compares favorably against the popular supervised fine-tuning benchmarks\nincluding low-rank adapters (LoRA) as well as full fine-tuning (FFT).\nExperiments are conducted on 18 different tasks across three different\ncategories namely commonsense reasoning, math reasoning and summary-dialogue.\nLatency measurements made on NPU (Samsung Galaxy S25+) as well as GPU (NVIDIA\nH100) platforms show that the proposed zFLoRA adapters introduce zero to\nnegligible latency overhead.",
      "authors": [
        "Dhananjaya Gowda",
        "Seoha Song",
        "Harshith Goka",
        "Junhyun Lee"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25784v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25784v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.24365v1",
      "title": "Text Simplification with Sentence Embeddings",
      "abstract": "Sentence embeddings can be decoded to give approximations of the original\ntexts used to create them. We explore this effect in the context of text\nsimplification, demonstrating that reconstructed text embeddings preserve\ncomplexity levels. We experiment with a small feed forward neural network to\neffectively learn a transformation between sentence embeddings representing\nhigh-complexity and low-complexity texts. We provide comparison to a Seq2Seq\nand LLM-based approach, showing encouraging results in our much smaller\nlearning setting. Finally, we demonstrate the applicability of our\ntransformation to an unseen simplification dataset (MedEASI), as well as\ndatasets from languages outside the training data (ES,DE). We conclude that\nlearning transformations in sentence embedding space is a promising direction\nfor future research and has potential to unlock the ability to develop small,\nbut powerful models for text simplification and other natural language\ngeneration tasks.",
      "authors": [
        "Matthew Shardlow"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24365v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24365v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.24358v1",
      "title": "Automatically Benchmarking LLM Code Agents through Agent-Driven Annotation and Evaluation",
      "abstract": "Recent advances in code agents have enabled automated software development at\nthe project level, supported by large language models (LLMs) and widely adopted\ntools. However, existing benchmarks for code agent evaluation face two major\nlimitations: high annotation cost and expertise requirements, and rigid\nevaluation metrics that rely primarily on unit tests. To address these\nchallenges, we propose an agent-driven benchmark construction pipeline that\nleverages human supervision to efficiently generate diverse and challenging\nproject-level tasks. Based on this approach, we introduce PRDBench, a novel\nbenchmark comprising 50 real-world Python projects across 20 domains, each with\nstructured Product Requirement Document (PRD) requirements, comprehensive\nevaluation criteria, and reference implementations. PRDBench features rich data\nsources, high task complexity, and flexible metrics. We further employ an\nAgent-as-a-Judge paradigm to score agent outputs, enabling the evaluation of\nvarious test types beyond unit tests. Extensive experiments on PRDBench\ndemonstrate its effectiveness in assessing the capabilities of both code agents\nand evaluation agents, providing a scalable and robust framework for annotation\nand evaluation.",
      "authors": [
        "Lingyue Fu",
        "Bolun Zhang",
        "Hao Guan",
        "Yaoming Zhu",
        "Lin Qiu",
        "Weiwen Liu",
        "Xuezhi Cao",
        "Xunliang Cai",
        "Weinan Zhang",
        "Yong Yu"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24358v1",
      "categories": [
        "cs.SE",
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24358v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.24345v1",
      "title": "LongWeave: A Long-Form Generation Benchmark Bridging Real-World Relevance and Verifiability",
      "abstract": "Generating long, informative, and factual outputs remains a major challenge\nfor Large Language Models (LLMs). Existing benchmarks for long-form generation\ntypically assess real-world queries with hard-to-verify metrics or use\nsynthetic setups that ease evaluation but overlook real-world intricacies. In\nthis paper, we introduce \\textbf{LongWeave}, which balances real-world and\nverifiable assessment with Constraint-Verifier Evaluation (CoV-Eval). CoV-Eval\nconstructs tasks by first defining verifiable targets within real-world\nscenarios, then systematically generating corresponding queries, textual\nmaterials, and constraints based on these targets. This ensures that tasks are\nboth realistic and objectively assessable, enabling rigorous assessment of\nmodel capabilities in meeting complex real-world constraints. LongWeave\nsupports customizable input/output lengths (up to 64K/8K tokens) across seven\ndistinct tasks. Evaluation on 23 LLMs shows that even state-of-the-art models\nencounter significant challenges in long-form generation as real-world\ncomplexity and output length increase.",
      "authors": [
        "Zikai Xiao",
        "Fei Huang",
        "Jianhong Tu",
        "Jianhui Wei",
        "Wen Ma",
        "Yuxuan Zhou",
        "Jian Wu",
        "Bowen Yu",
        "Zuozhu Liu",
        "Junyang Lin"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24345v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24345v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.24328v1",
      "title": "Beyond MCQ: An Open-Ended Arabic Cultural QA Benchmark with Dialect Variants",
      "abstract": "Large Language Models (LLMs) are increasingly used to answer everyday\nquestions, yet their performance on culturally grounded and dialectal content\nremains uneven across languages. We propose a comprehensive method that (i)\ntranslates Modern Standard Arabic (MSA) multiple-choice questions (MCQs) into\nEnglish and several Arabic dialects, (ii) converts them into open-ended\nquestions (OEQs), (iii) benchmarks a range of zero-shot and fine-tuned LLMs\nunder both MCQ and OEQ settings, and (iv) generates chain-of-thought (CoT)\nrationales to fine-tune models for step-by-step reasoning. Using this method,\nwe extend an existing dataset in which QAs are parallelly aligned across\nmultiple language varieties, making it, to our knowledge, the first of its\nkind. We conduct extensive experiments with both open and closed models. Our\nfindings show that (i) models underperform on Arabic dialects, revealing\npersistent gaps in culturally grounded and dialect-specific knowledge; (ii)\nArabic-centric models perform well on MCQs but struggle with OEQs; and (iii)\nCoT improves judged correctness while yielding mixed n-gram-based metrics. The\ndeveloped dataset will be publicly released to support further research on\nculturally and linguistically inclusive evaluation.",
      "authors": [
        "Hunzalah Hassan Bhatti",
        "Firoj Alam"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24328v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "68T50",
        "F.2.2; I.2.7"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24328v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.24320v1",
      "title": "Critique-RL: Training Language Models for Critiquing through Two-Stage Reinforcement Learning",
      "abstract": "Training critiquing language models to assess and provide feedback on model\noutputs is a promising way to improve LLMs for complex reasoning tasks.\nHowever, existing approaches typically rely on stronger supervisors for\nannotating critique data. To address this, we propose Critique-RL, an online RL\napproach for developing critiquing language models without stronger\nsupervision. Our approach operates on a two-player paradigm: the actor\ngenerates a response, the critic provides feedback, and the actor refines the\nresponse accordingly. We first reveal that relying solely on indirect reward\nsignals from the actor's outputs for RL optimization often leads to\nunsatisfactory critics: while their helpfulness (i.e., providing constructive\nfeedback) improves, the discriminability (i.e., determining whether a response\nis high-quality or not) remains poor, resulting in marginal performance gains.\nTo overcome this, Critique-RL adopts a two-stage optimization strategy. In\nstage I, it reinforces the discriminability of the critic with direct\nrule-based reward signals; in stage II, it introduces indirect rewards based on\nactor refinement to improve the critic's helpfulness, while maintaining its\ndiscriminability via appropriate regularization. Extensive experiments across\nvarious tasks and models show that Critique-RL delivers substantial performance\nimprovements. For example, it achieves a 9.02% gain on in-domain tasks and a\n5.70% gain on out-of-domain tasks for Qwen2.5-7B, highlighting its potential.",
      "authors": [
        "Zhiheng Xi",
        "Jixuan Huang",
        "Xin Guo",
        "Boyang Hong",
        "Dingwen Yang",
        "Xiaoran Fan",
        "Shuo Li",
        "Zehui Chen",
        "Junjie Ye",
        "Siyu Yuan",
        "Zhengyin Du",
        "Xuesong Yao",
        "Yufei Xu",
        "Jiecao Chen",
        "Rui Zheng",
        "Tao Gui",
        "Qi Zhang",
        "Xuanjing Huang"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24320v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24320v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.24302v2",
      "title": "Lookahead Tree-Based Rollouts for Enhanced Trajectory-Level Exploration in Reinforcement Learning with Verifiable Rewards",
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR), particularly with\nalgorithms like Group Relative Policy Optimization (GRPO), has proven highly\neffective in enhancing the reasoning capabilities of large language models.\nHowever, a critical bottleneck in current pipelines lies in the limited\ndiversity of sampled trajectories during group rollouts. Homogeneous\ntrajectories and their associated rewards would diminish the return signals for\npolicy updates, thereby hindering effective policy learning. This lack of\ndiversity stems primarily from token-level stochastic sampling, where local\nvariations are likely to collapse into near-identical reasoning paths. To\naddress this limitation, we propose Lookahead Tree-Based Rollouts (LATR), a\nnovel rollout strategy designed to explicitly promotes trajectory-level\ndiversity by enforcing branching into different candidate tokens likely to\nyield distinct continuations. Specifically, LATR iteratively operates in three\nstages: (1) branching at high-uncertainty generation steps, (2) performing\nlookahead simulation for each new branch, and (3) pruning branches that\nexhibits prolonged similarity during simulation. Compared with stochastic\nSampling, LATR accelerates policy learning by 131% on average and improves\nfinal pass@1 performance by 4.2% on both GRPO and Dynamic sAmpling Policy\nOptimization (DAPO) algorithms across different reasoning tasks. Our code and\ndata are publicly available at https://github.com/starreeze/latr.",
      "authors": [
        "Shangyu Xing",
        "Siyuan Wang",
        "Chenyuan Yang",
        "Xinyu Dai",
        "Xiang Ren"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24302v2",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24302v2"
    },
    {
      "url": "https://arxiv.org/abs/2510.25783v1",
      "title": "LASTIST: LArge-Scale Target-Independent STance dataset",
      "abstract": "Stance detection has emerged as an area of research in the field of\nartificial intelligence. However, most research is currently centered on the\ntarget-dependent stance detection task, which is based on a person's stance in\nfavor of or against a specific target. Furthermore, most benchmark datasets are\nbased on English, making it difficult to develop models in low-resource\nlanguages such as Korean, especially for an emerging field such as stance\ndetection. This study proposes the LArge-Scale Target-Independent STance\n(LASTIST) dataset to fill this research gap. Collected from the press releases\nof both parties on Korean political parties, the LASTIST dataset uses 563,299\nlabeled Korean sentences. We provide a detailed description of how we collected\nand constructed the dataset and trained state-of-the-art deep learning and\nstance detection models. Our LASTIST dataset is designed for various tasks in\nstance detection, including target-independent stance detection and diachronic\nevolution stance detection. We deploy our dataset on\nhttps://anonymous.4open.science/r/LASTIST-3721/.",
      "authors": [
        "DongJae Kim",
        "Yaejin Lee",
        "Minsu Park",
        "Eunil Park"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.25783v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.7"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.25783v1"
    },
    {
      "url": "https://arxiv.org/abs/2510.24295v1",
      "title": "MERGE: Minimal Expression-Replacement GEneralization Test for Natural Language Inference",
      "abstract": "In recent years, many generalization benchmarks have shown language models'\nlack of robustness in natural language inference (NLI). However, manually\ncreating new benchmarks is costly, while automatically generating high-quality\nones, even by modifying existing benchmarks, is extremely difficult. In this\npaper, we propose a methodology for automatically generating high-quality\nvariants of original NLI problems by replacing open-class words, while\ncrucially preserving their underlying reasoning. We dub our generalization test\nas MERGE (Minimal Expression-Replacements GEneralization), which evaluates the\ncorrectness of models' predictions across reasoning-preserving variants of the\noriginal problem. Our results show that NLI models' perform 4-20% worse on\nvariants, suggesting low generalizability even on such minimally altered\nproblems. We also analyse how word class of the replacements, word probability,\nand plausibility influence NLI models' performance.",
      "authors": [
        "Mădălina Zgreabăn",
        "Tejaswini Deoskar",
        "Lasha Abzianidze"
      ],
      "date": "2025-10-28",
      "extraction_method": "arxiv_api",
      "arxiv_id": "2510.24295v1",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "http://arxiv.org/pdf/2510.24295v1"
    }
  ]
}